SVMheavy 7.0: an SVM (and more) implementation by Alistair Shilton.           
============                                                                  
                                                                              
Copyright: all rights reserved.                                               
                                                                              
Usage:     svmheavyv7 {options}                                               
                                                                              
Each line  in the  training/testing files  contains a  single sample  with the
following general format (where  values in {} are  optional; and != and  ! are
functionally equivalent):                                                     
                                                                              
{>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} {sVAL} {SVAL} x                  
                                                                              
Classification:                                                               
                                                                              
       - {>,=,!=,!,<} must not be included.                                   
       - y must be included.                                                  
       - y=-1,0,+1,+2,... is classification (0 for test/unknown).             
       - {tVAL} or {TVAL} sets the empirical risk scale.                      
       - {eVAL} or {EVAL} sets the distance to surface scale.                 
       - {sVAL} or {SVAL} sets the empirical sigma scale.                     
       - x is the training vector.                                            
                                                                              
Regression:                                                                   
                                                                              
       - {>,=,!=,!,<} defines constraint type (optional, default is =):       
         o >: g(x) > y                                                        
         o =: g(x) = y                                                        
         o <: g(x) < y                                                        
         o ! or !=: ignore this vector (it may be referenced elsewhere).      
       - y must be included.                                                  
       - y gives the target value for this point.                             
       - if y is anion, vector or gentype then >,< not defined.               
       - if y is gentype then it must lie in the basis set u_i.               
       - {tVAL} or {TVAL} sets the empirical risk scale.                      
       - {eVAL} or {EVAL} sets the epsilon insensitivity scale.               
       - x is the training vector.                                            
                                                                              
Similarity learning:                                                          
                                                                              
       - Format as for  regression, except that x takes  the form xa ~ xb (the
         character ~ acts as a separator), so the constraint is on g({xa,xb}).
                                                                              
Multi-instance learning:                                                      
                                                                              
       - Format as for regression,  but in this case x has  the general format
         xa ~ xb ~ ... ~ xn  (the   character  ~  acts  as  a  separator),  so
         constraint is on g({xa,xb,...,xn}).                                  
                                                                              
Single class:                                                                 
                                                                              
       - {>,=,!=,!,<} must not be included.                                   
       - y must not be included.                                              
       - {tVAL} or {TVAL} sets the empirical risk scale.                      
       - {eVAL} or {EVAL} sets the distance to surface scale.                 
       - x is the training vector.                                            
                                                                              
Auto-Encoder:                                                                 
                                                                              
       - {>,=,!=,!,<} must not be included.                                   
       - y must not be included (target is same as input x).                  
       - {tVAL} or {TVAL} sets the empirical risk scale.                      
       - {eVAL} or {EVAL} sets the distance to surface scale.                 
       - x is the training vector.                                            
                                                                              
Classification with Scoring:                                                  
                                                                              
       - {>,=,!=,!,<} must not be included.                                   
       - y must be included.                                                  
       - y = integer indicates a standard class membership constraint.        
       - y = vector indicates a score.  Each element of the vector indicates a
         score on a different "axis",  where each axis is used to derive a set
         of rank constraints g(x_i) - g(x_j) > 1. Use null as a placeholder if
         the x is not scored on a given axis.                                 
       - {tVAL} or {TVAL} sets the empirical risk scale.                      
       - {eVAL} or {EVAL} sets the distance to surface scale.                 
       - x is the training vector.                                            
                                                                              
Regression with Scoring:                                                      
                                                                              
       - {>,=,!=,!,<} may be included for scalar constraints.                 
       - {=,!} only for scoring constraints.                                  
       - y must be included.                                                  
       - y = scalar indicates a standard constraint g(x) ? y.                 
       - y = vector indicates a score as per classification with scoring.     
       - {tVAL} or {TVAL} sets the empirical risk scale.                      
       - {eVAL} or {EVAL} sets the distance to surface scale.                 
       - x is the training vector.                                            
                                                                              
Gentype regression:                                                           
                                                                              
       - {>,=,!=,!,<} must not be included.                                   
       - y must be included.                                                  
       - y can be real, vector, anion, string, equation, set or graph (as long
         as the concept of (inner) product (possibly numeric) can be defined).
       - y is projected onto the "u" basis (see -Aby etc below).              
       - {tVAL} or {TVAL} sets the empirical risk scale.                      
       - {eVAL} or {EVAL} sets the distance to surface scale.                 
       - x is the training vector.                                            
                                                                              
Planar regression:                                                            
                                                                              
       - {>,=,!=,!,<} defines constraint type:                                
         o >: v'.g(x) > y                                                     
         o =: v'.g(x) = y                                                     
         o <: v'.g(x) < y                                                     
         o ! or !=: ignore this vector (it may be referenced elsewhere).      
       - y must be included.                                                  
       - y is the scalar-valued target.                                       
       - {tVAL} or {TVAL} sets the empirical risk scale.                      
       - {eVAL} or {EVAL} sets the distance to surface scale.                 
       - x is the training vector of form x ::: 7:v.  v may either be a vector
         or an integer index to an (output) basis vector u_i. If v is a vector
         then it must lie in the space spanned by the output basis vectors.   
                                                                              
Multi-expert ranking:                                                         
                                                                              
       - Like planar  regression but automatically  tunes basis u_i.  The idea
         is  that  many   "experts"  give  their  opinions   (usually  ranking
         constraints, but not necessarily)  and we want to construct a machine
         that synthesises these (not  always compatible) sources.  Each expert
         is assigned to a  particular basis vector u_i, and  the inner product
         between two such vectors <u_i,u_j> reflects how similar experts i and
         j are in  their assessments.  The  machine attempts to  automatically
         tune these basis vectors - that is,  to learn expert similarity - and
         thereby  combine them  (essentially transfer  learning).   Evaluating
         g can  either define  which  "expert"  x is  aligned with -  that is,
         g(x ::: 7:i) for expert i - or give all alignments - that is, g(x).  
                                                                              
The format of the x vector may be  either sparse or nonsparse.  Sparse vectors
vectors have the form  (noting that commas can be  used instead of or combined
with whitespace):                                                             
                                                                              
<feature1>:<valueF1> <feature2>:<valueF2> ... <featureN>:<valueFN>            
                                                                              
with all other values being assumed zero.  Sparse vectors have the form:      
                                                                              
<value1> <value2> ... <valueN>                                                
                                                                              
The values may, depending  on context, be real, anionic,  vector, matrix, set,
strings (encompassed in  quotes ", or a single non-numeric  character which is
read as a string by default) or equations  (use of symbolic features may incur
a significant performance hit).  Notes:                                       
                                                                              
       - The product of two vectors is the inner-product of the vectors.      
       - The product of two strings is 1 if they are identical, 0 otherwise.  
       - The sum of two strings is the concatenation of them.                 
       - The product of two sets is the number of elements in common.         
       - The sum of two sets is the union, the difference the intersection.   
       - Simple equations like sqrt(20) will be evaluated directly.           
       - Distributions  (eg grand(0,1)) are  processed by the kernel  and then
         sampled as per Muandet et al, Learning from Distributions via Support
         Measure Machines (so a support  measure machine (SMM) can be run as a
         standard SVM with distributions in the x vectors).                   
       - Functions  are evaluated  like distributions,  where you  provide the
         distribution of relevant variables to the kernel.                    
       - Infinite-dimensional  vectors  (functions  treated  as  vectors)  are
         written [[ : f : d ]],  where f is a function of  x (eg sin(x)) and d
         d the dimension  of x.  Domain of  x is [0,1]^d.  These  can be  used
         like vectors: norms,  inner products  etc work  exactly as  you would
         expect in L2 space.                                                  
       - RKHS vectors  can't be  entered directly,  but they  do occur  (eg in
         Bayesian  optimisation).  Inner products  and norms on these occur in
         RKHS space (not L2),  but mixed inner  products with  inf-dim vectors
         are calculated in L2.  Note  that you can't  calculate sum/difference
         between an RKHS vector and an inf-dim vector.                        
       - Scalar functions are  also evaluated.  These are function of the form
         @(i,j,n):fn, where  fn is a  function, (i,j) defines  the variable in
         the function being treated as a scalar.  The result of the product of
         two scalar functions is the inner  product int_0^1 conj(f(x)).g(x) dx
         (if i != 0 and/or  j !=0 then x  is replaced  by var(i,j)),  which is
         numerically approximated over n steps.  The product of a vector and a
         function  in this  context is  the  same,  assuming the  vector is  a
         sampled version of the function on [0,1].  Default values for i,j and
         n are 0,0 and 100, respectively.                                     
                                                                              
For example the following training sets all define variants of the classic XOR
problem:                                                                      
                                                                              
       - Standard (y x format):                                               
                                                                              
         -1   -1 -1                                                           
         1    -1 1                                                            
         1    1  -1                                                           
         -1   1  1                                                            
                                                                              
       - Vector x (y x format):                                               
                                                                              
         -1    [ -1 1 ] [ -1 1 ]                                              
         1     [ -1 1 ] [ 1 -1 ]                                              
         1     [ 1 -1 ] [ -1 1 ]                                              
         -1    [ 1 -1 ] [ 1 -1 ]                                              
                                                                              
       - Symbolic x (y x format):                                             
         -1    "a" "a"                                                        
         1     "a" "b"                                                        
         1     "b" "a"                                                        
         -1    "b" "b"                                                        
                                                                              
       - Set x with symbolic elements (y x format):                           
                                                                              
         -1    { "cat" "horse" }     { "car" }                                
         1     { "cat" "chicken" }   { "lemons" "x"  }                        
         1     { "wallaby" "mouse" } { "walrus" "car" }                       
         -1    { "mouse" "wombat" }  { "q" "lemons" }                         
                                                                              
       - Distribution x (y x format):                                         
                                                                              
         -1    grand(-1,0.1) grand(-1,0.1)                                    
         1     grand(-1,0.1) grand(1,0.1)                                     
         1     grand(1,0.1)  grand(-1,0.1)                                    
         -1    grand(1,0.1)  grand(1,0.1)                                     
                                                                              
       - Scalar function x (y x format):                                      
                                                                              
         -1    @():sin(2*pi()*x) @():sin(2*pi()*x)                            
         1     @():sin(2*pi()*x) @():cos(2*pi()*x)                            
         1     @():cos(2*pi()*x) @():sin(2*pi()*x)                            
         -1    @():cos(2*pi()*x) @():cos(2*pi()*x)                            
                                                                              
More generally x has the form (using {...} to denote optional arguments here):
                                                                              
x {~ x1 {~ x2 ...}} {: xb {~ xb1 {~ xb2 ...}}} {:: e} {::: a}                 
                                                                              
where xb, e and a  have the same format as x.  These  define rank constraints,
gradient constraints, tuple format and augmented data as follows:             
                                                                              
Rank constraints: some  models (SVMs, GPs,  LSVs) allow the  inclusion of rank
         constraints - basically rather then enforcing g(x) ? y, they enforce:
                                                                              
         g(xa) - g(xb) ? y                                                    
                                                                              
         (where ? depends on model). In the training file these have the form:
                                                                              
         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} x : xb                  
                                                                              
         where x and xb have the same format as x above. Notes:               
                                                                              
       - you cannot  combine gradient  constraints and rank  constraints in  a
         single vector x (it's ambiguous whether the constraint's on x or xb).
       - when evaluating the model g(x : xb) = g(x) - g(xb)                   
       - this enables ordinal regression.                                     
                                                                              
Gradient constraints: some  models (scalar  regression SVM,  scalar regression
         LS-SVM, scalar regression GP) allow the inclusion of grad constraints
         of the form:                                                         
                                                                              
         (e'.d/dx) g(x) ? y                                                   
                                                                              
         making this a  constraint on the direction  derivative of the trained
         machine.  In the training file these have the form:                  
                                                                              
         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} x :: e                  
                                                                              
         where e has the same format as x.   Constraints on higher derivatives
         may also be applied using  augmented data (a_6) as described shortly.
         Note that:                                                           
                                                                              
       - you cannot  combine gradient  constraints and rank  constraints in  a
         single vector x (it's ambiguous whether the constraint's on x or xb).
       - when evaluating the model g(x :: e) = (e'.d/dx) g(x).                
                                                                              
Multi-Instance  format:  most  models  can  be  used  for  multi-instance  and
         similarity (kernel function)  learning.   In this case  vectors x are
         replaced by sets {x0,x1,...,xm-1},  and g(x) becomes  g({x0,x1,...}).
         In this case the training file has the form (eg m = 2):              
                                                                              
         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} x0 ~ x1 ~ x2 ~ ... ~ xn 
                                                                              
         where n is the number of vectors in the set.  Note that:             
                                                                              
       - the option -kn does not work with tuple format (but -knn does).      
       - n must not exceed 4095 (4096 vectors - defined in sparsevector.h).   
       - you cannot combine gradient  constraints and multi-instance data in a
         single vector x.                                                     
                                                                              
Multi-Task format: multi-task learning  via ICM kernels is  possible using the
         same format  as multi-instance  learning.  See -XT  for  information.
         Multi-task and multi-instance can be  combined, but interpretation is
         non-trivial: see code for details.                                   
                                                                              
Augmented data: in some cases additional data may be included using the form: 
                                                                              
         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} x ::: a                 
                                                                              
         where elements in a have the following interpretation:               
                                                                              
       - a_0:  replace x with  vector x_{a_0}.  If this is a vector  then x is
               replaced  by a tuple [ x_{a_0_0}  x_{a_0_1} ... ]. nul for dft.
       - a_1:  replace xb with vector x_{a_1}.  If this is a vector then xb is
               replaced  by a tuple [ x_{a_1_0}  x_{a_1_1} ... ]. nul for dft.
       - a_2:  replace e with  vector x_{a_2}. null for default.              
       - a_3:  reserved (like a_7, but refers to gentype regr basis. nul dft).
       - a_4:  diagonal kernel over-ride.  If  present in x1,x2 then kernel is
               replaced by delta_{x_1(a_4), x_2(a_4)} (0 if only one,nul dft).
       - a_5:  reserved (null for default).                                     
       - a_6:  define order  of gradient constraint (1 if  unspecified).  Grad
               constraint   becomes   (e'.dn/dxn) g(x) ? y,   where   dn/dxn =
               kronecker_power(d/dx,n).  For  example if xdim = 2 then this is
               d2/dx2 = [ d2/dx0.dx0 ; d2/dx0.dx1 ; d2/dx1.dx0 ; d2/dx1.dx1 ].
               You can evaluate  g(x) without  specifying e  in some models to
               find  the gradient  dn/dxn g(x)  (and likewise  (co)variances).
               Null for default.                                              
       - a_7:  vector (or index) part of planar constraint.  Null for default.
       - a_8:  set 1 to treat distributions as samples from sets, so distances
               are to the nearest in set, and  inner products to most similar.
               Null for default.                                              
       - a_{100q}: for ranking constraints in the vector-target case {>,=,...}
               applies to  all elements of the target.  To  override for dim q
               for a given training pair use  this, specifically a_{100q} = -1
               means <, 0 means !,+1 means > and +2 means =.                  
                                                                              
Pipes: - Most output sent to standard error.                                  
       - Help sent to standard out.                                           
       - All other output sent direct to files.                               
                                                                              
Matlab/mex: - Standard out and standard error redirected to mexprintf.        
       - Logfiles written and contents mirrored by matlab variables.          
                                                                              
Arguments: in the descriptions below there are three types of arguments:      
       - Strings:  indicated by  leading $  (eg $file,  $fn).  These  must not
         contain whitespace, but note that any occurance of _ will be replaced
         by a space.                                                          
       - Sets: held in curly  brackets {} (eg {0,1,2}).  The  argument must be
         one of the options in the list.                                      
       - Variables: everything  else.  These  may be integers,  reals, vectors
         (eg [ 1 2 3.2 ] or [ 0:0.12 1:39 5:2 ]) or anions (eg 1.0i) depending
         on context.                                                          
                                                                              
Variable evaluation: a useful feature here  is that all variable arguments are
         evaluated.  That is,  you can enter them as  equations - for example,
         if you enter  2/3 this will  be evaluated  to 0.66666 -  and moreover
         those equations can have arguments  of the form var(i,j), where i and
         j are  non-negative integers (and  shortcuts x,y,z,v,w,g =  var(0,0),
         var(0,1),...,var(0,5) and h= var(42,42) (the current ML)). A complete
         list is  included (-??v).                                            
                                                                              
Global functions: these  may be used to  retrieve data  about ML  models.  Use
         fnA(h,i) to  retrieve value i  (eg C,  if i = 0) for the  current ML,
         which is stored in variable h).  Replace h with specific ML number to
         specify a different ML.  A complete list is included (-??v).         
                                                                              
Variables: a complete  list of vars is  given at the end  of this help.  Three
         functions  are  particularly  note-worthy  with regard  to variables,
         namely -fV n $f  and -fW n f,  which let  you set  var(0,n) = f (with
         -fV being unevaluated  and -fW evaluated), and  -echo x, which simply
         evaluates x and echoes it to standard out.                           
                                                                              
Comments: comments  can be  included in  argument streams,  which is  handy if
         you want to put commands in a  file.  Comments are C style - that is,
         /* comment goes here and gets ignored */.  Note that comments may not
         interupt commands, so for example  the sequence "-c 1 /* set c */" is
         good, but "-c /* set c */" 1 is a syntax error.                      
                                                                              
Help options (run when encountered):                                          
                                                                              
         -?              - basic help.                                        
         -??             - advanced help.                                     
         -??k            - list of all available kernel functions.            
         -??v            - variable assignment table.                         
         -??g            - variable types, functions etc.                     
         -???            - print blank lines to standard error.               
                                                                              
General options (run first):                                                  
                                                                              
         -v   {0,1}      - verbosity level (default 1).                       
                           0 = minimal - cerr feedback only.                  
                           1 = normal - write details to logfile.log.         
         -L   $file      - string used to  derive logfile  and ML description.
                           Files will be file.log, file.svm etc.              
         -LL  file       - string used to  derive logfile  and ML description.
                           Files will be file.log, file.svm etc.              
                                                                              
Multi-ML options (after general options):                                     
                                                                              
                  ** SVMHeavy can work on  arbitrarily many MLs at **         
                  ** once.  Each  ML is assigned  an index n >= 1. **         
                  ** The  working  ML  (default  index 1)  defines **         
                  ** which of  these is  operated on by  all other **         
                  ** commands.  Note that copying and swapping may **         
                  ** be very  slow for large MLs.  The  current ML **         
                  ** is h.                                         **         
                                                                              
         -qR  n          - delete ML  n completely.  If  n is working  ML then
                           return to  clean-slate state (no  data, all default
                           settings).                                         
         -qc  n m        - overwrite ML n with copy of ML m.                  
         -qs  n m        - swap ML n and ML m.                                
         -qw  n          - set ML n  as current (working) ML.   If n  not used
                           previously then create clean-slate ML first.       
         -qpush n        - push current ML index onto stack, run -qw n.       
         -qpop           - push ML index n off stack, run -qw n.              
                                                                              
Pre-Setup options (after multi-ML options):                                   
                                                                              
         -zl  $file      - preload the ML from file.                          
         -z   {...}      - ML type:                                           
                                                                              
                               Support Vector Machines (SVM):                 
                                                                              
                           s - SVM: single class.                             
                           c - SVM: binary classification (default).          
                           m - SVM: multiclass classification.                
                           r - SVM: scalar regression.                        
                           v - SVM: vector regression.                        
                           a - SVM: anionic regression.                       
                           u - SVM: cyclic regression.                        
                           g - SVM: gentype regression (any target).          
                           e - SVM: auto-encoding SVM.                        
                           p - SVM:*density estimation.                       
                           t - SVM: pareto frontier SVM.                      
                           l - SVM: binary scoring (zero bias by default).    
                           o - SVM: scalar regression with scoring.           
                           q - SVM: vector regression with scoring.           
                           i - SVM: planar regression.                        
                           h - SVM: multi-expert ranking.                     
                           j - SVM: multi-expert binary classification.       
                           b - SVM: similarity learning.                      
                                                                              
                               *Uses  1-norm cost,  kernel can be  non-Mercer.
                                Recommend setting r0 = N, norm with -Sna.     
                                                                              
                               Least-squares SVMs (LSV):                      
                                                                              
                         lsr - LSV: scalar regression.                        
                         lsv - LSV: vector regression.                        
                         lsa - LSV: anionic regression.                       
                         lsg - LSV: gentype regression.                       
                         lsi - LSV: planar regression.                        
                         lso - LSV: scalar regression with scoring.           
                         lsq - LSV: vector regression with scoring.           
                         lsh - LSV: multi-expert ranking.                     
                         lse - LSV: auto-encoding machine.                    
                                                                              
                               Super-Sparse support vector machines (SSV):    
                                                                              
                         sss - SSV: single class.                             
                         ssc - SSV: binary classification.                    
                         ssr - SSV: scalar regression.                        
                                                                              
                               Gaussian processes (GPR):                      
                                                                              
                         gpr - GPR: gaussian process scalar regression.       
                         gpv - GPR: gaussian process vector regression.       
                         gpa - GPR: gaussian process anionic regression.      
                         gpg - GPR: gaussian process gentype regression.      
                         gpc - GPR: gaussian process binary classification.*  
                                                                              
                               *Uses expectation propagation (EP).            
                                                                              
                               Type-II Multi-Layer Kernel Machines (MLM):     
                                                                              
                         mlr - MLM: Type-II MLK machine scalar regression.    
                         mlc - MLM: Type-II MLK machine scalar regression.    
                         mlv - MLM: Type-II MLK machine scalar regression.    
                                                                              
                         NB: MLM  is  extremely  experimental.   Binary  might
                             work (probably not), but none of the others do.  
                                                                              
                               K-nearest-neighbour networks (KNN):            
                                                                              
                         knc - KNN: binary classification.                    
                         knm - KNN: multiclass classification.                
                         knr - KNN: scalar regression.                        
                         knv - KNN: vector regression.                        
                         kna - KNN: anionic regression.                       
                         kng - KNN: gentype regression.                       
                         kne - KNN: auto-encoder.                             
                         knp - KNN: density estimation.                       
                         kne - KNN: auto-encoding machine.                    
                                                                              
                               One-layer Neural Networks (ONN):               
                                                                              
                         onc - ONN: binary classification.                    
                         onr - ONN: scalar regression.                        
                         onv - ONN: vector regression.                        
                         ona - ONN: anionic regression.                       
                         one - ONN: auto-encoding machine.                    
                         ong - ONN: gentype machine.                          
                                                                              
                               Improvement measures (IMPs):                   
                                                                              
                          ei - IMP: expected (hypervolume) improvement.       
                         svm - IMP: 1-norm 1-class modded SVM mono-surrogate. 
                                                                              
                               Learning blocks and glue (BLK):                
                                                                              
                         nop - BLK: NOP machine.                              
                         mer - BLK: Mercer kernel inheritance block.          
                         con - BLK: consensus machine.                        
                         fna - BLK: user function machine (elementwise).*     
                         fnb - BLK: user function machine (vectorwise).*      
                         mxa - BLK: mex function machine (elementwise).       
                         mxb - BLK: mex function machine (vectorwise).        
                          io - BLK: user I/O machine.                         
                         sys - BLK: system call machine.                      
                         avr - BLK: scalar averaging machine.                 
                         avv - BLK: vector averaging machine.                 
                         ava - BLK: anionic averaging machine.                
                         fcb - BLK: function callback (do not use).           
                         ber - BLK: Bernstein basis polynomial.               
                         bat - BLK: Battery model.**                          
                         ker - BLK: kernel specialisation.                    
                         mba - BLK: multi-block sum.                          
                                    (g(x), kernel transfer ave over multi MLs)
                                                                              
                               *This function can be a  distribution, in which
                                case g(x) is a sample from  this distribution.
                                You can "freeze"  this (ie. take a  sample and
                                return it consistently afterwards) by sampling
                                using -St.                                    
                                                                              
                              **Battery model as per Cer1.  Evaluation is:    
                                                                              
                                g([3 i(x) v s])= time to reach target  voltage
                                                with charging current i(t). If
                                                target not reached then result
                                                is max time + s.Verr.         
                                g([2 v(x) v s])= time to reach target  voltage
                                                with charging voltage v(t).   
                                g([1 p(x) v s])= time to reach target  voltage
                                                with charging power p(t).     
                                g([0 i(x) v s])= time to reach target  voltage
                                                with discharging current i(t).
                                g([-1 t i v s]) = given  vectors    time  (t),
                                                current (i),  voltage (v), for
                                                current  charging, return  how
                                                close the simulation is to the
                                                given data (L2 voltage error).
                                g([ -2 dfile m N s ]) = lie g([ -1 ... ]), but
                                                data is in a file dfile.  m is
                                                the startpoint in  the file, N
                                                is the  max number  of obs  to
                                                compare (-1 for all), s is the
                                                scalarisation.  Result is:    
                                                                              
                                                s*earlystop + ave_error       
                                                                              
                                                where ave_error is the average
                                                voltage  error  and  earlystop
                                                is the number  of observations
                                                skipped  at  the  end  due  to
                                                model failure.                
                                                                              
                           Using this  function at  any point  will completely
                           remove any existing ML.                            
         -zd  {...}      - like  -z,  but keeps  data.   Note that  this is  a
                           potentially lossy  function, and  may give an error
                           for  incompatible types  (eg going  from regression
                           to classification is not possible).  Moreover there
                           is a degree  of guesswork  involved, so  don't rely
                           too much on this option.                           
                                                                              
                  -- SVM specific options                          --         
                                                                              
         -zv  {once,red} - vectorial SVM type.  Modes are:                    
                           once   - at-once regression.                       
                           red    - reduction to binary regression (default). 
         -zc  {1vsA,1vs1,- multiclass classifier type.  Modes are:            
              DAG,MOC,     1vsA   - 1 versus all (reduction to binary).       
              maxwin,      1vs1   - 1 versus 1 (reduction to binary).         
              recdiv}      DAG    - directed acyclic graph (reduct to binary).
                           MOC    - minimum output coding (reduct to binary). 
                           maxwin - max-wins SVM (at once).                   
                           recdiv - recursive division SVM (at once, default).
         -zo  {sch,tax}  - one-class SVM method:                              
                           sch    - Scholkopf 1999 type (default).            
                           tax    - Tax  and   Duin,   "Support  Vector   Data
                                    Description" (SVDD), Machine Learning, 54,
                                    2004.                                     
                                                                              
Setup options (after pre-setup options):                                      
                                                                              
         -N   n          - hint of expected training  set size (this will help
                           minimise memory usage and duplication overhead).   
                                                                              
         -fo  n $file    - open file for  future processing  using placeholder
                           n.  The  file may  then  be  used for  training and
                           testing.  Variable var(0,n) will contain the number
                           of vectors remaining in  the file.  If n is already
                           in use then the  current file will be  closed and a
                           new file opened.                                   
         -foe n $file    - target-at-end version of -fo.                      
                                                                              
         -fret n m       - tag variable  var(n,m) to be retained  after return
                           from for example -MF or -g, -gb etc.               
         -fV  n $fn      - set argument var(0,n) = $fn (not evaluated).       
         -fW  n v        - set argument var(0,n) = v (evaluated).             
         -fWW n v        - set argument var(0,n) = v   (evaluated   but    not
                           finalised, so  for example  random numbers  are not
                           drawn from and globals are not evaluated.          
         -fru n          - set argument var(0,n) = uniform random U(0,1).     
         -frn n          - set argument var(0,n) = gaussian random N(0,1).    
         -fri n          - set argument var(0,n) = random integer.            
         -fM  n args     - set argument var(130,n) =  string  of  args.   args
                           should be enclosed in {}.  These are used as macros
                           later.                                             
                                                                              
         -fu  n i x      - single-objective  test function  evaluation.  Given
                           decision  vector  x evaluates  test  function  i to
                           produce output that is stored in variable var(0,n).
                           These functions can also be accessed in expressions
                           using fnB(-1,i,x) and fnC(-2,i,x,A).  Problem specs
                           are as follows (d = dim(x), c/f Wikipedia):        
                                                                              
+-------------------------------+---+------------------------+---------------+
|  i: Function name             | d | Range                  | Minimum       |
+-------------------------------+---+------------------------+---------------+
|  1: Rastrigin function        | d | -5.12   <= x_i <= 5.12 | f(0)     = 0  |
|  2: Ackley's function         | d | -5      <= x_i <= 5    | f(0)     = 0  |
|  3: Sphere function           | d | -inf    <= x_i <= inf  | f(0)     = 0  |
|  4: Rosenbrock function       | d | -inf    <= x_i <= inf  | f(1)     = 0  |
|  5: Beale's function          | 2 | -4.5    <= x,y <= 4.5  | f(3,0.5) = 0  |
|  6: Goldstein-Price function  | 2 | -2      <= x,y <= 2    | f(0,-1)  = 3  |
|  7: Booth's function          | 2 | -10     <= x,y <= 10   | f(1,3)   = 0  |
|  8: Bukin function N.6        | 2 | -15,-3  <= x,y <= -5,3 | f(-10,1) = 0  |
|  9: Matyas function           | 2 | -10     <= x,y <= 10   | f(0,0)   = 0  |
| 10: Levi function N.13        | 2 | -10     <= x,y <= 10   | f(1,1)   = 0  |
| 11: Himmelblau's function     | 2 | -5      <= x,y <= 5    | f(s,t)   = 0  |
| 12: Three-hump camel function | 2 | -5      <= x,y <= 5    | f(0,0)   = 0  |
| 13: Easom function            | 2 | -100    <= x,y <= 100  | f(pi,pi) = -1 |
| 14: Cross-in-tray function    | 2 | -10     <= x,y <= 10   | f(a,a)   = b  |
| 15: Eggholder function        | 2 | -512    <= x,y <= 512  | f(c,d)   = e  |
| 16: Holder table function     | 2 | -10     <= x,y <= 10   | f(f,f)   = g  |
| 17: McCormick function        | 2 | -1.5,-3 <= x,y <= 4,4  | f(h,j)   = k  |
| 18: Schaffer function N. 2    | 2 | -100    <= x,y <= 100  | f(0,0)   = 0  |
| 19: Schaffer function N. 4    | 2 | -100    <= x,y <= 100  | f(0,l)   = m  |
| 20: Styblinski-Tang function  | d | -5      <= x_i <= 5    | q <= f(p) <= r|
| 21: Stability test function 1 | 1 | 0       <= x   <= 1    | f(0.2)   = 1.3|
|     (also has unstable max at |   |                        |               |
|     f(0.5) = 1.65 (2nd order) |   |                        |               |
|     and f(1) = 1.5 (1st))     |   |                        |               |
| 22: Stability test function 2 | 1 | 0       <= x   <= 1    | f(1)     = 4  |
|     Sum  of   two  gaussians, |   |                        | f(0.5)   = 1  |
|     gamma  =   1/(5.sqrt(2)), |   |                        |               |
|     centres  at  1  (alpha 4) |   |                        |               |
|     and 0.5  (alpha  1).  Use |   |                        |               |
|     A = 0.2, B = 0.05, pmax=2 |   |                        |               |
| 23: Sum   of  RBFs   function | d | 0       <= x   <= 1    | depends       |
|     sum_i a_{i,0} exp(-(||x-a_{i,2:...}||_2^2)/(2*a_{i,1}*a_{i,1}))        |
| 10xx: function xx, normalised | d | -1      <= x   <= 1    | depends       |
|     (nominally) so  -1<=x<=1, |   |                        |               |
|     0<=f(x)<=1                |   |                        |               |
+-------------------------------+---+------------------------+---------------+
                                                                              
                           Stability test function:                           
                                                                              
                           f(x) = exp(-20*(x-0.2)*(x-0.2))                    
                                + exp(-20*sqrt(0.00001+((x-0.5)*(x-0.5))))    
                                + exp(2*(x-0.8))                              
                                                                              
                           Constants: a = +-1.34941                           
                                      b = -2.06261                            
                                      c = 512                                 
                                      d = 404.2319                            
                                      e = -959.6407                           
                                      f = +-8.05502                           
                                      g = -19.2085                            
                                      h = -0.54719                            
                                      j = -1.54719                            
                                      k = -1.9133                             
                                      l = 1.25313                             
                                      m = 0.292579                            
                                      p = -2.903534                           
                                      q = -39.16617n                          
                                      r = -39.16616n                          
                                      (s,t) = (3,2), (-2.805,3.131),          
                                          (-3.779,-3.283), (3.584,-1.848)     
                                                                              
         -fuu n i x a    - like -fu with additional user parameter matrix a.  
         -ft  n i M x    - multi-objective  test  function  evaluation.  Given
                           decision  vector x  evaluates  test  function i  to
                           produce  an  M-dimensional  output  vector  that is
                           stored  in  variable  var(0,n).  These can  also be
                           accessed via fnC(-3,i,x,M). Available tst functions
                           are (sources SCH1,DTLZ,FON1 and Wikipedia):        
                                                                              
+-------------+---+-----+-----------------------------------------+----------+
|  i: Fn name | n | M   | Function                                | Range    |
+-------------+---+-----+-----------------------------------------+----------+
|  1: DTLZ1   | n | <=n | [ x0...xM-2.(1+g(z))/2          ; ]     | 0<=x<=1  |
|             |   |     | [ x0...xM-3.(1-xM-2).(1+g(z))/2 ; ]     |          |
|             |   |     | [   ...                         ; ]     |          |
|             |   |     | [ x0...(1-x1).(1+g(z))/2        ; ]     |          |
|             |   |     | [ (1-x0).(1+g(z))/2               ]     |          |
|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |
|             |   |     | g(z) = 100.( #(z) + sum_i ( (zi-0.5)^2  |          |
|             |   |     |               - cos(20*pi*(zi-0.5)) ) ) |          |
|  2: DTLZ2   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |
|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |
|             |   |     | [   ...                               ] |          |
|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |
|             |   |     | [ (1+g(z)).s(x0)                      ] |          |
|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |
|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |
|             |   |     | s(x) = sin( x.pi/2 )                    |          |
|             |   |     | c(x) = cos( x.pi/2 )                    |          |
|  3: DTLZ3   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |
|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |
|             |   |     | [   ...                               ] |          |
|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |
|             |   |     | [ (1+g(z)).s(x0)                      ] |          |
|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |
|             |   |     | g(z) = 100.( #(z) + sum_i ( (zi-0.5)^2  |          |
|             |   |     |               - cos(20*pi*(zi-0.5)) ) ) |          |
|             |   |     | s(x) = sin( x.pi/2 )                    |          |
|             |   |     | c(x) = cos( x.pi/2 )                    |          |
|  4: DTLZ4   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x1<=1 |
|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] | -5<=xi<=5|
|             |   |     | [   ...                               ] | 2<=i<=n  |
|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |
|             |   |     | [ (1+g(z)).s(x0)                      ] |          |
|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |
|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |
|             |   |     | s(x) = sin( (x^alpha).pi/2 )            |          |
|             |   |     | c(x) = cos( (x^alpha).pi/2 )            |          |
|  5: DTLZ5   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |
|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |
|             |   |     | [   ...                               ] |          |
|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |
|             |   |     | [ (1+g(z)).s(x0)                      ] |          |
|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |
|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |
|             |   |     | s(x) = sin( theta.pi/2 )                |          |
|             |   |     | c(x) = cos( theta.pi/2 )                |          |
|             |   |     | theta = (pi/(4.(1+g(z)))).(1+2.g(z).xi) |          |
|  6: DTLZ6   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |
|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |
|             |   |     | [   ...                               ] |          |
|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |
|             |   |     | [ (1+g(z)).s(x0)                      ] |          |
|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |
|             |   |     | g(z) = sum_i zi^0.1                     |          |
|             |   |     | s(x) = sin( theta.pi/2 )                |          |
|             |   |     | c(x) = cos( theta.pi/2 )                |          |
|             |   |     | theta = (pi/(4.(1+g(z)))).(1+2.g(z).xi) |          |
|  7: DTLZ7   | n | <=n | [ x0                              ; ]   | 0<=x<=1  |
|             |   |     | [ x1                              ; ]   |          |
|             |   |     | [   ...                             ]   |          |
|             |   |     | [ xM-2                            ; ]   |          |
|             |   |     | [ (1+g(z)).h(f1,f2,...,fM-2,g(z))   ]   |          |
|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |
|             |   |     | g(z) = 1 + (9/#(z)) sum_i zi            |          |
|             |   |     | h = M-sum_i((fi/(1+g)).(1+sin(3pi.fi))) |          |
|  8: DTLZ8   | n | <n  | [ sum_ib^is xi ], i = 0,1,...,M-1       | 0<=x<=1  |
|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |
|             |   |     |        ib = floor(i*n/M)-1              |          |
|             |   |     |        is = floor((i+1)*n/M)-1          |          |
|             |   |     | **constraints are not implemented yet.  |          |
|  9: DTLZ9   | n | <n  | [ sum_ib^is xi^0.1 ], i = 0,1,...,M-1   | 0<=x<=1  |
|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |
|             |   |     |        ib = floor(i*n/M)-1              |          |
|             |   |     |        is = floor((i+1)*n/M)-1          |          |
|             |   |     | **constraints are not implemented yet.  |          |
| 10: FON1    | n | 2   | [ 1-exp(-|| x - 1/sqrt(n) ||^2) ; ]     | -4<=x<=4 |
|             |   |     | [ 1-exp(-|| x + 1/sqrt(n) ||^2)   ]     |          |
| 11: SCH1    | 1 | 2   | [ x^2     ; ]                           | free     |
|             |   |     | [ (x-2)^2   ]                           |          |
| 12: SCH2    | 1 | 2   | [ { -x     if     x <= 1 } ]            | -5<=x<=10|
|             |   |     | [ { x-2    if 1 < x <= 3 } ]            |          |
|             |   |     | [ { 4-x    if 3 < x <= 4 } ]            |          |
|             |   |     | [ { x-4    if 4 < x      } ]            |          |
|             |   |     | [                          ]            |          |
|             |   |     | [ (x-5)^2                  ]            |          |
+-------------+---+-----------------------------------------------+----------+
                                                                              
                           SCH1: Schaffer, J. D.:  Some Experiments in Machine
                                 Learning   using  Vector   Evaluated  Genetic
                                 Algorithms - PhD Thesis, 1984.               
                           DTLZ: Deb,   Kalyanmoy  and   Thiele,  Lothar   and
                                 Laumanns,   Marco    and   Zitzler,   Eckart:
                                 "Scalable  Test  Problems   for  Evolutionary
                                 Multiobjective Optimization".                
                           FON1: Fonseca,  C.  M.  and   Fleming,  P.  J.:  An
                                 Overview of Evolutionary Algorithms in Multi-
                                 Objective     Optimisation.      Evolutionary
                                 Multiobjective   Optimisation,    Theoretical
                                 Advances and Applications, pg. 105-145, 2005.
                                 (as re-interpretted in DTLZ for n-dim).      
                                                                              
         -fat alpha      - sets alpha value used by DTLZ4 in -ft evaluation.  
                                                                              
                  ** There are two  types of variable:  global and **         
                  ** local.  Local  variables are  specific to and **         
                  ** available everywhere  within a single thread. **         
                  ** Global  variables  are  accessible  from  all **         
                  ** current running threads,  allowing for inter- **         
                  ** thread  communications.    Use  local  unless **         
                  ** strictly necessary.                           **         
                                                                              
         -fVg  n $fn     - like -fV  but reads from global variables.         
         -fWg  n v       - like -fW  but reads from global variables.         
         -fVG  n $fn     - like -fV  but writes to global variables.          
         -fWG  n v       - like -fW  but writes to global variables.          
         -fuuG n i x a   - like -fuu but writes to global variables.          
         -fuG  n i x     - like -fu  but writes to global variables.          
         -ftG  n i M x   - like -ft  but writes to global variables.          
                                                                              
         -br  d          - binary   class  label:  by   default,   for  binary
                           classification,  class labels are  -1 and +1.  This
                           option lets  you automatically  re-label vectors as
                           they are  loaded from  a file,  so class  d becomes
                           class +1 and  all other classes  are relabelled -1.
                           (default 0, no relabelling).                       
         -bd  d          - class skipping: for classification, if this is non-
                           zero then points from  this class coming from files
                           will be silently ignored. This is handy if you want
                           to ignore a  particular class in a  multiclass case
                           (default 0, no skipping).                          
                                                                              
         -XT  [ x ]      - set x template  (sparse).  If there are elements in
                           this template  that are not  in a  testing/training
                           vector  then they will  be added.  This  is helpful
                           for multitask  learning.  Use -XT [ ~ i ],  where i
                           is the  task number, and  use  -kS -ks 2  -ki 0 ...
                           -ki 1 -kt 48 -kr r1, where ...  is the usual kernel
                           setup and r1 is  the similarity (scalar  or matrix)
                           between tasks.                                     
                                                                              
                  -- MEX (Matlab) only options                     --         
                                                                              
         -fWm n $fn v    - set argument var(0,n) = fn(v), where fn is a MATLAB
                           function.  Use v = null if no args required, set if
                           more than one arg required.                        
         -fWM n i v      - set argument var(0,n) = fni(v) (v evaluated), where
                           fni is a MATLAB function  handle specified when you
                           called svmmatlab.   If fni if a  variable and not a
                           handle then result is value of variable.           
                                                                              
                  -- SVM specific options                          --         
                                                                              
         -ac  {svc,svr}  - classification method:                             
                           svc: normal SVM classifier (default).              
                           svr: classify via regression.                      
         -B   {f,v,p,n}  - bias type:                                         
                           f - fixed bias (bias value defaults to 0).         
                           v - variable bias (default).                       
                           p - positive bias.                                 
                           n - negative bias.                                 
         -R   {l,q,o,g,G}- empirical risk type.                               
                           l - linear (default).                              
                           q - quadratic.                                     
                           o - linear cost, but with  1-norm regularisation on
                               alpha  (not feature  space:  use -m  for that).
                               (to  enforce 1-norm  regularisation  with hard-
                               margin use this in combination with -c 1e20).  
                           g - generalised linear cost (iterative fuzzy).     
                           G - generalised quadratic cost (iterative fuzzy).  
                           Note  that  quadratic  quadratic empirical  ignores
                           values set by -c+, -c-, -c=, -cc and -jc. Note also
                           that epsilon insensitivity  is used for both linear
                           and quadratic cases, so  to construct an LS-SVR you
                           need to set -R q -w 0.                             
         -T   {f,s}      - tube type:                                         
                           f - fixed tube (default).                          
                           s - tube shrinking on.                             
                                                                              
                  -- LSV specific options                          --         
                                                                              
         -bv             - LSV variable bias (default).                       
         -bz             - LSV zero bias.                                     
                                                                              
                  -- GPR specific options                          --         
                                                                              
         -bgv            - GPR variable bias.                                 
         -bgz            - GPR zero bias (default).                           
                                                                              
                  -- MLM specific options                          --         
                                                                              
         -mlR n {l,q}    - MLM regularisation type at layer n:                
                           l - 1-norm.                                        
                           q - 2-norm.                                        
         -mls n          - Set number of layers (not including output, dft 0).
                                                                              
                  -- SSV specific options                          --         
                                                                              
         -sR  {l,q}      - beta  regulation  type.   In SSV  models  the  beta
                           regularisation term is set by this:                
                           l - linear: sigma.||beta||_1.                      
                           q - quadratic: sigma.||beta||_2^2 (default).       
                                                                              
                  -- BLK specific options                          --         
                                                                              
         -mc  n          - (Mercer kernel inheritance block): sets the size of
                           the kernel cache (if any) stored inside this block.
                           Unlike  other  caches  this one  is  simply  an n*n
                           matrix, and needs to be manually flushed for kernel
                           changes etc.  Set  -1 for no cache.  This  does not
                           store 4-kernel  or m-kernel (m>2)  evaluations.  To
                           clear the cache reuse -mc n flag.                  
         -mcn {0,1}      - (Mercer  kernel inheritance  block):  normalisation
                           on (1) or off (0, default).                        
                                                                              
         -mba i n        - (Multi-block average): set ML n as element i.      
         -mbA i          - (Multi-block average): remove ML element i.        
         -mbw i w        - (Multi-block average): set ML n weight as w.       
                                                                              
         -msn n          - set order of Bernstein basis b_{i,n}(x).           
         -msw i          - set index of Bernstein basis b_{i,n}(x).           
                                                                              
         -bat p          - Set battery parameters.  p is a vector:            
                                                                              
                                          (defaults)                          
                                                                              
                         [ C_0star ] Battery capacity at Istar (261.9 Ah)     
                         [ K_c     ] Battery parameter (1.18)                 
                         [ theta_f ] Electrolyte freezing temp (-40 degree C) 
                         [ epsilon ] Battery parameter (1.29)                 
                         [ delta   ] Battery parameter (1.40)                 
                         [ Istar   ] Nominal battery current (49 A)           
                         [         ]                                          
                         [ E_m0    ] Battery voltage at full charge (2.135 V) 
                         [ K_E     ] Battery parameter (0.58e-3 V/degree C    
                         [ tau_1   ] Battery parameter (5000 sec)             
                         [ R_00    ] Battery parameter (2e-3 ohm)             
                         [ R_10    ] Battery parameter (0.7e-3 ohm)           
                         [ R_20    ] Battery parameter (15e-3 ohm)            
                         [ A_0     ] Battery parameter (-0.3)                 
                         [ A_21    ] Battery parameter (-8.0)                 
                         [ A_22    ] Battery parameter (-8.45)                
                         [         ]                                          
                         [ E_p     ] Battery parameter (1.95 V)               
                         [ V_p0    ] Battery parameter (0.1 V)                
                         [ A_p     ] Battery parameter (2.0)                  
                         [ G_p0    ] Battery parameter (2e-12 sec)            
                         [         ]                                          
                         [ C_theta ] Battery parameter (15 Wh/degree C)       
                         [ R_theta ] Battery parameter (0.2 degree C/W)       
                                                                              
                           use null to leave parameter as-is.                 
                                                                              
         -bam p          - Set time period for battery model (sec, dflt 3600).
         -bac p          - Set max (dis)charge current (amps, default 30).    
         -bad d          - Set time-step for battery model (sec, dflt 0.05).  
         -bav v          - Set start voltage for battery model (volts, 2.135).
         -baT t          - Set ambient/start temperture for battery (deg, 20).
                                                                              
                                                                              
Preload options (after setup options):                                        
                                                                              
         -pR             - reset the ML to initial state (keeps training dat).
         -pRR            - restart the ML (clean-slate, defaults, no data).   
         -pr  i          - remove training vector i.                          
         -pro n          - remove the first n training vectors.               
         -psz i z        - set target z for training vector i.                
         -pcw i C        - set C>0 weight for training vector i.              
         -pcs s          - scale all preloaded C weights (t/TVALs) by s>0.    
         -pww i eps      - set eps>=0 weight for training vector i.           
         -pws s          - scale all preloaded eps weight (e/EVALs) by s>=0.  
         -ps  s          - scale whole  ML by s>=0.  The exact meaning of this
                           depends on the ML type as follows:                 
         -pS             - scale to ensure that abs2(alpha) = 1.              
                                                                              
                           SVM: alpha and b are scaled by s.                  
                           LSV/SSV: like SVM.                                 
                           GPR: y and K (kernel weight) are scaled by s.      
                           BLK consensus: scale all parts.                    
                           Others: may or may not be implemented, see code.   
                                                                              
         -fic            - fill caches. This can be handy if you have a Mercer
                           kernel  inheritance block  (kernel cache)  that you
                           want to fill for a specific setting before modding.
                                                                              
         -pk  $file      - load kernel matrix from file (bypass kernel).  File
                           format is  MATLAB style matrix, []s  must included.
                           This may be handy for  multi-instance learning with
                           many (1000s) of instances  in some examples, as for
                           reasons unknown this can crash during kernel eval. 
                                                                              
                  -- SVM specific options                          --         
                                                                              
         -prz            - remove non-support vectors.                        
         -prm n          - like  -prz, but  continue removing  support vectors
                           until the number of vectors reaches at max n.      
         -psd i d        - set class d for training vector i.                 
                                                                              
                  -- GP specific options                           --         
                                                                              
         -pdw i C        - set C>0 weight for training vector i.              
         -pds s          - scale all preloaded C weights (t/TVALs) by s>0.    
                                                                              
Load options (adding training vectors, after preload options):                
                                                                              
         -AA  $file      - add vectors from fname.                            
         -AN  i j k $file -add vectors from file, ignoring i vectors at start,
                           adding at  most j vectors  (-1 if all),  and adding
                           vectors starting from index k (-1 to add to end).  
                                                                              
         -AU  d [x]      - add training vector x,  target d.  For single-class
                           ML, target d is ignored  but must still be present.
                           x must be an (optionally sparse) enclosed vector.  
                           eg -AU -1 [ 0:1 1:-1 ]                             
                           or, in non-sparse notation, -AU -1 [ 1 -1 ]        
         -AY  d x        - like -AU, but x is any vector.                     
         -AZ  d x m      - like -AU, but x is any vector and ML m.            
         -AV  [d] [[x]]  - add multiple training vectors, eg                  
                     -AV [ -1 -1 1 1 ] [ [ -1 -1 ] [ 1 1 ] [ 1 -1 ] [ -1 1 ] ]
                                                                              
         -AGl l          - Set lower bound (scalar or vector) for -AG, -AGc). 
         -AGu u          - Set upper bound (scalar or vector) for -AG, -AGc). 
         -Ag  N d f v    - generate and add training data.  N pairs generated,
                           vectors have dim d, function is f with noise var v,
                           features N(0,1).                                   
         -AG  N d f v    - generate and add training data.  N pairs generated,
                           vectors have dim d, function is f with noise var v,
                           features U(l,u).                                   
         -Agc N d f v c  - like -Ag, but only adds data when c(x) == 1.       
         -AGc N d f v c  - like -AG, but only adds data when c(x) == 1.       
                                                                              
         -AVv d x        - add multiple training vectors with  target vector d
                           and input  vectors x, eg  -AVv var(81,0)  var(80,0)
                           after -tx will train a model on LOO error.         
         -AVV d x s      - add multiple training vectors with target vector d,
                           input vectors x  and sigma weights s.  For  example
                           you might use var(83,0) in the above example.      
                                                                              
                  ** For target-at-end  format files, the e suffix **         
                  ** can be used.  So for example:                 **         
                  **                                               **         
                  ** -AA  trainfile   (target-at-start format)     **         
                  ** -AAe trainfile   (target-at-end format)       **         
                                                                              
                  ** To add training vectors  from open files, use **         
                  ** the i suffix and replace  the filename with n **         
                  ** (the file number).                            **         
                                                                              
                  ** To  add  training  vectors  from  open  files **         
                  ** but leave them so they can be reused, use the **         
                  ** I/R suffixes (just like i/r suffixes).        **         
                                                                              
                  ** To add training vectors randomly from an open **         
                  ** file, use the r suffix instead of i.          **         
                                                                              
                  ** As labels/targets  may not always  be present **         
                  ** training files,  the l suffix can  be used to **         
                  ** apply a label to all vectors in an unlabelled **         
                  ** training file (ie. a file containing only x). **         
                  ** General format is:                            **         
                  **                                               **         
                  ** {usual flag}l {usual options} y               **         
                  **                                               **         
                  ** where y is the required class/target.  eg.    **         
                  **                                               **         
                  ** -AA trainfile   becomes   -AAl trainfile y    **         
                  ** -ANe i j k fle  becomes   -ANel i j k fle y   **         
                                                                              
                  ** If the training file  has labels that are not **         
                  ** needed  (for  example,  you want  to  train a **         
                  ** 1-class  ML using  a labelled  training file) **         
                  ** then the  u suffix  can  be used  to  strip / **         
                  ** ignore the labels.  The general format is:    **         
                  **                                               **         
                  ** {usual flag}u {usual options}                 **         
                  **                                               **         
                  ** For example:                                  **         
                  **                                               **         
                  ** -AN i j k fle   becomes   -ANu i j k fle      **         
                  ** -AAe trainfile  becomes   -AAeu trainfile     **         
                  **                                               **         
                  ** NB: the l  suffix  is  incompatible  with the **         
                  **     1-class  ML,  and  the u  suffix is  only **         
                  **     compatible with the 1-class ML.           **         
                                                                              
                  ** The complete list of suffixed options are:    **         
                  ** -AAe,-AAi,-AAl,-AAu, -AAel,-AAeu,-AAil,-AAiu, **         
                  ** -ANe,-ANi,-ANr, -ANl,-ANu,-ANel, -ANeu,-ANil, **         
                  ** -ANiu, -ANrl, -ANru                           **         
                                                                              
         -Ad  n          - set target space dimension (vector regression).    
         -AD  n          - set target order (anionic regression: 0,1,2,...).  
                                                                              
                  ** Rudimentary  transductive learning  supported **         
                  ** by the following  functions.  Given a dataset **         
                  ** (labels ignored) points  are classified using **         
                  ** the ML and added to the  training set if g(x) **         
                  ** exceeds a given the threshold (set by -ATb).  **         
                  **                                               **         
                  ** If anomaly detection is  switched on, and the **         
                  ** trigger level (-ATn) is positive then if more **         
                  ** than this trigger level of anomalies are det- **         
                  ** ected  then a  new class  is created  and the **         
                  ** detected  anomalies  satisfying  the distance **         
                  ** requirement placed in this class.             **         
                  **                                               **         
                  ** File format is  the same as for  -AA etc, but **         
                  ** note  that labels  will always be  ignored if **         
                  ** present.                                      **         
                                                                              
         -ATA $file      - transductively add vectors from file (see -AA).    
         -ATN i j k $file -transductively add vectors from file (see -AN).    
         -ATb d          - distance from classification  boundary required for
                           point  to be  transductively  added  to that  class
                           (default: 1).                                      
         -ATa d          - trigger  distance (distance  point must  lie inside
                           anonaly class to be counted, default 1).           
         -ATn N          - set trigger level  for anomalies  (num of anomalies
                           anomalies required  before they  can be grouped and
                           incorporated as a new class (default 0, disabled). 
         -ATx d          - class  label  to be  used if  anomaly  class  added
                           (default 0, disabled).                             
         -ATy c          - method control for transduction:                   
                           0: add no training vectors.                        
                           1: add only non-anomalies satisfying distance -ATb.
                           2: add only anomalies  satisfying distance -ATa (if
                              new  class  creation   is  enabled  and  trigger
                              condition met, otherwise don't add).            
                           3: combination of methods 1 and 2 (default).       
                                                                              
         -Aq  n m v      - add n random features N(m,v) to all x.             
                                                                              
                  -- gentype basis definition options ("u" above)  --         
                                                                              
         -Aby            - set basis equal to training targets.               
         -Abu            - set basis equal to user defined values.            
         -AeA $file      - add basis elements from fname.                     
         -AeU f          - add basis element f.                               
         -AeR n d        - set basis of n elements, each a random, 1-norm unit
                           vector of dimension d.                             
         -Ar  i          - remove basis element i.                            
                                                                              
                  -- planar basis definition options ("v" above)   --         
                  -- (also multi-expert ranking)                   --         
                                                                              
         -ABy            - set basis equal to training targets.               
         -ABu            - set basis equal to user defined values.            
         -AEA $file      - add basis elements from fname.                     
         -AEU f          - add basis element f.                               
         -AER n d        - set basis of n elements, each a random, 1-norm unit
                           vector of dimension d.                             
         -AR  i          - remove basis element i.                            
                                                                              
                  -- MEX (Matlab) only options                     --         
                                                                              
         -AW  $yvar $xvar -get training data from  matlab variables.  xvar and
                           yvar  have n  rows,  each of  which  is a  training
                           vector.  yvar is not used  in the single-class case
                           but must still be present.                         
                                                                              
                                   [ y1 ]         [ x1' ]   [ x11 x12 .. x1d ]
                           yvar =  [ y2 ], xvar = [ x2' ] = [ x2d x2d .. x2d ]
                                   [ .. ]         [ ... ]   [                ]
                                   [ yn ]         [ xn' ]   [ xnd xnd .. xnd ]
                                                                              
                  -- SVM specific options                          --         
                                                                              
         -Ac  d          - add class to SVM  (multiclass only)  without adding
                           any vectors from that class.                       
         -Acz d          - like -Ac d, but sets epsilon = 0 for this component
                           if recursive division or max wins multiclass used. 
         -Aca d nu       - add anomaly detector to multiclass SVM with label d
                           and anomaly detection parameter nu.                
         -Acd            - remove anomaly detector.                           
         -As  n          - set single-class  SVM non-anomaly  label (+1 or -1,
                           +1 by defaulg).                                    
                                                                              
Post-load options (after adding training vectors) options:                    
                                                                              
         -Sa  $file      - load alpha from $file.                             
         -Sb  $file      - load bias from $file.                              
                           (NB: must be in raw format for recursive division) 
                                                                              
         -Snx            - remove any existing data normalisation.            
         -Sna            - normalise data features to zero mean, unit var.    
         -Snb            - normalise data features to zero median, unit var.  
         -Snc            - normalise data features to range 0,1.              
         -SNa            - like -Sna, but no shifting applied.                
         -SNb            - like -Snb, but no shifting applied.                
         -SNc            - like -Snc, but no shifting applied.                
         -SnA            - like -Sna, but applies min scale to all elements.  
         -SnB            - like -Snb, but applies min scale to all elements.  
         -SnC            - like -Snc, but applies min scale to all elements.  
         -SNA            - like -Sna, but no shift and min scale.             
         -SNB            - like -Snb, but no shift and min scale.             
         -SNC            - like -Snc, but no shift and min scale.             
                                                                              
         -Sra f          - random alpha/weight initialisation with sparsity f.
                                                                              
                  ** NOTES: - normalisation done by shift+scale in **         
                  **          the kernel, so  the same changes are **         
                  **          automatically  applied to all future **         
                  **          training/testing data.               **         
                  **        - calculated on per-feature basis.     **         
                  **        - does not  apply to  categorical, set **         
                  **          or graph valued features.            **         
                  **        - vector-valued  features  are  normed **         
                  **          to  zero  mean, unit  covar  matrix. **         
                  **          Hence  normalisation can  be used to **         
                  **          implement  the  Mahalanobis norm  if **         
                  **          data  is given as  a single  vector- **         
                  **          valued feature - ie. in the training **         
                  **          file use [ x ] rather than x.        **         
                                                                              
         -St  l u N      - Takes sample from whatever  distribution the ML is.
                           N  sample points are  taken from  x ~  U(l,u).  For
                           example  to  sample  from   the  posterior  of  the
                           current GP and put in in ML i you might use:       
                                                                              
                           -Zx -qc i h -qw i -St [ 0 0 ] [ 1 1 ] 200          
                                                                              
                           (Use fnb with grand(0,1) to get a block with random
                           output).                                           
         -Snt            - Reverse of -St, if that makes sense in context.    
                                                                              
         -Sx  f          - Target transform: y -> f(y).                       
                                                                              
Learning options (after post-load modifications):                             
                                                                              
         -c   CN         - Set  standard ML  tradeoff parameter  C/N = CN >= 0
                           (default 1).                                       
                                                                              
                  ** NB: in this code, C/N  is the upper bound for **         
                  **     |alpha| (unless  using quadratic cost, of **         
                  **     course, but in this  case similar changes **         
                  **     apply), whereas  elsewhere (SVMlight, for **         
                  **     example)  this upper bound is  denoted C. **         
                  **     C may mean different  things in different **         
                  **     ML blocks.                                **         
                                                                              
         -c+  s          - classification: C scale factor s>0 for class d = +1
                           regression:  C   scale  factor  for   lower  bounds
                           (default 1).                                       
         -c-  s          - classification: C scale factor s>0 for class d = -1
                           regression:  C   scale  factor  for   upper  bounds
                           (default 1).                                       
         -c=  s          - regression:  C  scale  factor  s>0  for  equalities
                           (default 1).                                       
         -cd  d s        - classification: C  scale  factor s>0  for  class d.
                           (default 1).                                       
         -cs  s          - Scale C/N by s>0.                                  
         -c+s s          - Scale c+ by s>0.                                   
         -c-s s          - Scale c- by s>0.                                   
         -c=s s          - Scale c= by s>0.                                   
         -cds d s        - Scale cd by s>0.                                   
                                                                              
         -cw  i w        - set C weight w>0 for training vector i.            
         -ww  i w        - set eps weight w>=0 for training vector i.         
                                                                              
         -mvb beta       - multi-ranking spread regularisation term (deflt 1).
         -mvi m          - maximum iterations m>=0 for multi-ranking outerloop
                           training (0 for unlimited - default).              
         -mvlr r         - multi-ranking learning rate r>0 (default 0.3).     
         -mvzt t         - multi-ranking zero tolerance t>=0 (default 0.01).  
                                                                              
                  -- SVM specific options                          --         
                                                                              
         -w   eps        - epsilon: sets  width of epsilon  tube/insensitivity
                           eps>=0 for  regression  or  boundary  distance  for
                           classification or  single class  (default 0.001 for
                           regression, 1 for  classification/single class, 0.1
                           for cyclic regression.                             
         -w+  s          - classification: epsilon scale s>=0 for class d = +1
                           regression:  epsilon scale factor  for lower bounds
                           (default 1).                                       
         -w-  s          - classification: epsilon scale s>=0 for class d = -1
                           regression:  epsilon scale factor  for upper bounds
                           (default 1).                                       
         -w=  s          - regression:  epsilon  scale   s>=0  for  equalities
                           (default 1).                                       
         -wd  d s        - classification: epsilon  scale  s>=0  for  class d.
                           (default 1).                                       
         -ws  s          - Scale eps by s>=0.                                 
         -w+s s          - Scale eps+ by s>=0.                                
         -w-s s          - Scale eps- by s>=0.                                
         -w=s s          - Scale eps= by s>=0.                                
         -wds d s        - Scale epsd by s>=0.                                
                                                                              
         -j   s          - equivalent to -c- 1 -c+ s.                         
         -jc  s          - equivalent to -c- 1 -c+ s.                         
         -jw  s          - equivalent to -w- 1 -w+ s.                         
                                                                              
         -Tl  s          - linear tube shrinking value s>=0 (if used, dflt 0).
         -Tq  s          - quadratic tube shrinking vl s>=0 (if used, dflt 0).
                                                                              
         -Bf  b          - fixed bias  used for  fixed bias case  (default 0).
                           For the vectorial case must be in raw format.      
                                                                              
         -Mn             - set no monotonicity constraints (default).         
         -Mi             - set monotonicity constraint increasing.            
         -Md             - set monotonicity constraint decreasing.            
                           (these constraints  are sufficient,  not necessary,
                           and  only  apply for  a  few  kernels  with  finite
                           dimensional feature maps.  They further assume that
                           all training vectors are elementwise non-negative).
                                                                              
         -Nl  f          - linear bias forcing term f, 1-class case (dflt 0). 
         -Nq  f          - quadratic bias forcing f>=0, 1-class case (dft 0). 
         -Nld d f        - linear bias forcing term f, multi case (dflt 0).   
         -Nqd d f        - quadratic bias forcing f>=0, multi case (deft 0).  
                                                                              
         -dd  d          - sets d in classify-with-reject  (binary and reduce-
                           to-binary multiclas only). Recommend combining with
                           -w 1 (defaults) or this  might not work  as you may
                           expect. For reduce-to-binary multiclass rejects are
                           labelled as anomalies if anomaly detection is used,
                           overriding the standard anomaly detector.  See:    
                                                                              
                           o Bartlett et al "Classification with Reject Option
                             using Hinge Loss", JMLR 2008.                    
                                                                              
         -nm  m          - norm  used   when  calculating  margin   (any  even
                           positive  integer  is  allowed, but  training  time
                           increases drastically for m > 4.  In practice m = 4
                           is feasible for  moderate datasets,  m > 4 only for
                           small (toy) datasets).                             
                                                                              
                           Notes: - suggest  using -om d  for speed  here (D2C
                           (m>=4)   won't actually get  used, but it stops the
                                    optimiser   attempting   to   maintain   a
                                    Cholesky factorisation of the Hessian).   
                                  - use of  -knn may  be required to  keep the
                                    Hessian from exploding.                   
                                  - large C values should be avoided.         
                                  - most kernels are fine up to about N = 250,
                                    but  suggest  using  -kan 2  -mtb  with  a
                                    distance based kernel form m > 4 to enable
                                    inner-product cacheing (or K4 pre-cacheing
                                    may become prohibitive).                  
                                                                              
         -Fi  m          - max iterations m>=0 for iterative fuzzy training (0
                           for unlimited - default).                          
         -Flr r          - iterative fuzzy learning rate r>0 (default 0.3).   
         -Fzt t          - fuzzy zero tolerance t>=0 (default 0.01).          
         -Fc  $fn        - generalised cost fn of var(0,0) (default tanh(x)). 
                                                                              
         -m   {r,m}      - hessian element type:                              
                           r - real-valued (default).                         
                           m - matrix/anion  valued  (vector/anion  regression
                               with  non-real  kernel  and  non-real  training
                               vectors).                                      
                                                                              
         -th  theta      - set theta (psd regularisation) for similarity.     
         -thn {0,1}      - set normalised (1, deft) or unnormal similarity.   
                                                                              
                  -- KNN specific options                          --         
                                                                              
         -k   k          - sets the number of neighours k in the KNN.         
         -K   i          - sets the KNN weight function.  The weight is:      
                                                                              
                             kappa(D(x))/kappabar                             
                                                                              
                           where D is set by the  kernel controls (this is the
                           distance  metric),  normalised  to range  0->1, and
                           kappa is set by this argument.  Options are:       
                                                                              
                           0: kappa(d) = 1/2                                  
                           1: kappa(d) = 1-d                                  
                           2: kappa(d) = 3/4 (1-d^2)                          
                           3: kappa(d) = 15/16 (1-d^2)^2                      
                           4: kappa(d) = 35/32 (1-d^2)^3                      
                           5: kappa(d) = pi/4 cos(pi/2 d)                     
                           6: kappa(d) = 1/sqrt(2.pi) exp(d^2/2)              
                           7: kappa(d) = 1/d                                  
                                                                              
                  -- GP specific options                           --         
                                                                              
         -d   sigma      - sets the measurement noise for the GP.             
         -dw  i w        - set sigma weight w>0 for training vector i.        
         -ds  s          - Scale sigma/N by s>0.                              
                                                                              
                  -- IMP specific options                          --         
                                                                              
         -iz  zref       - sets zref factor used by EHI.                      
         -ie  i          - sets EHI calculation method.  Options are:         
                           0: fully optimised recursive method.               
                           1: partially optimised recursive method.           
                           2: un-optimised recursive method.                  
                           3: Hupkens IRS method.                             
                           4: Couckuyt method with binary cells.              
                                                                              
                  -- BLK specific options                          --         
                                                                              
         -blx $fn        - set operating function for block (applied to output
                           except  for  user  function  machine,  where  it is
                           applied to all inputs).                            
         -bly $fn        - set MEX callback  function for block  (used to calc
                           g(x) for types mxa and mxb.                        
         -blz i          - set integer argument for MEX callback function (for
                           MEX  this is  >=0 for  commandline argument,  -1 to
                           load  named external  variable,  -3  to run  matlab
                           function.   External   variables  can  be  function
                           handles.  Default is -3).                          
         -bls fn         - set SYSTEM  call used  to calculate g(xx)  for sys.
                           On call, x,y,z,... are  sustituted with values from
                           from xx.  For example:                             
                                                                              
                           -bls \[ \".\/echoit.exe\" x \"\>\ temp.txt\" \]    
                                                                              
                           (note escape  characters - these may  or may not be
                           required  depending on  the operating  environment)
                           will  cause calls  to g(xx)  to be  evaluated using
                           the system call:                                   
                                                                              
                           ./echoit.exe x > temp.txt                          
                                                                              
                           where x is replaced with xx(0).  In this example we
                           use the shortcut where a vector is converted to the
                           elements concatenated with spaces between them.    
         -bfx $fn        - save x data to file $fn on sys g(x) if defined.    
         -bfy $fn        - save y data to file $fn on sys g(x) if defined.    
         -bfxy $fn       - save x,y data to file $fn on sys g(x) if defined.  
         -bfyx $fn       - save y,x data to file $fn on sys g(x) if defined.  
         -bfr $fn        - get result from file $fn on sys g(x) if defined.   
                                                                              
                  -- SSV specific options                          --         
                                                                              
         -nzs i          - sets number of support vectors for SSV.            
                                                                              
         -vlb [x]        - sets lower bound for support vectors for SSV.      
         -vub [x]        - sets upper bound for support vectors for SSV.      
                                                                              
         -sNl f          - (linear) bias forcing term f (default 0).          
                                                                              
                  -- MLM specific options                          --         
                                                                              
         -mlc i c        - Set C (regularisation) value for layer i (deflt 1).
                                                                              
Kernel selection options (after learning options):                            
                                                                              
                                                                              
                  ** By default a  single kernel is  chosen.  More **         
                  ** complex kernel  dictionaries are  possible by **         
                  ** setting kernel dictionary  size > 1.  In this **         
                  ** case K(x,y) = w0.K0(x,y)  + w1.K1(x,y) + ..., **         
                  ** where w0,w1,... are weights and K0,K1,... are **         
                  ** the kernel fucntions.                         **         
                                                                              
         -ki  i          - define which kernel elm i  is being set (default 0,
                           or 1 if this is an MLM non-input layer).           
         -ks  n          - set kernel dictionary size n (default 1).          
         -kn             - set kernel normalised (this element).              
         -ku             - set kernel unnormalised (this element, default).   
         -knn            - set kernel normalised (overall).                   
         -kuu            - set kernel unnormalised (overall, default).        
                                                                              
         -kg  g          - set x scale, non-ARD style ( x:= x/g ).            
         -kgg g          - set x scale, ARD style ( x_i := x_i/g_i for all i).
                                                                              
         -km             - modify so K(x,y) -> K(x,x).K(y,y).                 
         -kum            - undo -km (default).                                
                                                                              
         -kt  t          - type of kernel function:                           
                           Kernels 0-99 are intended for ML use (deft 2).     
                           Kernels 100-299 are intended for NN use (deft 201).
                           Kernels 300-399 are intended for kNN use (dft 300).
                           Defaults: 2 for most MLs.                          
                                     200 for density estimation.              
                                     300 for K-nearest neighbours.            
                                     200 for neural-networks.                 
                           For full list see -??k.                            
                                                                              
    0   = Constant kernel: K(x,y) = r1                                        
    1   = Linear kernel: K(x,y) = <x,y>/(r0.r0)                               
    2   = Polynomial kernel: K(x,y) = ( r1 + <x,y>/(r0.r0) )^i0               
    3   = Gaussian kernel: K(x,y) = exp(-||x-y||^2/(2*r0*r0)-r1)              
    4   = Laplacian kernel: K(x,y) = exp(-||x-y||/r0-r1)                      
    5   = Polynoise kernel: K(x,y) = exp(-||x-y||^r1/(r1*r0^r1)-r2)           
    7   = Sigmoid kernel (CPD): K(x,y) = tanh( <x,y>/(r0.r0) + r1 )           
    8   = Rational quadratic kernel: K(x,y) =( 1+||x-y||^2/(2*r0*r0*r1))^(-r1)
    9   = Multiquadric kernel (NM): K(x,y) = sqrt(||x-y||^2/(r0.r0)+r1^2)     
    10  = Inverse multiquadric kernel: K(x,y) = 1/sqrt(||x-y||^2/(r0*r0)+r1^2)
    11  = Circular kernel (MR2): K(x,y) = 2/pi * ( arccos(-||x-y||/r0)        
                                         - ||x-y||*sqrt(1-||x-y||^2/r0^2)/r0 )
    12  = Spherical kernel (MR3): K(x,y) = 1 - 1.5*||x-y||/r0                 
                                         + 0.5*||x-y||^3/r0^3                 
    13  = Wave kernel: K(x,y) = (r0/||x-y||).sin(||x-y||/r0)                  
    14  = Power kernel: K(x,y) = -(||x-y||/r0)^r1                             
    15  = Log kernel (CPD): K(x,y) = -log((||x-y||/r0)^r1 + 1)                
    19  = Cauchy kernel: K(x,y) = 1/(1+((||x-y||^2/(r0.r0))))                 
    23  = Generalised T-Student kernel: K(x,y) = 1/(1+||x-y||^r0)             
    24  = Vovk's real polynomial: K(x,y)= (1-((<x,y>/r0^2)^i0))/(1-<x,y>/r0^2)
    25  = Weak fourier kernel: K(x,y) = pi.cosh(pi-(||x-y||/r0))              
    26  = Thin spline (1): K(x,y) = ((||x-y||/r0)^(r1+0.5))                   
    27  = Thin spline (2): K(x,y) = ((||x-y||/r0)^r1)*ln(sqrt(||x-y||/r0))    
    32  = Diagonal offset kernel: r1 if diagonal Hessian, 0 otherwise         
    33  = Uniform kernel: K(x,y) = 1/2r0 if |||x-y||| < r0, 0 otherwise       
    34  = Triang kernel: K(x,y) = 1/r0 (1-|||x-y|||/r0) if |||x-y||| < r0, 0  
    35  = d-Matern kernel:                                                    
    37  = d+1/2-Matern kernel:                                                
    38  = 1/2-Matern kernel: K(x,y) = exp(-||x-y||/r0)                        
    39  = 3/2-Matern kernel: K(x,y) = (1 + sqrt(3)*||x-y||/r0).               
                                      exp(-sqrt(3)*||x-y||/r0)                
    40  = 5/2-Matern kernel: K = (1 + sqrt(5)*||x-y||/r0 + 5*||x-y||^2/r0^2). 
                                      exp(-sqrt(5)*||x-y||/r0)                
    41  = RBF rescale kernel: K(x,y) = z^(1/(2*r0*r0)) = exp(log(z)/(2*r0*r0))
    42  = Inverse gudermannian kernel: K(x,y) = igd(<x,y>/(r0.r0))            
    43  = Log ratio kernel: K(x,y) = log((1+<x,y>/(r0.r0))/(1-<x,y>/(r0.r0))) 
    44  = Exponential kernel: K(x,y) = exp(<x,y>/(r0.r0)-r1)                  
    45  = Hyperbolic sine kernel: K(x,y) = sinh(<x,y>/(r0.r0))                
    46  = Hyperbolic cosine kernel: K(x,y) = cosh(<x,y>/(r0.r0))              
    47  = Sinc kernel: K(x,y) = sinc(||x-y||/r0).cos(2*pi*||x-y||/(r0.r1))    
    48  = LUT kernel: K(x,y) = r1((int) x, (int) y) if r1 is a matrix         
                             = r1 if (int) x != (int) y and r1 not a matrix   
                             = 1  if (int) x == (int) y and r1 not a matrix   
               (this can be used eg to implement multitask learning with ICM).
                                                                              
    100 = Linear 0/1 neuron:    K(z) = z/(r0.r0)                              
    101 = Logistic 0/1 neuron:  K(z) = 1/(1+exp(-r0.z))                       
    102 = Gen. logistic 0/1:    K(z) = 1/(1+r1.exp(-r2.(z-r3)/(r0.r0)))^(1/r2)
    103 = Heavyside 0/1 neuron: K(z) = 1 if real(z) > 0, 0 otherwise          
    104 = ReLU 0/1 neuron:      K(z) = z/(r0.r0) if real(z) > 0, 0 otherwise  
    105 = Softplus 0/1 neuron:  K(z) = ln(r1+exp(z/(r0.r0)))                  
    106 = Leaky ReLU 0/1 neuron:K(z) = z/(r0.r0) if real(z) > 0               
                                     = (r1*z)/(r0.r0) if real(z) <= 0         
                                                                              
    200 = Linear -1/+1 neuron:    K(z) = z/(r0.r0)-1                          
    201 = Logistic -1/+1 neuron:  K(z) = 2/(1+exp(-z/(r0.r0))) -1             
    202 = Gen. logistic -1/+1: K(z) = 2/(1+r1.exp(-r2.(z-r3)/(r0^2)))^(1/r2)-1
    203 = Heavyside -1/+1 neuron: K(z) = 1   if real(z) > 0, -1 otherwise     
    204 = ReLU -1/+1 neuron:      K(z) = z/(r0.r0)-1 if real(z) > 0, -1 other 
    205 = Softplus -1/+1 neuron:  K(z) = 2.ln(r1+exp(z/(r0.r0))) -1           
    204 = Leaky ReLU -1/+1 neuron:K(z) = z/(r0.r0)-1 if real(z) > 0           
                                       = (r1*z)/(r0.r0)-1 if real(z) <= 0     
                                                                              
    300 = Euclidean distance: K(x,y) = -1/2 ||x-y||_2^2/(r0.r0)               
    301 = 1-norm distance:    K(x,y) = -1/2 ||x-y||_1^2/(r0.r0)               
    302 = inf-norm distance:  K(x,y) = -1/2 ||x-y||_inf^2/(r0.r0)             
    303 = 0-norm distance:    K(x,y) = -1/2 ||x-y||_0^2/(r0.r0)               
    304 = p-norm distance:    K(x,y) = -1/2 ||x-y||_r1^2/(r0.r0)              
                                                                              
    8xx = kernel transfer (see below)                                         
                                                                              
    900 = evaluate kernel by sending x,y data to unix socket kerni0.sock.  See
          kernel9xx function in mercer.cc for details.  Assumes K symmetric.  
    901 = like 900, but assumes K anti-symmetric.                             
    902 = like 900, but assumes K non-symmetric.                              
                                                                              
        Notes: - ' indicates conjugate transpose                              
               - ||x||^2 = conj(x)'x        (not the norm if (hyper-)complex).
               - ||x-y||^2 = (x-y').(x-y')  (not the norm if (hyper-)complex).
                           = ||x||^2 + ||y||^2 - 2<x,y>                       
               - <x,y> = ( x'y + conj(x)'conj(y) )/2.                         
               - for neural kernels, z=<x,y>.                                 
                                                                              
         -kg  x          - kernel parameter r0 = x (default 1).               
         -kr  x          - kernel parameter r1 = x (default 0 or 1).          
         -kf  $fn        - kernel param r10 = $fn (dft (var(0,1)+var(0,2))/2).
         -kv  i x        - kernel parameter ri = x (default 0).               
         -kd  x          - kernel parameter i0 = x (default 2).               
         -kG  x          - kernel parameter i0 = x (default 1).               
         -kV  i x        - kernel parameter ii = x (default 0).               
                                                                              
                                                                              
         -kI  v          - set kernels  indexing using  given index  vector v,
                           where  the  argument  is a  vector of  non-negative
                           integers   (eg [ 0 4 5 ])   in   increasing   order
                           corresponding to the indexes used.                 
         -kU             - set kernel unindexed.                              
         -kw  w          - set weight w>=0 of kernel function (default 1). The
                           weight can be anything (not just double), which may
                           be useful eg for matrix-valued kernels.            
                                                                              
         -ka  n          - number of samples used  when computing distribution
                           similarity (Muandet et al SMM).                    
         -kb  [ i j .. ] - indices of var(0,..) variables sampled.            
         -ke  [ fi fj ..]- distribution useds for var(0,..) ].                
                                                                              
                  ** Kernel  chaining  allows the  construction of **         
                  ** rudimentary deep kernels.  If a kernel elm is **         
                  ** chained then rather than adding the output of **         
                  ** this element to the result it is instead used **         
                  ** to calculate  the next kernel  element in the **         
                  ** chain.   So if the  dictionary size  is 3, k0 **         
                  ** is chained but k1 and k2 are not then:        **         
                  **                                               **         
                  ** K(x,y) = k1(m0(x),m0(y)) + k2(x,y)            **         
                  **                                               **         
                  ** where m0 is  the feature map  associated with **         
                  ** k0.  This only works  for kernels that can be **         
                  ** written as:                                   **         
                  **                                               **         
                  ** kn(x,y) = kn(||x||^2,||y||^2,<x,y>)           **         
                  **                                               **         
                  ** as for example in the above example:          **         
                  **                                               **         
                  ** K(x,y) = k1(k0(x,x),k0(y,y),k0(x,y))+k2(x,y)  **         
                  **                                               **         
                  ** (it's  a little  more  involved for  division **         
                  ** algebraic kernels, but that's essentially it) **         
                  **                                               **         
                  ** NB: - chaining   is  only   implemented   for **         
                  **       standard 2-norm kernels.                **         
                  **                                               **         
                  ** Example: -ks 3 -ki 0 -kc -kt 2 -kd 2 -ki 1    **         
                  **          -kt 7 -ki 2 -kt 1                    **         
                  **                                               **         
                  **   gives: k(x,y) = tanh(1+(1+<x,y>)^2) + <x,y> **         
                                                                              
         -kc             - set kernel chained.                                
         -kuc            - set kernel unchained.                              
                                                                              
                  ** Kernel splitting allows  different kernels to **         
                  ** applied to  different parts  of the  vectors. **         
                  ** For example if splitting  is set of element 1 **         
                  ** of the kernel with dictionary size 2 then:    **         
                  **                                               **         
                  ** K(x0 ~ x1,x2 ~ x3) = k1(x0,x2).k2(x1,x3)      **         
                  **                                               **         
                  ** Note  that  this  is instead  of  the  multi- **         
                  ** instance  interpretation  -  you  cannot  use **         
                  ** both at the same time.  m-kernel splitting is **         
                  ** also supported but is somewhat complicated.   **         
                                                                              
         -kS             - set kernel split.                                  
         -kuS            - set kernel unsplit (default).                      
                                                                              
                  ** Kernel  multiply points allow  for support of **         
                  ** product kernels of the form:                  **         
                  **                                               **         
                  ** K(x,y) = k0(x,y).k0(x,y)....                  **         
                  **                                               **         
                  ** where k0 is specified  by elements 0 to first **         
                  ** element  with -kMS  set, k1  is specified  by **         
                  ** the  element  immediately  after this  to the **         
                  ** next with -kMS set,  and so on (final kn goes **         
                  ** to end of elements).                          **         
                  **                                               **         
                  ** NB: on  evaluation, kernel is  first split at **         
                  **     at  multiply points,  then the  fragments **         
                  **     are split at split  points, then chaining **         
                  **     occurs.                                   **         
                                                                              
         -kMS            - set kernel multiply point.                         
         -kMuS           - set kernel non-multiply point.                     
                                                                              
                  ** These options  allow kernel  parameters to be **         
                  ** taken from elements  of the training vectors. **         
                  ** That is, for example:                         **         
                  **    ri = conj(xj).yj                           **         
                  **   (ri = yj for neural networks)               **         
                  ** This allows for example the RBF width to be a **         
                  ** function of position  in input space tuned on **         
                  ** the density of points.   The kernel weight is **         
                  ** parameter r-1 (-ko -1 ...).                   **         
                                                                              
         -ko  i j        - replace parameter  ri with input  product xj.yj (or
                           just yj for neural networks).                      
         -kO  i j        - replace  parameter ii  with input  (int) xj.yj  (or
                           just (int) yj for neural networks).                
         -koz            - delete all defined ri parameter replacements.      
         -kOz            - delete all defined ii parameter replacements.      
                                                                              
                  ** When computing m-kernels we replace ||x-y||^2 **         
                  ** with one of:                                  **         
                  **                                               **         
                  ** 0: ||x||_m^m + ||y||_m^m +..- m.<<x,y,...>>_m **         
                  ** 1: ||x||_p^2 + ||y||_p^2 +..- p.<<x,y,...>>_m **         
                  ** 2: ||x||_p^2 + ||y||_p^2 +..-                 **         
                  **                    (1/m).(sum_{ij} <xi,xj>))  **         
                  **                                               **         
                  ** Alternatively  you  can   use  a  moment-like **         
                  ** kernel:                                       **         
                  **                                               **         
                  ** K(x0,x1,...) = D sum_{s} K(||x{s}||_p^2)      **         
                  **                                               **         
                  ** where: x{s} = sum_i s_i x_i                   **         
                  **        s = [ +-1 +-1 ... ] has dim m          **         
                  **                                               **         
                  ** The following variants are available:         **         
                  **                                               **         
                  ** 103: D = 1/2^{m-1}                            **         
                  **      s : |i:si=+1| + |i:si=-1| in 4Z_+        **         
                  ** 104: D = 1/m!                                 **         
                  **      s : |i:si=+1| = |i:si=-1|                **         
                  **                                               **         
                  ** 203: like 103,  but expansion  only occurs on **         
                  **      first kernel in chain.                   **         
                  ** 204: like 104,  but expansion  only occurs on **         
                  **      first kernel in chain.                   **         
                  **                                               **         
                  ** 300: true moment-kernel expansion.            **         
                                                                              
         -kan i          - Set difference definition (default 1).             
                                                                              
                  ** Assumptions: these can speed up optimisation, **         
                  ** but make  sure they're  valid and  disable if **         
                  ** they are not.                                 **         
                                                                              
         -mtb            - always use  inner-product cache for speed.  This is
                           faster  if you're  doing kernel  tuning,  but  uses
                           almost double  the memory and offers  no speedup if
                           kernel is not tuned.                               
         -bmx            - save memory by not using inner-prd cache (default).
                                                                              
                  -- Kernel transfer                               --         
                                                                              
                  ** Kernel  transfer  is  a   method  for  taking **         
                  ** features learnt in  training an ML and coding **         
                  ** it into a kernel that  can be used elsewhere. **         
                  ** For  example for  an SVM  with  kernel  K the **         
                  ** transferred kernel Kx is defined to be:       **         
                  **                                               **         
                  ** Kx(y,z) = sum_ij alpha_i alpha_j K(y,z,xi,xj) **         
                  **                                               **         
                  ** where K here is  interpretted as an m-kernel. **         
                  ** You  can  do this  for  multiple  levels  and **         
                  ** treat  them like  any other  kernel.  To  use **         
                  ** the  kernel  so constructed  from an  ML  set **         
                  ** kernel 8xx (-kt 8xx)  and the ML number using **         
                  ** -ktx  i (i  is the  ML number  providing Kx). **         
                  ** Available kernels are (Kx is kernel of ML i): **         
                  **                                               **         
                  ** 800: Trivial: K(x,y) = Kx(x,y)                **         
                  ** 801: m-norm:                                  **         
                  **      K(x,y) = sum_ij ai aj Kx(x,y,xi,xj)      **         
                  **      where for SVM ai = alpha_i for xi        **         
                  ** 802: Moment:                                  **         
                  **      K(x,y) = sum_ij ai aj Kx(x,xi) Kx(y,xj)  **         
                  **      where for SVM a_i = alpha_i for x_i      **         
                  ** 803: reserved.                                **         
                  ** 804: K-learn:                                 **         
                  **      K(x,y) = sum_i ai Kx(xi,(x,y))           **         
                  **      where for SVM a_i = alpha_i for x_i      **         
                  **      Typically xi = (xai,xbi)                 **         
                  ** 805: K2-learn:                                **         
                  **      K(x,y) = (sum_i ai Kx(xi,(x,y)))^2       **         
                  **      where for SVM a_i = alpha_i for x_i      **         
                  **      Typically xi = (xai,xbi)                 **         
                  ** 806: Traditional multilayer network:          **         
                  **      K(x,y) = Kx(f(x),f(y))                   **         
                  ** 81x: like 80x, but assumes  a common dataset, **         
                  **      so indices are passed through and caches **         
                  **      used.                                    **         
                  **                                               **         
                  ** Example: suppose ML 0  is a trained  SVM with **         
                  **      kernel 3 (RBF), and we are setting up ML **         
                  **      1s kernel using the command:             **         
                  **                                               **         
                  **      -ks 2 -ki 0 -kc -kt 801 -ktx 0 -ki 1     **         
                  **      -kt 2 -kd 2                              **         
                  **                                               **         
                  **      gives you the kernel function:           **         
                  **                                               **         
                  **      K(x,y) = (K0(x,y)+1)^2                   **         
                  **                                               **         
                  **      where:                                   **         
                  **                                               **         
                  **      K0(x,y) = sum_ij ai aj Kr(x,y,ui,uj)     **         
                  **      Kr(x,y,u,v) = exp(4<x,y,u,v>-x4-y4-uU-vV)**         
                  **      ai = alpha_i for ML 0                    **         
                  **      ui = x_i for ML 0                        **         
                  **      uU = ||u||_4^4                           **         
                  **                                               **         
                  **      That is,  m(x) (the  feature  map of the **         
                  **      the rbf  kernel Kr(x,y))  is elementwise **         
                  **      weighted  to  give   w.*m(x)  using  the **         
                  **      weights  found by  the  SVM  ML 0,  then **         
                  **      mapped using  n (the feature  map of the **         
                  **      second order  polynomial kernel) to give **         
                  **      the  composite  map n(w.m(x)),  which is **         
                  **      the feature map embodied by K(x,y).      **         
                                                                              
         -ktx i          - obtain (transfer) this kernel from ML i.           
                                                                              
                  -- MLM specific options                          --         
                                                                              
         -ktk i          - which layer kernel is set above (-1 output, deflt).
                           Use -ktk -1 to adjust the inheritance type (must be
                           a type 8xx kernel, default is 802).                
                                                                              
                  -- gentype specific options                      --         
                                                                              
                  ** When dealing  with generic targets  MLs there **         
                  ** may also be kernel defined on target space to **         
                  ** measure  similarity.   To modify  this kernel **         
                  ** the above commands but with the prefix -e.    **         
                  **                                               **         
                  ** For example:                                  **         
                  **                                               **         
                  **  -ekt sets output kernel type                 **         
                  **  -eks seta parameter r0 for kernel            **         
                  **                                               **         
                  ** etc.                                          **         
                                                                              
         -e...   sets kernel parameters for output kernels.                   
                                                                              
Automatic parameter tuning options (after kernel selection):                  
                                                                              
         -bal            - for all classes i, set Ci = N/Ni.                  
                                                                              
                  -- SVM specific options                          --         
                                                                              
                  ** These  options set  parameters in  such a way **         
                  ** that  they  will  change  automatically  when **         
                  ** relevant parameters (N,kern) are changed.  So **         
                  ** for example -NlA will modify learning options **         
                  ** appropriately when additionl training vectors **         
                  ** are  added or  removed.  More  importantly it **         
                  ** will  automatically   modify  the  parameters **         
                  ** during n-fold-error analysis, for example.    **         
                                                                              
         -cA             - C/N = 1/(N*mean(kern(i,i))) (updated auto).        
         -cB             - C/N = 1/(N*median(kern(i,i))) (updated auto).      
         -cAN            - C/N = 1/mean(kern(i,i)) (updated auto).            
         -cBN            - C/N = 1/median(kern(i,i)) (updated auto).          
         -cX  x          - C/N = x/N (updated auto).                          
                                                                              
         -NlA nu C       - automatically  set  linear  bias  forcing  for  the
                           1-class and  CS++-SVM given  nu and C as  for those
                           formulations,  with  N and  n as  for  the  current
                           trained SVM.  For  the CS++-SVM this  is equivalent
                           to:                                                
                           -c C(n-1)/(N.nu) -w sqrt((n-1)/(2.(n-2))) -Nld d C 
                                                                              
                           where d is  as set  by the  -Acz d  call.  For  the
                           1-class SVM this is equivalent to:                 
                           -c C/(N.nu) -w 0 -Nl -C                            
                           (update auto).                                     
                                                                              
         -cua            - turns off auto update if currently in use.         
                                                                              
Grid search parameter selection (after automatic parameter tuning):           
                                                                              
         -g   ...        - select  given parameters  using a  grid  search  to
                           minimise some error measure.  Arguments are:       
                                                                              
                           -g nargs $evalstring $setstring ...                
                                                                              
                           where evalstring is the string used when evaluating
                           a  particular  choice and  setstring  is used  when
                           setting the final optimal  parameter choice.  nargs
                           sets the number  of parameters being  tuned, and in
                           the strings  themselves  the  variable  var(0,n) is
                           used to represent  these parameters,  where n is an
                           argnum except 0.  So for example:                  
                                                                              
             -g 2 "-c y -kd z -tx" "-c y -kd z" ...                           
                                                                              
                           tells svmheavy to  do a grid search  over C and the
                           kernel parameter d (r2), as represented by var(0,1)
                           (y) and  var(0,2) (z)  respectively,and  then set C
                           and d to the  optimal value resulting  from this as
                           measured using leave-one-out error (-tx).          
                                                                              
                           The  ranges are  set in  quadruples,  one for  each
                           var(0,n), after the first three arguments, namely: 
                                                                              
                           ... = t n m M I                                    
                                                                              
                           where t sets the argument type, n is the arg num, m
                           is the minimum  value, M the maximum  value, with I
                           steps.  Valid options for the type t are:          
                                                                              
                           zb - parameter is integer, linear increments.      
                           zl - parameter is integer, logarithmic increments. 
                           za - parameter is integer, exponential increments. 
                           zc - parameter is integer, inverse logistic incr.  
                           zr - parameter is integer, random increms (-g only)
                           fb - parameter is real, linear increments.         
                           fl - parameter is real, logarithmic increments.    
                           fa - parameter is real, exponential increments.    
                           fc - parameter is real, inverse logistic incr.     
                           fr - parameter is real, random increments (-g only)
                                                                              
                           so the grid search  is done on t in  the range 0,1,
                           and t = 0.01 => x = m,  t = 0.99 => x = M, t = 0 =>
                           x < -1e12, t = 1 => x > 1e12.                      
                                                                              
                           In the above example we might say:                 
                                                                              
             -g 2 "-c y -kd z -tx" "-c y -kd z" fl 1 1e-2 1e2 10 zb 2 1 5 5   
                                                                              
                           which tells svmheavy to do a grid search over C and
                           kernel parameter  d, with C ranging  over 10 values
                           on a log scale from 0.01 to 100 and d Selected_from
                           1,2,3,4,5, find  the optimal selection  to minimise
                           leave-one-out error, then set C and d to the values
                           so selected.                                       
                                                                              
                           In the  case of a  non-unique minimum,  the minimum
                           closest  to the  centre  of the  grid is  selected.
                           This is based on  the assumption that  the grid has
                           been chosen  such that the central  values are more
                           practical (or  likely) than the edge  values.  This
                           distance is stored as var(1,0).                    
                                                                              
                           These  functions can  do more  than just  parameter
                           selection - they  can also  do global  optimisation
                           more generally.  For example the command:          
                                                                              
             -gd 2 "-tM y^2+(z-1)^2" "-echo y -echo z" fb 1 -2 3 1 fb 2 -2 4 1
                                                                              
                           will minimise the function y^2+(z-1)^2 using DIRect
                           and echo the result (note that y = var(0,1) and z =
                           var(0,2); and that  x = var(0,0) cannot  be used as
                           it is reserved).  See -tM for details.  Another eg:
                                                                              
             -gb 1 "-fu 2 22 y -Zx -tM -z" "-echo y" fb 1 0 1 1               
                                                                              
                           will  maximise  test  function  22  using  Bayesian
                           optimisation.  Finally, in mex,  if f is a function
                           handle - eg:                                       
                                                                              
                             f = @(x) x(1)^2+(x(2)-1)^2                       
                                                                              
                           (note that it takes a vector argument) then you can
                           minimise in matlab using for example:              
                                                                              
                            svmmatlab('-gd 2 "-fWM 4 0 [ y z ] -tM var(0,4)"  
                             "-echo y -echo z" fb 1 -2 3 1 fb 2 -2 4 1',1,f)  
                                                                              
                           Alternatively if you have a function bayestest.m:  
                                                                              
                             function x = bayesTest(y,z)                      
                             x = y^2+(z-1)^2;                                 
                             end                                              
                                                                              
                           Then you could use:                                
                                                                              
                             f = @(x) bayestest(x(1),x(2))                    
                                                                              
                           Apart from grid optimisation the following exist:  
                                                                              
         -gd  ...        - like  above, but uses DIRect global optimiser.     
         -gN  ...        - like  above, but uses Nelder-Mead local optimiser. 
         -gb  ...        - like  above, but uses Bayesian optimisation.       
                                                                              
                           Overall algorithm  (steps in brackets  for Bayesian
                           or model-based only):                              
                                                                              
                          (0. Set up models for Bayesian optimisation.)       
                                                                              
                           1. Set up  projections.  The core  optimiser always
                              sees  a finite  dimensional  problem  on [0,1]^d
                              with linear scaling, specifically:              
                                                                              
                              - min_{x in [0,1]^d} f(q(p(x)))                 
                                                                              
                              where:                                          
                                                                              
                              - p : [0,1]^d  ->  R^d is  a finite  dimensional
                                projection operator  defined by fb,fl,... etc.
                                z... variants just round to nearest integer.  
                                                                              
                              - q : R^d -> F is present  for random projection
                                and functional optimisation. See -gp -gP, ....
                                                                              
                           2. Run $pstring defined by -gtp.   This can be used
                              to set  non-standard  kernels  etc for  Bayesian
                              optimisation and  functional optimisation.   The
                              following indices are set (-1 where/if n/a):    
                                                                              
                              - var(90,0): model for Bayesian optimisation.   
                              - var(90,1): noise model for Bayesian optim..   
                              - var(90,2): weighting projection template.     
                              - var(90,5): source data model (env-GP,diff-GP).
                              - var(90,6): difference data model (diff-GP).   
                                                                              
                              var(90,2)  and  var(90,3) are  used  for  random
                              projection and functional  analysis, where q has
                              the form:                                       
                                                                              
                              - q(x)(t) = [ x_0.q_0(t) + x_1.q_1(t) + ... ]   
                                                                              
                              where:                                          
                                                                              
                              - x is the vector p(x) in R^d.                  
                              - q_i is a draw from ML var(90,3).              
                                                                              
                              Typically ML  var(90,3) will be  a random vector
                              distribution, a GP  (function distribution) or a
                              set of basis functions (eg Bernstein polys).    
                                                                              
                           3. Random  projection:  draw  q_0,  q_1,  ...  from
                              defined distribution (template ML var(90,3)).   
                                                                              
                           4. Run $mstring defined by  -gtP.  This can be used
                              used  for  things  like  tweaking  the  template
                              distributions between inner-loop opts, so for eg
                              you could start with a very smooth SE kernel and
                              gradually make  it sharper.  The  following vars
                              are defined here (-1 if n/a):                   
                                                                              
                              - var(90,0): model for Bayesian optimisation.   
                              - var(90,1): noise model for Bayesian optim.    
                              - var(90,2): weighting projection template.     
                              - var(90,3): current q(t) function.             
                              - var(90,4): iteration  count  (0 first  time, 1
                                           second time etc... up to -gpr).    
                              - var(90,5): source data model (env,diff-GP).   
                              - var(90,6): difference data model (diff-GP).   
                                                                              
                              Note  that   changes  here  affect   the  *next*
                              projection step 3, not the current one.         
                                                                              
                           5. Inner  optimiser to  solve  using relevant  alg,
                              where evaluation of f calls $evalstring:        
                                                                              
                              - min_{x in [0,1]^d} f(q(p(x)))                 
                                                                              
                              Available variables are as-per step 4.          
                                                                              
                           6. Repeat from step 3 as defined by -gpr, ie:      
                                                                              
                              - -gpr 0: run steps 3-5 once.                   
                              - -gpr 1: run steps 3-5 twice.                  
                              - -gpr 2: run steps 3-5 thrice.                 
                                   ...                                        
                                                                              
                           Notes:                                             
                                                                              
                           - be wary  of using  -fo, -foe, -AAi...  and -tI...
                             here as they will  not work as  expected (vectors
                             taken out of  files in one iteration will  not be
                             put back for the next).                          
                                                                              
                           - $evalstring etc work  as function calls.  Changes
                             to (non-global)  variables made  during each call
                             will not  be saved.  If you  want to  return vars
                             from these calls use global variables - eg:      
                                                                              
             -gb 2 "-tM y^2+(z-1)^2" "-fWG 10 y -fWG 11 z" fb 1 -2 3 1        
                                                                   fb 2 -2 4 1
                                                                              
                             will store  y and z  in global  vars that  can be
                             retrieved, for example using:                    
                                                                              
                                    -fWg 1 var(0,10) -fWg 2 var(0,11)         
                                                                              
                           - multi-objective  optimisation  is possible  using
                             the  Bayesian optimiser  and  an appropriate  IMP
                             (see -gbq below) and the -tm ... option, e.g.    
                                                                              
                                    -tm [ var(1,37) var(1,42) ] -tc 5         
                                                                              
                             evaluates   performance    with   5-fold   cross-
                             validation and  seeks to  minimise both  negative
                             accuracy   (var(1,37))  and   negative   sparsity
                             (var(1,42))  (which is  equivalent to  maximising
                             accuracy and sparsity.  The Pareto set is written
                             to logfilename.pareto).                          
                                                                              
                           - complete  results  are   stored  in  grid  files.
                             ....xgrid  is  the  x   values  tested  (inputs),
                             ....fgrid  is the  f(x)  values found  (outputs),
                             ....mgrid  is f(x)  modified for  feasibility and
                             unscented   optimisation,    and   ...sgrid   are
                             suplementary result, the  nature of which depends
                             on  the specific optimiser used. In each file the
                             optimum  value  (or  values  for  multi-objective
                             optimisation)   are  marked   with  an   asterix.
                             ....xpareto,    ....fpareto,   ....mpareto    and
                             ....spareto are also written  containing just the
                             pareto set and associated.                       
                                                                              
                             Supplementary results are (bayesian):            
                                                                              
                                    [ tstart = start time of iteration (sec) ]
                                    [ tend   = end time of iteration (sec)   ]
                                    [ nrec   = rec. number in this batch     ]
                                    [ beta   = beta value for this batch     ]
                                    [ mu     = mu(x) for this recomm.        ]
                                    [ sigma  = sigma(x) for this recomm.     ]
                                    [ UCB    = UCB(x) for this recomm.       ]
                                    [ LCB    = LCB(x) for this recomm.       ]
                                    [ DVAR   = DVAR(x) for this recomm.      ]
                                    [ UDIST  = UDIST(x) for this recomm.     ]
                                    [ r      = r(x) for this recomm.         ]
                                    [ dtime  = DIRect run time (sec).        ]
                                    [ mutime = mu GP training time (sec).    ]
                                    [ sitime = sigma GP training time (sec). ]
                                    [ ftime  = function evaluation time (sec)]
                                    [ gridi  = grid index of point (if grid) ]
                                    [ gridy  = grid value of point (if grid) ]
                                    [ fvar   = variance of f (if available)  ]
                                                                              
                             noting that mu is modelled on -f(x); and:        
                                                                              
                                    UCB(x)   = mu(x) + sqrt(beta).sigma(x)    
                                    LCB(x)   = mu(x) - sqrt(beta).sigma(x)    
                                    DVAR(x)  = 2.sqrt(beta).sigma(x)          
                                    UDIST(x) = -fmin - -f(x)                  
                                    r(x)     = min(UDIST(X),DVAR(X))          
                                                                              
                           - also saved in ....xygrid,  which is in y x format
                             ready to train another  model, and xytgrid, which
                             includes variance for GP training.               
                                                                              
                           - you can  recurse these  (grid search  within grid
                             search) to arbitrary depth.                      
                                                                              
                           - when evaluating the setstring and  after the vars
                             that have been optimised are stored:             
                                                                              
                                    var(50,0): optimal x vector.              
                                    var(51,0): optimal f(x).                  
                                    var(52,0): optimal index.                 
                                    var(53,0): optimal supplement result.     
                                    var(54,0): optimal hypervolume.           
                                    var(55,0): mean f(x) (see -gr).           
                                    var(56,0): reserved for mean hypervolume. 
                                    var(57,0): mean index (time to min f(x)). 
                                    var(58,0): mean iterations to softmin.    
                                    var(59,0): mean iterations to hardmin.    
                                               (variances in var(..,65536)).  
                                                                              
                                    var(6x,...): as  for var(5x,...),  but all
                                                 pareto-optimal results.      
                                    var(7x,...): as  for var(6x,...),  but all
                                                 results are included.        
                                    var(8x,0): as for var(7x,...) in one.     
                                                                              
                                    var(90,0): SMBO model index (or -1).      
                                    var(90,1): SMBO sigma model index (or -1).
                                    var(90,2): optimal function index (or -1).
                                    var(90,3): functional model.              
                                    var(90,4): iteration count.               
                                    var(90,5): source data model (env,dif-GP).
                                    var(90,6): difference data model (dif-GP).
                                                                              
                             Note that var(50,0) and var(53,0) are expanded.  
                                                                              
                  ** Non-trivial optimisation examples:            **         
                  **                                               **         
                  ** 1. Minimise   y^2 + (z-1)^2  using   Bayesian **         
                  **    optimisation:                              **         
                  **                                               **         
                  ** -gmd 0.01 -gbH 3 -gb 2 "-tM y^2+(z-1)^2"      **         
                  **  "-echo y -echo z" fb 1 -2 3 1 fb             **         
                  ** 2 -2 4 1                                      **         
                  **                                               **         
                  **  - -gmd 0.01 sets the  (nominal) noise in the **         
                  **    target; and while the target is noiseless, **         
                  **    it is best to select non-zero to avoid bad **         
                  **    conditioning on the kernel matrix.         **         
                  **  - -gbH 3 selects GP-UCB optimisation.        **         
                  **  - -tM ... specifies the target function.     **         
                  **  - fb 1 ...  fb 2 ... specifies the variables **         
                  **    (y and z) and their ranges.                **         
                  **                                               **         
                  ** 2. As  for example  1,  but  this time  using **         
                  **    random   projections  (REMBO   style)  for **         
                  **    demonstrative purposes:                    **         
                  **                                               **         
                  ** -gmd 0.01 -gp urand([ -2 -2 ],[ 2 2 ]) -gbH 3 **         
                  ** -gb 2 "-tM derefv(y,0)^2+(derefv(y,1)-1)^2"   **         
                  ** "-echo y" fb 1 -2 3 1 fb 2 -2 4 1             **         
                  **                                               **         
                  **    In  this variant  the  optimiser  searches **         
                  **    over y,z, but the actual target becomes:   **         
                  **                                               **         
                  **          f( y.r0 + z.r1 )                     **         
                  **                                               **         
                  **    where r0 and r1 are  both vectors from the **         
                  **    (uniform) distribution U([-2,-2],[2,2]) as **         
                  **    specified by -gp ...  The number of random **         
                  **    vectors ri  in this sum  and the  range of **         
                  **    their weights  are controlled by  the args **         
                  **    in -gb ...                                 **         
                  **                                               **         
                  **    Note  that the  function minimised  is now **         
                  **    written in terms of  the components of y - **         
                  **    ie derefv(y,0) and  derefv(y,1) - which is **         
                  **    a  vector  (y   here  because   the  first **         
                  **    in the -gb ... expression is y = var(0,1). **         
                  **    In general it will be var(0,i), where i is **         
                  **    provided by the first var defined in -gb   **         
                  **                                               **         
                  ** 3. As for  example 1,  but using  Kirschner's **         
                  **    method  of  repeated  1-d  subspaces  (see **         
                  **    Adaptive and Safe Bayesian Optimization in **         
                  **    High   Dimensions    via   One-Dimensional **         
                  **    Subspaces):                                **         
                  **                                               **         
                  ** -gpr 3 -gmd 0.01 -gp urand([ -2 -2 ],[ 2 2 ]) **         
                  ** -gbH 3 -gb 1                                  **         
                  ** "-tM derefv(y,0)^2+(derefv(y,1)-1)^2"         **         
                  ** "-echo y" fb 1 -2 3 1                         **         
                  **                                               **         
                  **    The main distinction between this and eg 2 **         
                  **    is that -gpr 3 tells  the optimiser to run **         
                  **    an additional 3 times  in sequence (K-1 in **         
                  **    Kirschner)  with  a new  random  direction **         
                  **    from the previous  best.  We use -g 1 here **         
                  **    to specify  line-search,  but you  can use **         
                  **    more to specify narg-dim subspace search.  **         
                  **                                               **         
                  ** 4. Functional  optimisation using the  method **         
                  **    of   random  subspaces    (target  is   to **         
                  **    replicate the sin(x) on range [0,1]):      **         
                  **                                               **         
                  ** -gf 1 -gpr 5 -gmd 0.01 -gP 0.5 -gbH 3 -gb 1   **         
                  ** "-tM norm2(sin(2*pi()*x)-y)" "-echo y"        **         
                  ** fb 1 -10 10 1 -Zx -qw var(90,2) -Zx Zinteract **         
                  **                                               **         
                  **    In this case,  rather than -gp  to specify **         
                  **    a distribution for a  random-vector basis, **         
                  **    we use  -gP 0.5 to  specify  a GP  with SE **         
                  **    kernel  with  lengthscale  0.5 from  which **         
                  **    random  functions  are to  be drawn.  Note **         
                  **    also that  -gf 1 specifies that  functions **         
                  **    are to be treated as scalar functions, not **         
                  **    zero-variance  distributions, allowing  us **         
                  **    evaluate  norm2(sin(2*pi()*x)-y) to  real. **         
                  **    Finally,  -Zinteract  allows you  to  test **         
                  **    the resulting function.                    **         
                                                                              
                  ** Generic parameter search options.             **         
                                                                              
         -gy  t          - max training  time for  search alg (in  seconds - 0
                           for no limit, default).                            
         -gxs [...]      - initial value of x vector (default []).            
         -gfm l          - min value for function, stop if f<=l (deflt -inf). 
         -gfu m          - max value of function, stop if f>=l (deflt +inf).  
         -gfM l          - soft minimum value of  function (don't stop, but is
                           used by some variants of for example Bayesian opt).
         -gfU l          - soft max value  for function.  If  exceeded, result
                           clipped (that is, res = max(f(x),l) (deflt +inf).  
                                                                              
         -gr  n          - number of repeats (default 1).  If > 1 then results
                           for the  final repeat  are  returned,  except fgrid
                           values are replaced by [ fmean, fvar ], and fres is
                           meaningless.                                       
                                                                              
                                                                              
         -gnp            - set no projection (default).                       
         -gp  $fn        - set  projection.  If  set,  what  you are  actually
                           optimising   is  f(p(x)),  where  p   is  a  random
                           projection of type:                                
                                                                              
                              p(x) = [ x_0.p_0 + x_1.p_1 + ... ]              
                                     [ x_1                     ]              
                                     [ x_2                     ]              
                                     [ ...                     ]              
                                                                              
                           where p_i  is taken from  the distribution $fn  (eg
                           you could used grand([0 0],M:[1 0 ; 0 1]) for norml
                           random  vectors, or urand([ 0 0 0 ],[ 1 1 1 ])  for
                           uniform. Note that in this scheme the first element
                           in -g may be anything valued  (but is vector valued
                           in the example).                                   
         -gP  g          - like -gp but in this case p_i are functions sampled
                           from a GP with RBF kernel of lengthscale g.        
         -gPk...         - set kernel parameters on GP for -gP.               
         -gpb            - like -gp, but p_i are Bernstein basis polynomials. 
         -gpB n          - like -gpb, but with  schedule.  Degree of Bernstein
                           starts at n, then increases  with every repeat (set
                           by -gpr).  Note that in this case random repeats do
                           not give an alternate re-projection. The max degree
                           is just the number of variables.                   
         -gph            - set RKHS projection.  Like -gp, except in this case
                           the function is in a random RKHS, that is:         
                                                                              
                              p(x) = [ sum_i x_i K(xx_i,x) ]                  
                                     [ x_1                 ]                  
                                     [ x_2                 ]                  
                                     [ ...                 ]                  
                                                                              
                           where xx_i  are selected uniform  randomly [0,1] to
                           required dimension.                                
         -gpbk...        - set kernel parameters on RKHS for -gph.            
                                                                              
         -gpd d          - for functional optimisation  (-gP, -gpb) by default
                           the function is of one variable.  This lets you set
                           d variables instead  (default 1).  Variables are x,
                           y,... (var(0,0), var(0,1), ...) and range [0,1].   
         -gpr n          - number of sequential random projections. If NZ then
                           n random subspaces will  be found in sequence, with
                           each having as "point zero" the previous best.  For
                           RKHS this acts  like n restarts (but  make sure you
                           clear the model between restarts (-gmt 1 or 2).    
         -gf  n          - selects how functions are treated by the kernel:   
                           0 - fns are treated as zero-mean distributions.    
                           1 - fns are treated as scalar functions @():f(x).  
         -gc  n          - include bias step:                                 
                           0 - fns as described previously.                   
                           1 - for first iteration, final p_... replaced by C.
         -gC  C          - constant used in -gc 1.                            
         -gns n          - number of pts in approx integration for scalar fns.
                                                                              
                                                                              
         -gtp  $pstring  - string  to be  evaluated  at start  of  optim.  Put
                           fancy kernel setups etc here.  Evaluated once, just
                           before random projection (c/f -gpr).               
         -gtP  $mstring  - to be evaluated after  each random projection.  Put
                           kernel tweaking steps here.   Evaluated after inner
                           optimisation and random projections (c/f -gpr).    
                                                                              
         -gtx $xfn       - if set, the x  stored and logged is  not the x that
                           was  evaluated but  rather xfn(x).   null (default)
                           disables this  feature.  This is  evaluated *after*
                           $evalstring, which is handy for nested bayesian.   
                                                                              
         -g+ [ $fn1 .. ] - Add penalty sum_i max(0,$fni(x))  to the objective.
                           For example -g+ [ norminf(x)-B ] will add a penalty
                           term if ||p(x)||_inf > B.  This  can be helpful for
                           constraining projected searches.                   
                                                                              
                  ** Grid-search specific options.                 **         
                                                                              
         -ggm n          - number of zooms.  After  the optimum for a grid has
                           been  found a  zoom  involves  doing an  additional
                           grid-search with a  finer grid over a smaller range
                           around  the  previous  solution.  This  is  done  n
                           times (default 0).                                 
         -ggi f          - width  of  zoomed grid  is width  of previous  grid
                           multiplied  by f (real,  < 1).  Grid is  trimmed to
                           lie inside previous range.  Default 0.3333.        
                                                                              
                  ** DIRect global optimiser options.              **         
                                                                              
         -gdc m          - max number of cube divisions (default 200).        
         -gdf m          - max number of function evaluations (default 1000). 
         -gde e          - epsilon factor (default 1e-4).                     
         -gda t          - algorithm.  0 is original (default), 1 Gablowsky.  
         -gdy t          - max training time over-ride for DIRect.  Applies to
                           the direct algorithm only, so when DIRect is called
                           by another  algorithm (eg by a  Bayesian optimiser)
                           then  this controls  the time  spent in  the DIRect
                           calls  and -gy  controls  the  total overall  time.
                           Value is in seconds, default is zero (no override).
                                                                              
                  ** Nelder-Mead optimiser options.                **         
                                                                              
         -gNa e          - minimum f value (default -HUGE_VAL).               
         -gNb e          - relative f value tolerance (default 0).            
         -gNc e          - absolute f value tolerance (default 0).            
         -gNd e          - relative x value tolerance (default 0).            
         -gNg e          - relative x value tolerance (default 0).            
         -gNe m          - max number of function evaluations (default 1000). 
         -gNf t          - algorithm:   0  is  subplex  (default), 1  original
                           Nelder-Mead algorithm.                             
                                                                              
                  ** Stopping criteria are as follows:             **         
                  ** - f goes below minimum f value.               **         
                  ** - a step happens were  f changes by less than **         
                  **   the relative f tolerance times |f|.         **         
                  ** - a step happens there the absolute change in **         
                  **   |f| is less than the absolute f tolderance. **         
                  ** - a step happens where x changes by less than **         
                  **   the relative x tolerance times ||x||.       **         
                  ** - a step happens there the absolute change in **         
                  **   |x[i]| for any i is  less than the absolute **         
                  **    x tolderance.                              **         
                  ** - the max number  of function  evaluations is **         
                  **   exceeded.                                   **         
                  ** - the max training time is exceeded.          **         
                                                                              
                  ** Model-based optimisation options.             **         
                  ** (this includes Bayesian optimisation)         **         
                                                                              
         -gms            - Select single-objective optimisation model (dflt). 
         -gmo            - Select multi-objective optimisation model.         
         -gma n          - set dim of default GPR for multi-objective optim.  
                                                                              
         -gmT [ xt ]     - set (sparse vector) template  for model data.  Data
                           added to  the model is  x overlaid onto  xt, so for
                           example  -gmT [ ~ 1 ] means  that if a  vector x is
                           added into the model as [ x ~ 1 ]. This can be used
                           in multi-task learning, with -kS to signify a split
                           kernel, -ks 2 to indicate two  parts to the kernel,
                           -ki 0 ... to set usual covariance (over x), and -ki
                           1 -kg 48 -kr v to  set the variance  between tasks.
                           If -gmw has been used to spacify an ML with data of
                           the form [ xi ~ 0 ] then:                          
                                                                              
                           K([xi ~ ti],[xj ~ tj]) = K0(xi,xj)   if ti == tj   
                                                  = v.K0(xi,xj) if ti != tj   
                                                                              
                           will do transfer learning via multi-task kernel.   
                                                                              
         -gmd s          - set noise variance for default GPR model.          
         -gmg g          - set length scale for default GPR kernel.           
         -gmgg g         - set ARD length scale vector for default GPR kernel.
                                                                              
         -gmn n          - sigma estimation model:                            
                           0 - mu and  sigma approximated by a single  ML that
                               gets updated after each "batch".               
                           1 - mu and sigma approximated by separate MLs.  The
                               mu model  is updated  after  each  "batch", the
                               sigma model after each  experiment (during each
                               batch).                                        
                                                                              
         -gmma n         - Automatic tuning for optimisation model.           
         -gmmb n         - Automatic tuning for noise variance model.         
         -gmmc n         - Automatic tuning for source model (see -gmx).      
         -gmmd n         - Automatic tuning for difference model (see -gmx).  
                                                                              
                           The above tuning is of  (some of the) kernel params
                           on a heuristic basis using rudimentary grid search.
                           Note that the source model is only tuned once.  The
                           n value controls what is to be minimised:          
                                                                              
                           0: no automatic tuning                             
                           1: negative-log-likelihood minimisation.           
                           2: leave-one-out error minimisation (dflt for all).
                           3: recall minimisation.                            
                                                                              
         -gmw n          - by default the objective is modelled by a GPR.  You
                           can replace this with ML n using this command. Note
                           that for multi-objective  optimisation this must be
                           vector-valued.                                     
                        ** You can also use this for  transfer learning.  Data
                           already in this model is treated as observations of
                           y = -f(x) (NOTE THE NEGATIVE SIGN THERE).  See also
                           -gmx for more on transfer learning.                
         -gmW n          - like -gmw, but for the sigma model (if separate).  
                                                                              
         -gmt n          - model basis:                                       
                           0 - model f(p(x)) using p(x) (default).            
                           1 - model f(p(x)) using p(x), clear after subspace.
                           2 - model f(p(x)) using x, clear after subspace.   
                           3 - model f(p(x)) using x.                         
                                                                              
         -gmq n          - direction oracle mode (for -gp, -gP, -gpr):        
                           0 - direction is sample from gradient GP at current
                               best solution, as per GP model (default).      
                           1 - direction  is gradient  of model GP  at current
                               best solution, as per GP model.                
                           2 - direction is random sample.                    
                           3 - mode 0 for primary axis, mode 2 for the rest.  
                           4 - mode 1 for primary axis, mode 2 for the rest.  
                                                                              
         -gmx n          - controls how data already in model is treated (that
                           is, if you use  -gmw n where ML n  has data already
                           added).  Options are:                              
                           0 - assume data from target model (default).       
                           1 - use env-GP as per Joy1/Shi21.                  
                           2 - use diff-GP as per Shi21.                      
         -gmxa a         - alpha0 value for env-GP.                           
         -gmxb b         - beta0 value for env-GP.                            
                                                                              
         -gmy n          - Kernel transfer learning from ML n.                
         -gmya n         - Method for kernel  transfer, if -gmy used.  Options
                           as per -kt 8xx, so:                                
                           800 - trivial (K(x,y) = Kn(x,y)).                  
                           801 - m-norm (free kernel) transfer (default).     
                           802 - moment (Der and Lee) transfer.               
                           804 - K-learn transfer.                            
                           805 - K2-learn transfer.                           
                           806 - Multi-layer transfer.                        
         -gmyb n         - Kernel transfer normalisation:                     
                           0 - no normalisation.                              
                           1 - normalisation on (default).                    
                                                                              
                  ** env-GP:  - source model copied from the ML of **         
                  **            -gmw n, though  you can do further **         
                  **            tuning via var(90,5). Model asumed **         
                  **            to be already trained.             **         
                  ** diff-GP: - source model as-per env-GP.        **         
                  **          - difference model  also copied from **         
                  **            -gmw n, but  data added  and model **         
                  **            retrained throughout (var(90,6)).  **         
                                                                              
                  ** Bayesian optimiser options.                   **         
                                                                              
         -gbH n          - method used to select points in optimisation:      
                           0  - mean only minimisation.                       
                           1  - EI (expected improvement - default).          
                           2  - PI (probability of improvement).              
                           3  - GP-UCB as per Brochu (recommended GP-UCB).*   
                           4  - GP-UCB |D| finite as per Srinivas.            
                           5  - GP-UCB |D| infinite as per Srinivas.          
                           6  - GP-UCB p based on Brochu.                     
                           7  - GP-UCB p |D| finite based on Srinivas.        
                           8  - GP-UCB p |D| infinite based on Srinivas.      
                           9  - PE (variance-only maximisation).              
                           10 - mean-only minimisation.                       
                           11 - GP-UCB with user-defined beta_t (see -gbv).   
                           * beta_n = nu.2.log((n^{2+dim/2}).(pi^2)/(3.delta))
         -gbj n          - number  of random  start/seed  points  (default  -1
                           translates to d+1, where d is the dimension).      
         -gbt m          - max  iteration  count for  search algorithm  (0 for
                           no limit, default -1 which translates to 10d, -2 to
                           stop when min_x err(x) <= maxerr).                 
         -gbe e          - maxerr as required with -gbt -2 (see Kirschner et.,
                           default 0.1).                                      
         -gbz z          - zero  tolerance for  search algorithm  (def 1e-12).
                           Used when assessing if sigma^2 == 0.               
                                                                              
         -gba n          - if >=  0 then  this is  used to  seed the  RNG when
                           generating initial  points (see -gbj,  default 42).
                           -2 means seed with time, -1 means no seed.         
         -gbb n          - RNG seed right before main optimisation loop if >=0
                           Default 69.  -2 means seed with time, -1 no seed.  
                                                                              
         -gbG n          - do  grid-search,  where ML n  defines grid  data in
                           terms of x (dimensions  must agree  with definition
                           in -gb) and y (must be real).                      
                                                                              
         -gbD d          - delta factor used in GP-UCB method (default 0.1).  
         -gbk n          - nu factor used in GP-UCB (deft 0.2, see Srivinas). 
         -gbx n          - |D| (size  of search  space grid) for  gpUCB finite
                           (default -1, which  means set to size  of grid data
                           (if available) or 10 (which is arbitrary)).        
         -gbo a          - a constant for Srinivas |D|-infinite gpUCB (def 1).
         -gbB b          - b constant for Srinivas |D|-infinite gpUCB (def 1).
         -gbr r          - r constant for Srinivas  |D|-infinite gpUCB (def 1,
                           which is correct in all cases due to scaling).     
         -gbu p          - p value for GP-UCB p variants (default 2).         
         -gbv $fn        - user function for beta in  GP-UCB user-defined.  In
                           this function:                                     
                           - var(0,1) (y) = iteration number.                 
                           - var(0,2) (z) = x dimension.                      
                           - var(0,3) (v) = delta.                            
                           - var(0,4) (w) = |D| specified by -gbx.            
                           - var(0,5) (g) = a as specified by -gbo.           
                           For   multi-recommendation    via   multi-objective
                           optimisation on (mu,sigma) use -gbv null. This will
                           apply  multi-objective  optimisation to  (mu,sigma)
                           and the  resultant Pareto  set will all be  used as
                           recommendations  (null  means  beta  undefined,  so
                           select for all values of beta).                    
                                                                              
         -gbp [ n0 n1 ... ]- Add penalty  terms to  the acquisition  function.
                           This is helpful if there are non-linear constraints
                           on the feasible region.  Each element of the vector
                           should be an  ML.  The total penalty  is the sum of
                           the  outputs of  all MLs.  Penalty  should  be near
                           zero  in the  feasible region,  very large  outside
                           (that  is, a  penalty for  a minimisation problem).
                           Default value is an empty vector.                  
         -gbl w          - distance   weight   (default   0).   Assuming   the
                           acquisition function is  strictly positive replaces
                           it with  (1-w.||p-q||_2)*acqusition(q),  where p is
                           the previous  parameter set  (specified by  -g) and
                           q  is the  proposed parameter  set.  Handy  to make
                           incremental retrains faster.                       
                                                                              
         -gbts $estring  - string  to be evaluated  before each (inner)  iter.
                           var(90,4) is the inner iteration count.            
                                                                              
         -gbq n          - use IMP to pre-process  the output of the GP.  This
                           is required for multi-objective optimisation as the
                           improvement function requires a scalar. Essentially
                           mean :=  imp(mean,var).  Processing  done using IMP
                           with ML number  n (c/f -qw n) which  must be an IMP
                           type  object.  Note  that the  acquisition function
                           defined by -gbH will still be applied after this(to
                           do passthrough use -gbH 0).  Some IMPs  will update
                           the variance if  required.  For EHI use -gbH 0, for
                           SVM mono-surrogate use for example -gbH 3.         
                                                                              
         -gbmm n         - if using  Bayesian multi-step then  for all but the
                           first  recommendation in  each batch  pre-process x
                           using xi ->  f([ xi x0 ]), where f is  the function
                           of the  ML n  defined  here  and x0  is  the  first
                           recommendation in  this batch and the  dimension of
                           of xi is determined by -gbpd d (see below).        
                                                                              
         -gbim {0,1,2,3} - iteration count method.  In GP-UCB beta calculation
                           an iteration  counter t  is used.   The value  of t
                           (var(0,1)  in -gbv equation/vector)  is  controlled
                           by this setting:                                   
                           0 - t  =  (N/B)+1  initially, t ->  t+1 after  each
                               batch, where  B is the batch size (size of -gbv
                               vector, or  1 by default) and N  is the initial
                               number of  training vectors in  the GP model (0
                               by default  unless you have  pre-training pts).
                               Default, consistent with the GP-UCB-PE method. 
                           1 - t = N+1  initially,  t -> t+B after  each batch
                               (t is incremented by 1  for each recommendation
                               within a  batch).  This is  consistent with the
                               GP-BUCB method.                                
                           2 - like 1, but t  actually used to  calculate beta
                               is the t value at the start of this batch (that
                               is,  B*floor(t/B)).  Use  for GP-UCB  finite or
                               GP-UCB martingale.                             
                           3 - like 1, but t starts as 1.                     
         -gbs ibs        - intrinsic number  of recommendations  (see method 2
                           for multi-recommendation below).                   
         -gbm ims        - method for intrinsic batch:                        
                           0: use max mean, det(covar)^(1/(2*ibs)) (default). 
                           1: use ave mean, det(covar)^(1/(2*ibs)).           
                           2: use min mean, det(covar)^(1/(2*ibs)).           
                           3: use max mean, sqrt(ibs/Tr(inv(covar))).         
                           4: use ave mean, sqrt(ibs/Tr(inv(covar))).         
                           5: use min mean, sqrt(ibs/Tr(inv(covar))).         
         -gbpp q         - pre-process DIRect output using ML q (see below).  
         -gbpd d         - dimension of pre-process input (see below).        
         -gbpl lv        - min vector for pre-process input (see below).      
         -gbpu uv        - max vector for pre-process input (see below).      
                                                                              
         -gbsp p         - set >= 1 to enforce  mu_{1:p}-stability, where this
                           is the maximum allowed value of p (default 0).     
         -gbsP p         - minimum value of p (default 1).                    
         -gbsA A         - upper bound on output variation.                   
         -gbsB B         - maximum input perturbation.                        
         -gbsF F         - total output range (max-min).                      
         -gbsr b         - policy balance (0 = conservat (deflt), 1 = risky). 
         -gbsz z         - zero reference point for f (default 0).            
         -gbss c         - use sigmoid compresion on stability scores (def 0).
         -gbst t         - threshold for sigmoid compression (def 0.8).       
                                                                              
                  ** Notes on Stable Methods:                      **         
                  **                                               **         
                  ** - sigmoid compression  can make EI  work, but **         
                  **   not very well.  Without compression however **         
                  **   the  algorithm tends  to stick  at unstable **         
                  **   peaks where high gain swamps low stability. **         
                  ** - strongly suggest using  GP-UCB, which works **         
                  **   very  well.  The  expected  return (mu)  is **         
                  **   scaled, but the variance isn't.             **         
                                                                              
         -gbuu {0,1}     - Unscented  optimisation on (1) or off  (0,default).
                           See Nogueira et al, Unscented Bayesian Optimisation
                           for Safe Robot Grasping.                           
         -gbuk k         - Set k value (0 or -3, 0 default).                  
         -gbuS S         - sqrt(Sigma), square-root of matrix variance of x.  
                                                                              
                  ** Notes on Multi-Recommendation Methods:        **         
                  **                                               **         
                  **                                               **         
                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         
                  **                                               **         
                  ** Method 1 (multi-objective optimisation):      **         
                  **                                               **         
                  ** To use multi-objective optimisation to define **         
                  ** multiple    recommendations,     where    the **         
                  ** recommendations  correspond  to  the  Pareto- **         
                  ** optimal solutions of:                         **         
                  **                                               **         
                  **  max(-mu(x),sigma(x))                         **         
                  **                                               **         
                  ** (the negative  arising because  we are trying **         
                  ** to minimise our target here) use the command: **         
                  **                                               **         
                  ** -gbH 11 -gbv null                             **         
                  **                                               **         
                  ** where beta =  null is shorthand  for beta not **         
                  ** defined, so  solve for  all beta.  -gbH 11 is **         
                  ** GP-UCB with  user-defined  beta_t (so -gbv is **         
                  ** used instead).                                **         
                  **                                               **         
                  **                                               **         
                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         
                  **                                               **         
                  ** Method 2 (intrinsic batch):                   **         
                  **                                               **         
                  ** In the standard methods  the two variables to **         
                  ** be optimised in the  inner loop are mu(x) and **         
                  ** sigma(x).  Method 2 replaces these with:      **         
                  **                                               **         
                  **               [   mu(x_0)   ]                 **         
                  ** mu(x) -> max( [   mu(x_1)   ] )               **         
                  **               [     ...     ]                 **         
                  **               [ mu(x_{d-1}) ]                 **         
                  **                                               **         
                  **                 [ covar_00 covar_01 ... ]     **         
                  ** sigma(x) -> det([ covar_10 covar_11 ... ])^r  **         
                  **                 [    ...      ...   ... ]     **         
                  ** (r = 1/2d)                                    **         
                  **                                               **         
                  ** To select this  use -gbs d.  Can  be combined **         
                  ** with other methods (assuming no constraints). **         
                  ** Use -gbm 1 to slct mu(x) -> min(...) instead. **         
                  **                                               **         
                  ** If you want to apply constraints to this use: **         
                  **                                               **         
                  ** - use -gbpp to  specify the  ML defining  the **         
                  **   them, so DIRect  optimises a(f(x)), where a **         
                  **   is the usual  activation function  and f is **         
                  **   specified by ml number q.                   **         
                  ** - use -gbpd to specify  the dimension  of the **         
                  **   input to f.                                 **         
                  ** - use -gbpl/-gbpu to specify lower  and upper **         
                  **   bnd vector (respectively)  on the x vector. **         
                  **   In this case the variables specified in the **         
                  **   original -gb call are the outputs of f(x).  **         
                  **                                               **         
                  **                                               **         
                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         
                  **                                               **         
                  ** Method 3 (hybrid multi-strategy):             **         
                  **                                               **         
                  ** To define  multiple GP-UCB  strategies select **         
                  ** -gbH 11 (GP-UCB with user defined beta_t) and **         
                  ** then let -gbv be a vector, each corresponding **         
                  ** to an equation for calculating beta.  eg:     **         
                  **                                               **         
                  ** -gbH 11                                       **         
                  ** -gbv [ 2*log((x^(2+(y/2)))*zeta(2)/z) ;       **         
                  **        2*log((x^(3+(y/2)))*zeta(3)/z) ;       **         
                  **        2*log((x^(4+(y/2)))*zeta(4)/z) ]       **         
                  **                                               **         
                  ** gives  three equations  for beta  (p=2,3,4 in **         
                  ** this  case)  that   will  each  result  in  a **         
                  ** separate  recommendation.   Alternatively you **         
                  ** can define  beta_t indirectly  by making each **         
                  ** element of  the -gbv vector a  vector of  the **         
                  ** form:                                         **         
                  **                                               **         
                  ** [ methd {p betfn |D| nu delt a b r ibs ims} ] **         
                  **                                               **         
                  ** where method selects an option asper -gbH and **         
                  ** the  remaining   (optional)   arguments   the **         
                  ** various parameters therein.  For example:     **         
                  **                                               **         
                  ** -gbH 11                                       **         
                  ** -gbv [ [ 3 ] ;                                **         
                  **        [ 6 3 ] ;                              **         
                  **        [ 6 4 ] ]                              **         
                  **                                               **         
                  ** is equivalent to the  first example.  You can **         
                  ** leave "gaps" using []  (don't overwrite).  So **         
                  ** for example to set nu = 0.9 for the first two **         
                  ** in the above you would use:                   **         
                  **                                               **         
                  ** -gbH 11                                       **         
                  ** -gbv [ [ 3 [ ] [ ] [ ] 0.9 ] ;                **         
                  **        [ 6 3 [ ] [ ] 0.9 ] ;                  **         
                  **        [ 6 4 ] ]                              **         
                  **                                               **         
                  ** You can intersperse these techniques.  eg:    **         
                  **                                               **         
                  ** -gbH 11                                       **         
                  ** -gbv [ [ 3 [ ] [ ] [ ] 0.9 ] ;                **         
                  **        2*log((x^(5+(y/2)))*zeta(5)/z) ;       **         
                  **        [ 6 3 [ ] [ ] 0.9 ] ;                  **         
                  **        [ 6 4 ] ]                              **         
                  **                                               **         
                  ** defines four recommendation methods; and also **         
                  ** that you can include method 1 as part of this **         
                  ** so for example:                               **         
                  **                                               **         
                  ** -gbH 11                                       **         
                  ** -gbv [ [ 3 [ ] [ ] [ ] 0.9 ] ;                **         
                  **        2*log((x^(5+(y/2)))*zeta(5)/z) ;       **         
                  **        null ;                                 **         
                  **        [ 6 3 [ ] [ ] 0.9 ] ;                  **         
                  **        [ 6 4 ] ]                              **         
                  **                                               **         
                  ** defines  five   recommendation  methods,  the **         
                  ** third of which is itself multi-recommendation **         
                  ** method 1.                                     **         
                  **                                               **         
                  ** Finally,  you can  control how  the model  is **         
                  ** updated  between each round (element  in -gbv **         
                  ** vector) using  the -gmn option.  If -gmn 0 is **         
                  ** used (default) both mu and sigma use the same **         
                  ** model (GP) updated batchwise,  whereas -gmn 1 **         
                  ** uses separate  models, the  mu model  updated **         
                  ** batchwise and  the sigma model  updated after **         
                  ** each recommendation (halucinated samples).    **         
                  **                                               **         
                  **                                               **         
                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         
                  **                                               **         
                  ** Standard methods: GP-                         **         
                  **                                               **         
                  **UCB-PE: -gbH 11 -gmn 1 -gbv [ [ 3 ] ; [ 9 ] ...]**        
                  **BUCB: -gbH 11 -gmn 1 -gbim 1 -gbv [ [ 3 ] ; [ 3 ]**       
                  **                                         ... ] **         
                  ** UCB-multi: -gbH 11 -gbv null                  **         
                  **                                               **         
                  ** where  the number  of recommendations  is the **         
                  ** total number of elements in the -gbv vector.  **         
                  **                                               **         
                  **                                               **         
                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         
                  **                                               **         
                  ** Multi-recommendation with constraints:        **         
                  **                                               **         
                  ** We  take  as our  example  problem  the 2-dim **         
                  ** optimisation  problem given  previously  (see **         
                  ** -gb). The flags below should come just before **         
                  ** this statement.   The following  examples are **         
                  ** for a  batch size of  2 (recommendations  per **         
                  ** batch)  with the  restriction that  the first **         
                  ** element  of all  recommendations must  be the **         
                  ** same).                                        **         
                  **                                               **         
                  **                                               **         
                  ** GP-UCB-det (based on method 2 above) ver 1:   **         
                  **                                               **         
                  ** 1. Set up map function:                       **         
                  **                                               **         
                  **    -Zx -qw 2 -Zx -z fnb                       **         
                  **    -blx M:[1,0,0;0,1,0;1,0,0;0,0,1]*x         **         
                  **                                               **         
                  **    where -z fnb is a  block function and -blx **         
                  **    sets the map. -qw 2 sets it as ML block 2. **         
                  **    This implements the map:                   **         
                  **                                               **         
                  **    [ [ x00 ] ]   [ 1 0 0 ] [ z0 ]             **         
                  **    [ [ x01 ] ]   [ 0 1 0 ] [ z1 ]             **         
                  **    [         ] = [       ] [ z2 ]             **         
                  **    [ [ x10 ] ]   [ 1 0 0 ]                    **         
                  **    [ [ x11 ] ]   [ 0 0 1 ]                    **         
                  **                                               **         
                  **    The inner DIRect optimiser in the Bayesian **         
                  **    optimisation  optimises over  z, while the **         
                  **    GP model is built over x.                  **         
                  **                                               **         
                  ** 2. Set up the Bayesian optimiser:             **         
                  **                                               **         
                  **    -Zx -qw 0 -Zx                              **         
                  **    -gbH 3 -gbs 2 -gbm 0 -gbpp 2 -gbpd 3       **         
                  **    -gbpl [ -4 -4 -4 ] -gbpu [ 4 4 4 ]         **         
                  **                                               **         
                  **    Bayesian optimiser using method 3 (GP-UCB) **         
                  **   (setby -gbH 3), 2 intrinsic recommendations **         
                  **    per  batch (set  by -gbs 2),  balanced max **         
                  **    mean / determinant  (set by -gbm 0), using **         
                  **    ML 2  as  a  map  to  enforce  constraint, **         
                  **    pre-process  dimension (input  to ML 2) of **         
                  **    3 (-gbpd 3), with lower  and upper  bounds **         
                  **    for direct set by -gbpl and -gbpu.         **         
                  **                                               **         
                  **                                               **         
                  ** GP-UCB-det (based on method 2 above) ver 2:   **         
                  **                                               **         
                  ** 1. Set up penalty function:                   **         
                  **                                               **         
                  **    -Zx -qw 2 -Zx -z fnb                       **         
                  **    -blx 1000*(([1;0;1;0]*x)^2)                **         
                  **                                               **         
                  **    where -z fnb is a  block function and -blx **         
                  **    sets the map. -qw 2 sets it as ML block 2. **         
                  **    This is a quadratic penalty for failure to **         
                  **    satisfy the constraint.                    **         
                  **                                               **         
                  ** 2. Set up the Bayesian optimiser:             **         
                  **                                               **         
                  **    -Zx -qw 0 -Zx                              **         
                  **    -gbH 3 -gbs 2 -gbm 0 -gbp [ 2 ] -gbpd 4    **         
                  **    -gbpl [ -4 -4 -4 -4 ] -gbpu [ 4 4 4 4 ]    **         
                  **                                               **         
                  **    Bayesian optimiser using method 3 (GP-UCB) **         
                  **   (setby -gbH 3), 2 intrinsic recommendations **         
                  **    per  batch (set  by -gbs 2),  balanced max **         
                  **    mean / determinant  (set by -gbm 0), using **         
                  **    ML 2 as  a penalty to  enforce constraint, **         
                  **    pre-process  dimension (input  to ML 2) of **         
                  **    4 (-gbpd 4) - you need this -  with lower/ **         
                  **    upper bounds for direct set by -gbpl/-gbpu.**         
                  **                                               **         
                  **                                               **         
                  ** GP-UCB-cPE:                                   **         
                  **                                               **         
                  ** 1. Set up map function:                       **         
                  **                                               **         
                  **    -Zx -qw 2 -Zx -z fnb                       **         
                  **    -blx M:[0,1,0;1,0,0]*x                     **         
                  **                                               **         
                  **    where -z fnb is a  block function and -blx **         
                  **    sets the map. -qw 2 sets it as ML block 2. **         
                  **    This implements the map:                   **         
                  **                                               **         
                  **    [ x10 ] = [ 0 1 0 ] [ z0  ]                **         
                  **    [ x11 ]   [ 1 0 0 ] [ x01 ]                **         
                  **                        [ x11 ]                **         
                  **                                               **         
                  **    The second recommendation  is generated by **         
                  **    the  DIRect  optimiser  optimising  on  z, **         
                  **    while the GP model is built over x.        **         
                  **                                               **         
                  ** 2. Set up the Bayesian optimiser:             **         
                  **                                               **         
                  **    -Zx -qw 0 -Zx                              **         
                  **    -gbH 11 -gmn 1 -gbv [ [ 3 ] ; [ 9 ] ]      **         
                  **    -gbpp 2 -gbpd 1                            **         
                  **    -gbpl [ -4 ] -gbpu [ 4 ]                   **         
                  **                                               **         
                  **    Bayesian  optimiser using  above GP-UCB-PE **         
                  **    using ML 2 as a map to enforce constraint, **         
                  **    pre-process  dimension (input  to ML 2) of **         
                  **    1 for  non-first recommendation (-gbpd 1), **         
                  **    with lower and upper bounds for direct set **         
                  **    by -gbpl and -gbpu.                        **         
                  **                                               **         
                  **                                               **         
                  ** GP-cBUCB:                                     **         
                  **                                               **         
                  ** 1. Set up map function:                       **         
                  **                                               **         
                  **    -Zx -qw 2 -Zx -z fnb                       **         
                  **    -blx M:[0,1,0;1,0,0]*x                     **         
                  **                                               **         
                  **    where -z fnb is  a block function and -blx **         
                  **    sets the map. -qw 2 sets it as ML block 2. **         
                  **    This implements the map:                   **         
                  **                                               **         
                  **    [ x10 ] = [ 0 1 0 ] [ z0  ]                **         
                  **    [ x11 ]   [ 1 0 0 ] [ x01 ]                **         
                  **                        [ x11 ]                **         
                  **                                               **         
                  **    The second  recommendation is generated by **         
                  **    the  DIRect  optimiser  optimising  on  z, **         
                  **    while the GP model is built over x.        **         
                  **                                               **         
                  ** 2. Set up the Bayesian optimiser:             **         
                  **                                               **         
                  **    -Zx -qw 0 -Zx                              **         
                  **    -gbH 11 -gmn 1 -gbim 1 -gbv [ [ 3 ] ; [ 3 ] ]**       
                  **    -gbpp 2 -gbpd 1                            **         
                  **    -gbpl [ -4 ] -gbpu [ 4 ]                   **         
                  **                                               **         
                  **    Bayesian  optimiser  using  above  GP-BUCB **         
                  **    using ML 2 as a map to enforce constraint, **         
                  **    pre-process  dimension (input  to ML 2) of **         
                  **    1 for non-first  recommendation (-gbpd 1), **         
                  **    with lower and upper bounds for direct set **         
                  **    by -gbpl and -gbpu.                        **         
                  **                                               **         
                  **                                               **         
                  ** GP-cBO:                                       **         
                  **                                               **         
                  ** This method uses an outer loop for the common **         
                  ** variable (y  in this case) and  an inner loop **         
                  ** to recommend  batch of other  variables (z in **         
                  ** this case:                                    **         
                  **                                               **         
                  ** 1. Set up GP model for inner loop:            **         
                  **                                               **         
                  **    -Zx -qw 1 -Zx -z gpr -d 0.01               **         
                  **                                               **         
                  **                                               **         
                  ** 2. Set up GP model for outer loop:            **         
                  **                                               **         
                  **    -Zx -qw 2 -Zx -z gpr -d 0.01               **         
                  **                                               **         
                  ** 3. Inner loop is called  as a function, which **         
                  **    we define here.  This optimises over y and **         
                  **    z, but  the range  of y  is restricted  so **         
                  **    that y  = var(0,100).   Note that  this is **         
                  **    basically GP-UCB-PE.  Note also the use of **         
                  **    -fret to return var(0,1),  var(0,2) (y and **         
                  **    z), var(51,0) (f(y,z)) and var(53,10) (the **         
                  **    upper bound for the outer loop).           **         
                  **                                               **         
                  **    -Zx -fM 42 { -gbt 1 -gbH 11 -gmw 1 -gmn 1  **         
                  **    -gbv [ [ 3 ] ; [ 9 ] ] -gb 2 "-tM          **         
                  **    y^2+(z-1)^2" "fret 0 1 -fret 0 2           **         
                  **    -fret 51 0 -fret 53 10" fb 1 var(0,100)    **         
                  **    var(0,100) 1 fb 2 -2 4 1 }                 **         
                  **                                               **         
                  ** 4. Run the outer loop.   This will return the **         
                  **    optimal  result in y  and z.  Note  use of **         
                  **    -fret on y and  z in $evalstring  as these **         
                  **    would otherwise be lost and -tMv to modify **         
                  **    variance using the upper bound.  Note also **         
                  **    that  in  $setstring   y  and  z  must  be **         
                  **    retrieved from var(50,...) and returned    **         
                  **    using -fret.                               **         
                  **                                               **         
                  **    -Zx -qw 0                                  **         
                  **-Zx -gbt 20 -gbH 3 -gmw 2 -gtx [ var(0,100) ; z ] **      
                  **    -gb 1 "-MM 42 -Zx -tM var(51,0) -tMv       **         
                  **    var(53,10)-0.01" "-fW 1 var(50,0) -fW 2    **         
                  **    var(50,1) -fret 0 1 -fret 0 2" fb 100 -2 3 **         
                  **                                               **         
                  ** 5. Report result:                             **         
                  **                                               **         
                  **    -echo y -echo z                            **         
                  **                                               **         
                                                                              
         -gBbH n         - EHI  calculation  method  for  multi-recommendation
                           via  multi-objective  optimisation.   See  -ie  for
                           possible values.                                   
         -gB...          - Options for multi-recommendation via multi-objctive
                           optimisation.   These control the  Bayesian (inner,
                           direct and generic) options used in the inner loop.
                           -gB.. canbe -gBy, -gBdc, -gBdf, -gBde, -gBda, -gBbt
                           -gBdy, -gBbj, -gBfm or -gBfM (eg -gBy  controls the
                           inner-loop training time, c/f see -gy).            
                                                                              
Transfer learning via kernels:                                                
                                                                              
         -x n [ i j .. ] - if the current ML  is a binary SVM  from which SVMs
                           i,j,... inherit their  kernel via kernel  801 (with
                           no  additional non-linearities) then this  function
                           will define  a kernel with n bases (seen  from SVMs
                           i,j,...):                                          
                                                                              
                             K(x,y) = sum_ij alpha_i alpha_j K(x,y,zi,zj)     
                                                                              
                           that is tuned to optimise performance.  This choice
                           is regularised  by C  (upper bound on  alpha_i) and
                           epsilon (linear sum on alpha) for current ML.      
         -xi  n          - max training iterations for -x (default 20).       
         -xt  t          - max training time for -x (default 0 = nothing).    
         -xs  s          - solution tolerance (alpha step) for -x (deft 0.01).
                                                                              
Feature selection options (after parameter tuning):                           
                                                                              
         -fsx            - feature selection via hill climbing, min LOO error.
         -fsr            - feature selection via hill climbing, min recal err.
         -fsc n          - feature  selection via  hill  climbing, min  n-fold
                           cross validation error.                            
         -fsC m n        - feature  selection via  hill  climbing, min  n-fold
                           cross-validation error, randomised, m repetitions. 
         -fsf $file      - feature selection via hill climbing, min test err. 
         -fsF i j $file  - feature selection via hill  climbing, min test err,
                           ignoring i  vectors  at start,  testing  at most  j
                           vectors (-1 if all).                               
         -fss n          - set number  of sweeps (0  default) for  this set of
                           feature  selection.  If  >1 then,  after the  first
                           hill-climb(descent) the  algorithm will follow with
                           hill-descent(climb)   starting  with   the  optimal
                           features found previously.   The alternating climb,
                           descent sequence will run n times.                 
         -fsd            - start  with  existing  features  for  this  set  of
                           feature selection rather than from scratch.        
         -fsD            - undoes -fsd.                                       
                                                                              
                  ** Use -fS...  to use hill  descent rather  than **         
                  ** hill climbing.  Suffixes as per -tx etc.      **         
                                                                              
Fuzzy ML Support (after feature selection):                                   
                                                                              
                  ** So-called  fuzzy MLs  use functions  inspired **         
                  ** by fuzzy  logic to  set the individual  C and **         
                  ** epsilon  weights  for  each  training  vector **         
                  ** based  on   some  estimate  of   how  much  a **         
                  ** particular  vector  "belongs"  to its  class. **         
                  ** The degree of belonging  is calculated by the **         
                  ** membership function,  typically  based on the **         
                  ** relative distances to the class centre of the **         
                  ** class  to which  the vector  belongs and  the **         
                  ** distances  to  other  classes.   The  inbuilt **         
                  ** membership functions are:                     **         
                                                                              
    q1 = 0.5+((exp(f*(d_d-d_l)/d)-exp(-f))/(2*(exp(f)-exp(-f))))              
    q2 = ((2*(0.5+((exp(f*(d_d-d_l)/d)-exp(-f))/(2*(exp(f)-exp(-f))))))-1)^m  
    q3 = 0.5+((1-(d_l/(r_l+f))/2)                                             
    q4 = 0.5*(1+tanh(f*((2*g_x)+m)))                                          
                                                                              
                  ** (Keller and  Hunt, modified Keller  and Hunt, **         
                  ** Lin  and  Wang, and  cluster-based).  In  all **         
                  ** cases, for each training vector pair (x,y):   **         
                                                                              
    q   = var(2,0) = either t (C weight) or s (epsilon weight), pre-fuzzing.  
    d_l = var(2,1) = distance from x to the mean of class y.                  
    d_d = var(2,2) = min distance from x to the mean of any other class !y.   
    d   = var(2,3) = distance between the mean of classes y and !y.           
    r_l = var(2,4) = radius of  smallest  sphere  centred at  mean of  class y
                     containing all elements of class y.                      
    r_d = var(2,5) = radius of  smallest  sphere  centred at mean  of class !y
                     containing all elements of class !y.                     
    g_x = var(2,6) = output of 1-class ML trained  with all vectors of class y
    q1  = var(2,7) = Keller and Hunt membership.                              
    q2  = var(2,8) = Modified Keller and Hunt membership.                     
    q3  = var(2,9) = Lin and Wang membership.                                 
    q4  = var(2,10)= cluster-based membership.                                
    f   = var(3,0) = user parameters set below.                               
    m   = var(3,1) = user parameters set below.                               
    nu  = var(3,2) = nu value used for clustering.                            
                                                                              
         -fzt $fn        - Apply fuzzy to C weights using function given.     
         -fzs $fn        - Apply fuzzy to epsiln weights using function given 
         -fztk...        - Modify kernel function  to be used in fuzzification
                           of C weights.   ... is any  of the kernel functions
                           above (so for  example -fztkt 3 uses the RBF kernel
                           for  fuzzification).  Note  that the kernel used is
                           initially set  to the  ML kernel,  and this  simply
                           modifies it.                                       
         -fztf f         - Set user parameter f in  -fzt function (default 1).
                           (should be small +ve number for Lin and Wang).     
         -fztm m         - Set user parameter m in -fzt function (default 1). 
         -fztNlA nu      - Set nu for 1-class SVM if needed for -fzt (df 0.5).
         -fzsk...        - Modify kernel function to  be used in fuzzification
                           of eps weights.  ... is any of the kernel functions
                           above (so for example  -fztkt 3 uses the RBF kernel
                           for fuzzification).   Note that the  kernel used is
                           initially set  to the  ML kernel,  and this  simply
                           modifies it.                                       
         -fzsf f         - Set user parameter f in  -fzs function (default 1).
                           (should be small +ve number for Lin and Wang).     
         -fzsm m         - Set user parameter m in -fzs function (default 1). 
         -fzsNlA nu      - Set nu for 1-clas SVM if needed for -fzs (dft 0.5).
                                                                              
Bootstrap ML Support (after fuzzy ML):                                        
                                                                              
                  ** To approximate variance  calculations you can **         
                  ** use (pseuso)-boostrapping.                    **         
                  **                                               **         
                  ** 1. Set up your ML (say ML 0).                 **         
                  ** 2. Make B copies of your ML (say 1..B):       **         
                  **    -qc 1 0 -Zx                                **         
                  **    -qc 2 0 -Zx                                **         
                  **      ...                                      **         
                  **    -qc B 0 -Zx                                **         
                  ** 3. "Bootstrap" ML copies:                     **         
                  **    -qw 1 -boot -Zx                            **         
                  **    -qw 2 -boot -Zx                            **         
                  **      ...                                      **         
                  **    -qw B -boot -Zx                            **         
                  ** 4. Set up multi-block averaging block (B+1):  **         
                  **    -qw B+1                                    **         
                  **    -mba 1 1                                   **         
                  **    -mba 2 2                                   **         
                  **      ...                                      **         
                  **    -mba B B                                   **         
                  ** 5. ML B+1 now calculates  variance and allows **         
                  **    (800) kernel transfer inc kernel variance. **         
                                                                              
         -boot           - pseudo-bootstrap ML by taking all (non-constrained)
                           training vectors (there  are m), randomly selecting
                           m with replacement, then constraining the rest.    
                                                                              
Macros (after bootstrap ML):                                                  
                                                                              
         -M0             - execute macro var(130,0):                          
                           1. normalise training data.                        
                           2. perform grid-search of C,d on polynomial kernel.
                           3. perform hill-climbing feature selection.        
                           4. repeat steps 2,3.                               
                           5. repeat steps 2,3,4 for RBF kernel (C,g) and arc-
                              cosine kernel (C,n).                            
                           6. select most optimal kernel and C found.         
         -M1             - execute macro var(130,1):                          
                           1. normalise training data.                        
                           2. perform grid-search of C,d on polynomial kernel.
                           3. perform hill-climbing feature selection.        
                           4. repeat steps  2,3 for RBF kernel  (C,g) and arc-
                              cosine kernel (C,n).                            
                           5. select most optimal kernel and C found.         
         -M2             - execute macro var(130,2):                          
                           1. normalise training data.                        
                           2. perform grid-search of C,d on polynomial kernel.
                           3. repeat  step  2 for  RBF kernel  (C,g) and  arc-
                              cosine kernel (C,n).                            
                           4. select most optimal kernel and C found.         
         -M3             - execute macro var(130,3):                          
                           1. perform grid-search of C,d on polynomial kernel.
                           2. perform hill-climbing feature selection.        
                           3. repeat steps 1,2.                               
                           4. repeat steps 1,2,3 for RBF kernel (C,g) and arc-
                              cosine kernel (C,n).                            
                           5. select most optimal kernel and C found.         
         -M4             - execute macro var(130,4):                          
                           1. perform grid-search of C,d on polynomial kernel.
                           2. perform hill-climbing feature selection.        
                           3. repeat steps  1,2 for RBF kernel  (C,g) and arc-
                              cosine kernel (C,n).                            
                           4. select most optimal kernel and C found.         
         -M5             - execute macro var(130,5):                          
                           1. perform grid-search of C,d on polynomial kernel.
                           2. repeat  step  1 for  RBF kernel  (C,g) and  arc-
                              cosine kernel (C,n).                            
                           3. select most optimal kernel and C found.         
         -M6...11        - like -M0...5, but only search the polynomial kernel
         -M12            - grid-search to minimise negative log-likelihood:   
                           o 1e-2 <= C <= 1e2 (21 increments)                 
         -M13            - grid-search to minimise recall error:              
                           o 1e-2 <= C <= 1e2 (21 increments)                 
         -M14            - grid-search to minimise leave-one-out error:       
                           o 1e-2 <= C <= 1e2 (21 increments)                 
         -M15            - grid-search to minimise random 10-fold error:      
                           o 1e-2 <= C <= 1e2 (21 increments)                 
         -M16            - grid-search to minimise negative log-likelihood:   
                           o 1e-2 <= C <= 1e2 (11 increments)                 
                           o 1e-1 <= g <= 1e1 (11 increments)                 
         -M17            - grid-search to minimise recall error:              
                           o 1e-2 <= C <= 1e2 (11 increments)                 
                           o 1e-1 <= g <= 1e1 (11 increments)                 
         -M18            - grid-search to minimise leave-one-out error:       
                           o 1e-2 <= C <= 1e2 (11 increments)                 
                           o 1e-1 <= g <= 1e1 (11 increments)                 
         -M19            - grid-search to minimise random 10-fold error:      
                           o 1e-2 <= C <= 1e2 (11 increments)                 
                           o 1e-1 <= g <= 1e1 (11 increments)                 
         -M20            - grid-search to minimise negative log-likelihood:   
                           o 1e-2 <= C <= 1e2 (11 increments)                 
                           o d = 1,2,3,4,5                                    
         -M21            - grid-search to minimise recall error:              
                           o 1e-2 <= C <= 1e2 (11 increments)                 
                           o d = 1,2,3,4,5                                    
         -M22            - grid-search to minimise leave-one-out error:       
                           o 1e-2 <= C <= 1e2 (11 increments)                 
                           o d = 1,2,3,4,5                                    
         -M23            - grid-search to minimise random 10-fold error:      
                           o 1e-2 <= C <= 1e2 (11 increments)                 
                           o d = 1,2,3,4,5                                    
         -M24            - grid-search to minimise negative log-likelihood:   
                           o 1e-1 <= g <= 1e1 (21 increments)                 
         -M25            - grid-search to minimise recall error:              
                           o 1e-1 <= g <= 1e1 (21 increments)                 
         -M26            - grid-search to minimise leave-one-out error:       
                           o 1e-1 <= g <= 1e1 (21 increments)                 
         -M27            - grid-search to minimise random 10-fold error:      
                           o 1e-1 <= g <= 1e1 (21 increments)                 
         -M28            - grid-search to minimise negative log-likelihood:   
                           o d = 1,2,3,4,5                                    
         -M29            - grid-search to minimise recall error:              
                           o d = 1,2,3,4,5                                    
         -M30            - grid-search to minimise leave-one-out error:       
                           o d = 1,2,3,4,5                                    
         -M31            - grid-search to minimise random 10-fold error:      
                           o d = 1,2,3,4,5                                    
         -MM  i          - execute  var(130,j) (must  be a string).   See pre-
                           defined macros below.                              
         -MF  i          - like -MM, but works  as a function (won't overwrite
                           variables).                                        
                                                                              
Optimization options (after macros):                                          
                                                                              
         -oo             - do not optimise the ML this time.                  
         -oO             - optimise the ML this time (default).               
                           NB: -oo and  -oO are persistent, so if  you use -oo
                               to save time you need  to use -oO later so that
                               optimisation occurs.                           
         -oe  e          - accuracy e>=0 required of  g(x) in soln (dflt .01).
                           (use -oe A to set accuracy = max(0.01*eps,100*zt)).
         -oz  z          - set zero tolerance.                                
         -ot  m          - terminate optimization after  m iterations, even if
                           solution not found (default LONG_MAX).             
         -oy  t          - set  training  timeout  time  (t  in  seconds).  If
                           training is not  completed after this  time then it
                           will be stopped  prematurely.  t is a float.  Times
                           less than 1 second will be interpretted unlimited. 
         -oM  n          - size of  kernel  cache  in MB  (default  200).  For
                           unlimited (aka 50000 rows), select -1.             
         -ofy            - turn on Cholesky factorisation fudging (that is, if
                           Cholesky becomes  near singular  then  add a  small
                           diagonal offset.  This is  arguably a bad idea, but
                           sometimes it is necessary to make it work.         
         -ofn            - turn off Cholesky factorisation fudging (default). 
                                                                              
                  -- SVM specific options                          --         
                                                                              
         -om  {a,s,d,g}  - optimisation method used.  Methods are:            
                           a - active set optimisation (default).             
                           s - SMO   optimisation   (only  valid   for  binary
                               classification, regression or single-class with
                               no tube shrinking).                            
                           d - D2C  optimisation   (only   valid   for  binary
                               classification, regression or single-class with
                               no tube shrinking).                            
                           g - gradient descent (using -ofa, -of... settings).
         -ofa {0,1,2,3}  - method for 4-norm optimisation:                    
                           0: simple gradient steps.                          
                           1: line-search gradient steps (default).           
                           2: simple Newton steps.                            
                           3: line-search Newton steps.                       
                           (2 and 3 are not recommended unless using -R q).   
         -ofe e          - tolerance e>=0 for 4-norm optim (default 0.005).   
         -ofm m          - Momentum factor for 4-norm optim (default 0.05).   
         -ofr t          - Learning rate for 4-norm optim (default 0.3).      
         -ofs s          - lr scaleback factor for 4-norm optim (default 0.8).
         -oft m          - max iterations for 4-norm optim (default 100).     
         -ofM n          - max 4-kernel cache for 4-norm optim (default 1000).
                                                                              
                  -- ONN specific options                          --         
                                                                              
         -olr r          - sets learning rate (def 0.01).                     
                                                                              
                  -- SSV specific options                          --         
                                                                              
         -oge e          - tolerance e>=0 for SSV optim (default 0.005).      
         -ogm m          - Momentum factor for SSV optim (default 0.05).      
         -ogr t          - Learning rate for SSV optim (default 0.3).         
         -ogs s          - lr scaleback factor for SSV optim (default 0.8).   
         -ogt m          - max iterations for SSV optim (default 100).        
         -ogT n          - max training time for SSV optim (default 1000).    
                                                                              
                  -- MLM specific options                          --         
                                                                              
         -omr r          - Learning rate for MLM optim (default 0.3).         
         -ome d          - Stop when change in average error less than (0.02).
         -oms s          - Initialisation sparsity (default 1).               
                                                                              
Performance estimation options (after training):                              
                                                                              
                  ** The result will be stored in var(1,1). **                
                                                                              
         -tQ             - Store the actual outputs in var(1,5), var(1,6).    
         -tnQ            - Don't store the actual output (default).           
         -tvar           - Save x variance to .var file when testing.         
                                                                              
         -tn  n          - Sets performance measure:                          
                           0: error (default)                                 
                           1: 1 - precision.                                  
                           2: 1 - recall.                                     
                           3: 1 - F1 score.                                   
                           4: 1 - AUC score.                                  
                           5: 1 - sparsity.                                   
         -tm  $fn        - Sets performance measurement function, based on:   
                           var(1,2)  = error (default - see below).           
                           var(1,3)  = count vector.                          
                           var(1,4)  = confusion matrix.                      
                           var(1,37) = accuracy   (1-error   for   classifier,
                                       1/(error+eps) for regressor).          
                           var(1,38) = precision (if binary classifier).      
                           var(1,39) = recall (if binary classifier).         
                           var(1,40) = F1 score (if binary classifier).       
                           var(1,41) = AUC score (if binary classifier).      
                           var(1,42) = sparsity of ML (0 dense -> 1 sparse).  
                           all other variables are described below.           
         -tM  $fn        - Like  -tm, but  this then  directly  evaluates  the
                           performance  measurement   function  straight  away
                           without  first  evaluating  performance.  That  is,
                           this  function can  load anything  into the  result
                           variable,  allowing   non-standard  usage   of  for
                           example using -g to minimise an arbitrary function.
         -tMd d r $fn    - Like -tM,  but  evaluates  L2 distance  between  r,
                           which must be  an RKHS Vector, and $fn,  which must
                           be  a   function  of   var(0,0),   var(0,1),   ...,
                           var(0,d-1), where d is the dimension.              
         -tMD d $fn $fn  - Like  -tMd but  between two  functions, integrating
                           over [0,1]^d.                                      
         -tMv v          - Appends variance  to the result.  This  is used for
                           Bayesian optimisation  - it operates  by converting
                           the  result r  to a set  { r v }.  This  must  come
                           after the result is evaluated (eg after -tM).  Note
                           that -tC stores variance in var(1,46).             
                                                                              
         -tMpy   fname x - evaluate python script fname with scalar x.        
         -tMpyv  fname x - evaluate python script fname with vector x.        
         -tMpyf  fname x - evaluate python script fname with function x.      
                                                                              
         -tMxpy  fname x - like -tMpy, but write to lRateList.txt (15).*      
         -tMxpyv fname x - like -tMpyv, but write to lRateList.txt (15).*     
         -tMxpyf fname x - like -tMpyf, but write to lRateList.txt (15).*     
                           *result loaded from pyres.txt.                     
                                                                              
         -tMypy  fname x - like -tMpy, but write to lRateList.txt (15).*      
         -tMypyv fname x - like -tMpyv, but write to lRateList.txt (15).*     
         -tMypyf fname x - like -tMpyf, but write to lRateList.txt (15).*     
                           *result loaded from pyres.txt.                     
                                                                              
         -tMexe  fname x - evaluate executable fname with scalar x.           
         -tMexev fname x - evaluate executable fname with vector x.           
         -tMexef fname x - evaluate executable fname with function x.         
                                                                              
                  ** Function  evaluation works as  follows.  SVM- **         
                  ** Heavy works as follows:                       **         
                  **                                               **         
                  ** - SVMHeavy opens a unix socket as server.     **         
                  ** - SVMHeavy calls  external program  pname and **         
                  **   tells it the socket name, fname sockname.   **         
                  ** - SVMHeavy waits for pname to send gentype i: **         
                  **   * if !NULL, scalar x SVMHeavy returns x.    **         
                  **   * if !NULL, vector x SVMHeavy returns x(i). **         
                  **   * if !NULL, function x SVMHeavy returns x(i)**         
                  ** - when  NULL   received  SVMHeavy   waits  to **         
                  **   receive the result, then kills the socket.  **         
                  **                                               **         
                  ** The python script is  assumed to wait 5 secs, **         
                  ** open the unix socket as  client, then perform **         
                  ** as per above.                                 **         
                  **                                               **         
                  ** Example: see unixevaltest.py                  **         
                                                                              
         -tl             - Calculate  negative  (of) log-likelihood.  This  is
                           well defined for  the GPR and formally  defined (by
                           analogy) for SVM and LSV.                          
         -tx             - Calculate  leave-one-out error.  In  the regression
                           case the result is the RMS leave-one-error.        
         -tr             - Calculate recall error.  In the regression case the
                           result is the RMS recall error.                    
         -tc  n          - Calculate the n-fold cross-validation error (in the
                           regression case RMS cross-fold error).             
         -tC  m n        - Calculate the n-fold cross-validation error (in the
                           regression   case  RMS   cross-fold   error)   with
                           randomisation (ie. shuffle points before starting).
                           This variant will repeat m times and average result
                           over all runs.  Note  that the result of  this will
                           vary from run to run due to randomisation.         
         -tf  $file      - validate ML using file, result sent to file.res.   
         -tF  i j $file  - validate ML  using file,  result sent  to file.res,
                           ignoring  i vectors  at start,  testing  at  most j
                           vectors (-1 if all).                               
         -tb i j m v     - test performance of ML  with added "noise" features
                           in x vectors.  Tests range  from i->j such features
                           where each feature is gaussian random N(m,v).      
         -tV  [d] [[x]]  - test training vectors (see -AV).                   
         -tg  N d f v    - generate and test training data. N pairs generated,
                           vectors have dim d, function is f with noise var v,
                           features N(0,1).                                   
         -tG  N d f v    - generate and test training data. N pairs generated,
                           vectors have dim d, function is f with noise var v,
                           features U(0,1).                                   
         -tgc N d f v c  - like -tg, but only uses vectors x for which c(x)=1.
         -tGc N d f v c  - like -tG, but only uses vectors x for which c(x)=1.
                                                                              
                  ** If a test stalls,  it may be  useful to reset **         
                  ** the ML prior to performing each optimisation. **         
                  ** Use the z suffix  to do this.  To  obtain raw **         
                  ** ML outputs for recdiv tests, use the B suffix.**         
                  ** To  test only  on  first cross  batch use  Z. **         
                  ** Other suffixes for -tf and -tF as per -AA.    **         
                  **                                               **         
                  ** The complete list of suffixed options are:    **         
                  **                                               **         
                  ** -tx, -txz, -txB, -txzB                        **         
                  ** -tr, -trz, -trB, -trzB                        **         
                  ** -tc, -tcz, -tcB, -tczB                        **         
                  ** -tC, -tCz, -tCB, -tCzB                        **         
                  ** -tfe,-tfi,-tfl,-tfu,-tfel,-tfeu,-tfil,-tfiu,  **         
                  ** -tFe,-tFi,-tFr,-tFl,-tFu,-tFel,-tFeu,-tFil,   **         
                  ** -tFiu,-tFrl,-tFru                             **         
                  ** -tfeB,-tfiB,-tflB,-tfuB,-tfelB,-tfeuB,-tfilB,-tfiuB, **  
                  ** -tFeB,-tFiB,-tFrB,-tFlB,-tFuB,-tFelB,-tFeuB,-tFilB,  **  
                  ** -tFiuB,-tFrlB,-tFruB                          **         
                  ** -tV  (plus I/R variants)                      **         
                  **                                               **         
                  ** Single class:                                 **         
                  ** -tfl, -tFl etc: these  will assume base truth **         
                  ** labels are present in  training file and test **         
                  ** against these.                                **         
                  **                                               **         
                  ** GP Pareto:                                    **         
                  ** GP Pareto  acts as  a classifier  (interior / **         
                  ** exterior).  Hence  if test file does not have **         
                  ** class information use -tfl filename 1.        **         
                  **                                               **         
                  ** Logfiles written:                             **         
                  **                                               **         
                  ** *.method.cfm: confusion matrix.               **         
                  ** *.method.cnt: vector of numbers in each class.**         
                  ** *.method.clr: error for each class.           **         
                  ** *.method.res: actual output g(x) for vectors. **         
                  ** *.method.cla: classification h(g(x)) .     **         
                  ** *.method.gra: classification gradients.       **         
                  ** *.method.sum: results summary, containing:    **         
                  **                                               **         
                  **   Accuracy: this is 1-error for classifier or **         
                  **             1/(RMSerror+1e-6) for regressor.  **         
                  **   Precision: binary only, otherwise -1.       **         
                  **   Recall: binary only, otherwise -1.          **         
                  **   F1 score: binary only, otherwise -1.        **         
                  **   AUC: binary only, otherwise -1.             **         
                  **   Sparsity: meaning depends on ML.            **         
                  **   Error: classification error for classifier, **         
                  **             RMSerror for regressor, see above.**         
                  **                                               **         
                  ** Note:- * is the base logname set by -L or -LL **         
                  **      - method is LOO for -tx, recall for -tr, **         
                  **        nfoldcross for -tc,  filename for -tf, **         
                  **        and test for -tV (and -tW below).      **         
                  **      - for -tc and  -tC a suffix  is added to **         
                  **        .res, .cla and .gra  to indicate which **         
                  **        repetition output  refers to (eg .res0 **         
                  **        is the  output for the first  round of **         
                  **        tests run, .res1 for the second etc).  **         
                  **      - if  MEX  is  present  these  are  also **         
                  **        written to  Matlab variables  with the **         
                  **        same  name,  except that  all .'s  are **         
                  **        replaced by _'s.                       **         
                                                                              
                  -- MEX (Matlab) only options                     --         
                                                                              
         -tW  $yvar $xvar -test training vectors (see -AW)                    
                                                                              
Reporting options (after performance estimation):                             
                                                                              
         -s   $file      - write ML to $file.                                 
         -a   $file      - write alpha vector to $file.                       
         -b   $file      - write bias value to $file.                         
                                                                              
                  ** -a and -b also write to mex if present        **         
                                                                              
         -echo $fn       - echoes $fn to cerr and summary file.               
         -ECHO $fn       - echoes  $fn (evaluated and  finalised) to cerr  and
                           summary file.                                      
                                                                              
         -echosock $sock $fn - echoes $fn to unix socket server $sock.        
         -ECHOsock $sock $fn - echoes $fn (eval/final) to " " " $sock.        
                                                                              
         -K0             - evaluate K0().                                     
         -K1  [x]        - evaluate K1(x).                                    
         -K2  [x] [y]    - evaluate K2(x,y).                                  
         -K3  [x] [y] [u]- evaluate K3(x,y,u).                                
      -K4 [x] [y] [u] [v]- evaluate K4(x,y,u,v).                              
         -Km  m [x0] ... - evaluate Km(x0,...).                               
                                                                              
         -hU  [x]        - test vector (see -AU) and echo result.             
         -hY  x          - test vector (see -AY) and echo result.             
         -hZ  x m        - like -hY but for ML m.                             
         -hV  [[x]]      - test vectors (see -AV) and echo results.           
         -hW  i          - test training vector i and echo result.            
         -hX             - test all training vectors and echo result.         
                                                                              
         -hUv [x]        - variance test vector (see -AU) and echo result.    
         -hYv x          - variance test vector (see -AY) and echo result.    
         -hZv x m        - like -hYv but for ML m.                            
         -hVv [[x]]      - variance test vectors (see -AV) and echo results.  
         -hWv i          - variance test training vector i and echo result.   
         -hXv            - variance test all training vectors and echo result.
         -hVV [[x]]      - covariance matrix version of -hVv                  
                                                                              
         -hUc [x] [y]    - covariance test vectors and echo result.           
         -hYc x y        - covariance test vectors and echo result.           
         -hZc x y m      - covariance test vectors and echo result.           
         -hWc i j        - covariance test vectors and echo result.           
                                                                              
         -hP [x] p mu B n- test Pr(1:p) stability, ||.||_n norm.              
         -hp [x] p mu B  - test Pr(1:p) stability, rotated ||.||_inf norm.    
         -hPi  i p mu B n- test Pr(1:p) stability, ||.||_n norm.              
         -hpi  i p mu B  - test Pr(1:p) stability, rotated ||.||_inf norm.    
                                                                              
                  ** For -hU, -hV, -hW, -hX results are stored in: **         
                  **                                               **         
                  ** var(1,8)  = result of evaluation.             **         
                  ** var(1,9)  = classification (sgn).             **         
                  ** var(1,10) = output of ML (not just sgn).      **         
                  **                                               **         
                  ** Gradients of  g, var and cov can  be accessed **         
                  ** (when available) using augmented format.  For **         
                  ** example -hU  [ x :::  6:1  ] the  gradient of **         
                  ** g(x), and -hUv [ x :::  6:2 ] the variance of **         
                  ** d^2/dx^2 g(x).                                **         
                                                                              
                  -- MEX (Matlab) only options                     --         
                                                                              
         -hM  $dst v     - Set MATLAB variable $dst = v (evaluated).          
         -hN  n $dst     - Set  var(1,n) =  MATLAB variable  $dst.  If type is
                           ambiguous favour existing type of var(1,n).        
                                                                              
Multipass and input switching options (asynchronous):                         
                                                                              
         -Zx             - Multiple training  passes are  possible by  putting
                           commands into blocks,  separated by -Zx.  The first
                           block starts  things going,  and subsequent  blocks
                           can  adjust  parameters,  add/remove  points,   run
                           various  tests etc.   Blocks are  separated  by the
                           -Zx  flag.  Note  that a  separate logfile  will be
                           written for each block and may overwrite.          
                                                                              
         -Zmute          - mute standard error.                               
         -ZMute          - mute standard out.                                 
         -ZMUTE          - mute standard error and standard out.              
         -Zunmute        - un-mute standard error.                            
         -ZunMute        - un-mute standard out.                              
         -ZunMUTE        - un-mute standard error and standard out.           
                                                                              
         -Zs  s          - seed random number generator  with seed s.  To seed
                           with time use -Zs time.                            
                                                                              
         -Zinteract      - start interactive (god) mode operation.            
         -Zgod           - enable entry to god-mode during optimisation if key
                           pressed (default).                                 
         -Zdawkins       - disable god-mode during optimisation.              
                                                                              
         -Zif b args     - if condition b is true (non-zero) then process args
                           args should be enclosed in {}.                     
         -Zifelse b t f  - if condition b is true (non-zero) then process args
                           t, otherwise process args f.                       
         -Zwhile b args  - while condition b is true (non-zero), process args 
         -Zrep i args    - process args i times (count var(0,1000)=0,1,...).  
         -Zwait b        - pause until conditions b become true (non-zero).   
         -Zusleep n      - Sleep for n usec (accuracy is system dependent).   
         -Zmsleep n      - Sleep for n msec (accuracy is system dependent).   
         -Zsleep  n      - Sleep for n sec  (accuracy is system dependent).   
                                                                              
         -Zw  $file      - process args from $file (push).                    
         -Zk             - process args from cin (push).                      
         -Zc  n          - process args from shared stream n.                 
         -Zwf $file $of  - process args from $file, feedback to $of (push).   
         -Zkf            - process args from cin, feedback cout (").          
         -Zaw $file      - pop first, then -Zw $file.                         
         -Zak            - pop first, then -Zk.                               
         -Zac n          - pop first, then -Zc.                               
         -Zawf $file $of - pop first, then -Zwf $file $of.                    
         -Zakf           - pop first, then -Zkf.                              
         -Zu  $p         - process args from UDP socket client port $p (push).
         -Zt  $p         - process args from TCP socket client port $p (push).
         -Zn  $f         - process args from unix sock. client file $f (push).
         -ZU  $p $addr   - process args from UDP socket server port $p (push).
         -ZT  $p $addr   - process args from TCP socket server port $p (push).
         -ZN  $f         - process args from unix sock. server file $f (push).
         -Zuf $p         - like -Zu but with feedback.                        
         -Ztf $p         - like -Zt but with feedback.                        
         -Znf $f         - like -Zn but with feedback.                        
         -ZUf $p $addr   - like -ZU but with feedback.                        
         -ZTf $p $addr   - like -ZT but with feedback.                        
         -ZNf $f         - like -ZN but with feedback.                        
         -Zau $p         - pop first, then -Zu $p.                            
         -Zat $p         - pop first, then -Zt $p.                            
         -Zan $f         - pop first, then -Zn $p.                            
         -ZaU $p $addr   - pop first, then -ZU $p $addr.                      
         -ZaT $p $addr   - pop first, then -ZT $p $addr.                      
         -ZaN $f         - pop first, then -ZN $p $addr.                      
         -Zauf $p        - like -Zau but with feedback.                       
         -Zatf $p        - like -Zat but with feedback.                       
         -Zanf $f        - like -Zan but with feedback.                       
         -ZaUf $p $addr  - like -ZaU but with feedback.                       
         -ZaTf $p $addr  - like -ZaT but with feedback.                       
         -ZaNf $f        - like -ZaN but with feedback.                       
         -Za             - abandom input stream and return to previous (pop). 
                                                                              
         -Zcw n $pstring - push string onto shared stream n.                  
         -ZcW n arg      - push evaluated arg onto shared stream n.           
                                                                              
                  ** When svmheavy is  run, the command  line args **         
                  ** are placed in an istream  and the buffer gets **         
                  ** pushed onto the "input  stack".  The commands **         
                  ** -Zw,-Zk,-Zu create new istreams and push them **         
                  ** onto the same stack.  Input is taken from the **         
                  ** istream on top of the stack until that stream **         
                  ** is exhausted,  at which point  it pops it off **         
                  ** and moves  to the  next one  down.  Once  the **         
                  ** last  istream is  popped  off  the stack  the **         
                  ** program will exit.                            **         
                                                                              
                  ** Feedback is a method  of passing results back **         
                  ** when  using UDP/TCP  streaming.  If selected, **         
                  ** some results/output  (eg -echo)  will be sent **         
                  ** back up  the stream.  These results  can also **         
                  ** be sent to files or cout.                     **         
                                                                              
                  ** Shared  streams  (-Zc and  -Zac above)  allow **         
                  ** commands  to be  passed between  threads.  To **         
                  ** write a string  to a shared stream  that will **         
                  ** appear  as  input  to   whichever  thread  is **         
                  ** accessing this stream (ie has -Zc or -Zac for **         
                  ** relevant  stream  number)  use -Zcw  (put raw **         
                  ** string into stream) or  -ZcW (evaluated).  To **         
                  ** pass variables use either -ZcW (note possible **         
                  ** loss  of  precision)  or   global  variables. **         
                  ** Notes:                                        **         
                  **                                               **         
                  ** - if stream  is empty  then thread  will wait **         
                  **   indefinitely  for something to  be put into **         
                  **   it.  Thus  to terminate  a stream  you must **         
                  **   end the stream with eg.:                    **         
                  **                                               **         
                  **     -Zcw n "-Zx -Za"                          **         
                  **                                               **         
                  ** - The operations -Zcw and -ZcW are atomic, so **         
                  **   you  can  have  multiple   threads  passing **         
                  **   commands (preferably  ending with -Zx) to a **         
                  **   single stream.                              **         
                                                                              
         -Zf  $file      - create an empty file of given name.                
         -Zp  $file      - pause until $file exists.                          
         -Zff $file v    - create file and write v to it.                     
         -Zpp $file n    - pause until $file exists and read var(0,n) to it.  
                                                                              
                  ** Background  training  will  optimise  the  ML **         
                  ** while waiting for input  from the user.  Note **         
                  ** that  there is  no guarantee  that the  ML is **         
                  ** optimal  at  any given  time when  background **         
                  ** training is enabled - use fnA(h,14) to **         
                  ** see this information at any time.             **         
                                                                              
         -Zob            - turn background training off (default).            
         -ZoB            - turn background training on (stop on interupt).    
         -ZoBB           - turn background training on (train to completion). 
                                                                              
         -ZZ             - End command sequence now, even if stream nonempty. 
         -ZZZZ           - Exit now.                                          
         -ZZif t         - Run -ZZ if t evaluates true (non-zero integer).    
         -ZZZZif t       - Run -ZZZZ if t evaluates true (non-zero integer).  
                                                                              
                  ** Shortcuts:                                    **         
                  **                                               **         
                  ** 1. -Zx can be replaced by ;                   **         
                  ** 2. -ZZ can be replaced by end.                **         
                  ** 3. -ZZZZ can be replaced by exit.             **         
                  ** 4. -ZZif can be replaced by endif.            **         
                  ** 5. -ZZZZif can be replaced by exitif.         **         
                                                                              
Training examples                                                             
=================                                                             
                                                                              
Example 1: svmheavyv6 -c 1 -kt 3 -kg 20 -tc 5 -AA tr1s.txt                    
                                                                              
SV  classifier with  C/N = 1  (-c 1),  using  a gaussian  RBF kernel  function
(-kt 2) with gamma = 20  (-kg 20).  Validation carried  out using 5-fold cross
validation  (-tc 5).  Training data  is contained in  the file  tr1s.txt.  The
model file will be saved to tr1s.txt.svm and the logfile to tr1s.txt.log.     
                                                                              
                                                                              
Eg 2: svmheavyv6 -c 1 -kt 3 -kg 20 -fo 1 tr1s.txt -ANr 0 "idiv(2*var(0,1),3)" 
      -1 1 -tfi 1                                                             
                                                                              
Similar to  example 1, except  that in this  case the training  file is opened
using the -fo  flag first.  2/3 are  then randomly selected  from it using the
-ANr flag and the equation idiv(2*var(0,1),3) (the quotes may not be needed on
some OSes).  The remaining 1/3 is used for testing via the -tfi flag.         
                                                                              
                                                                              
Eg 3: svmheavyv6 -c 1 -kt 3 -kg 20 -fo 1 tr1s.txt -ANr 0 "idiv(2*var(0,1),3)" 
      -1 1 -g 2 "-c var(0,1) -kg var(0,2) -tc 5" "-c var(0,1) -kg var(0,2)"   
       fl 1 1e-1 1e1 5 fl 2 1 1e2 5 -tfi 1                                    
                                                                              
Building on  example 2, this incorporates  a grid-search to select  C and g to
minimise 5-fold cross-validation  error, with C ranging 0.1 to 10 over 5 steps
and g ranging from 1 to 100 over 5 steps (log in both cases).                 
                                                                              
                                                                              
Eg 4: svmheavyv6 -c 1 -kt 3 -kg 20 -tc 5 -AA tr1s.txt -Zx -L trx -c 2 -tc 5   
                                                                              
Similar to  example 1, but  with an  additional  block.  The  additional block
(ie. after the -Zx flag)  sets C/N = 2 (-c 2).  The model  file for the second
block will be  written to trx.svm  (-L trx), and  the logfile to  trx.log.  As
for the  first block,  validation of  the second  block is  done using  5-fold
cross-validation.                                                             
                                                                              
