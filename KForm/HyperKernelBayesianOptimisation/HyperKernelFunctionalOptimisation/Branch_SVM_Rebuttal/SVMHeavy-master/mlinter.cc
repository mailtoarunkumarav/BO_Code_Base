
//
// SVMHeavyv7 abstracted interface
//
// Version: 6
// Date: 05/12/2014
// Written by: Alistair Shilton (AlShilton@gmail.com)
// Copyright: all rights reserved
//

#include "mlinter.h"

#include <iostream>
#include <sstream>
#include <iomanip>
#include <fstream>
#include <time.h>
#include <ctype.h>
#include <string.h>
#include <string>
#include <math.h>
#include "hillclimb.h"
#include "fuzzyml.h"
#include "errortest.h"
#include "addData.h"
#include "analyseAnomaly.h"
#include "balc.h"
#include "gridopt.h"
#include "directopt.h"
#include "nelderopt.h"
#include "bayesopt.h"
#include "globalopt.h"
#include "opttest.h"
#include "paretotest.h"
#include "xferml.h"


// uncomment to process underscores in input as spaces
//#define MANGLE_UNDERSCORES


// Argument extraction
// ===================
//
// grabnextarg: extracts the next argument (string) from the relevant input
//              source as defined by streamactive (see runsvm documentation),
//              updates any relevant counters etc as required, and places the
//              result into currentarg.  Returns 0 on success, 1 if no argument
//              can be extracted.  stdcinin is used in leui of cin
//
// grabargs: extracts the next num arguments (strings) as per grabnextarg.
//           The resulting arguments are placed in destlist (currentarg
//           contains the very last argument extracted).   If allowvector is
//           set then "vectorial" arguments of the form eg:
//
//           [ blah bah ... ]
//
//           are treated as on argument (eg "[ blah bah ... ]") rather than a
//           list of individual arguments separated by whitespace (eg
//           "[","blah","bah","...","]").  Returns 0 on success, 1 on fail.
//
// safeatoi: a "safe" version of the standard atoi function.  Reads input
//           string src into an gentype variable, substitutes argvariables into
//           this, then attempts to cast the result as an integer.  Thus you
//           could have an expression like "1+var(60,6)^4" will substitute
//           the current SVM's m value (typically 2) into var(60,6), evaluate
//           the equation, and convert the result to an integer.  On success
//           this will return the integer so obtained.  On failure it will
//           throw an exception.
//
// safeatof: a "safe" version of the standard atof function.   Operation is
//           essentially the same as safeatoi, but for floats.
//
// safeatof: a "safe" version of ascii to gentype

int grabnextarg(Stack<awarestream *> &commstack, std::string &currentarg);
int grabargs(int num, Vector<Vector<std::string> > &destlist, Stack<awarestream *> &commstack, std::string &currentarg, int allowvetor = 1);
int puttylump(const std::string &src, Stack<awarestream *> &commstack);

int safeatoi(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
double safeatof(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
gentype safeatog(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
template <class T> Vector<T> &safeatoVector(Vector<T> &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
template <class T> Matrix<T> &safeatoMatrix(Matrix<T> &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
void safeatowhatever(int &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
void safeatowhatever(double &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
void safeatowhatever(gentype &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);

void stripcurlybrackets(std::string &evalargs);
void stripquotes(std::string &evalargs);

// Data File Suffix Decoder
// ========================
//
// Decodes the standard suffix string on -AA, -tf etc flags.  These have the
// form:
//
// -AA... {i j {k}} filename/number {n}
//
// where ... is the subcom, and currcommand is the vector:
//
// [ -AA...          ]
// [ {i}             ] - present if isANtype is set
// [ {j}             ] - present if isANtype is set
// [ {{k}}           ] - present if isANtype and setibase is set
// [ filename/number ] - located at position fileargpos in vector
// [ {n}             ] - present if coercefromsingle is set
//
// The following vectors are then set:
//
// int reverse           - set 1 if e present in suffix string (target-at-end format)
// int ignoreStart       - set i, or -1 if i not present
// int imax              - set j, or -1 if j not present
// int ibase             - set k, or -1 if k not present
// int uselinesvector    - set 1 if i or r present in suffix string (data taken from open file, lines in linesread)
//                       - set 2 if I or R present in suffix string (data taken from open file, lines in linesread, but linesread returned to original state after use so lines can be reused)
// int israw             - set 1 if B present in suffix string (save output in raw format)
// int startpoint        - set 1 if z present in suffix string (reset SVM for each iteration in LOO etc)
// int coercetosingle    - set 1 if u present in suffix string (read class/target but disgard)
// int coercefromsingle  - set 1 if l present in suffix string (don't read class/target but substitute given target n)
// double fromsingletar  - regression target n
// int fromsingletarget  - regression target (int) n
// std::string trainfile - filename
// Vector<int> linesread - numbers to be used if data taken from open file.

void xlateDataSourceSuffixes(int isANtype, int fileargpos, int setibase, const Vector<std::string> &currcommand, const std::string &subcom, SparseVector<SparseVector<gentype> > &argvariables, SparseVector<ofiletype> &filevariables,
     int &reverse, int &ignoreStart, int &imax, int &ibase, int &uselinesvector, int &israw, int &startpoint, int &coercetosingle, int &coercefromsingle, gentype &fromsingletarget, std::string &trainfile,Vector<int> &linesread);

// Data File Line Selection
// ========================
//
// preExtractLinesFromFile: removes unused lines from file filelines and puts
//                          them in linesread.  That is, the argument filelines
//                          keeps track of which lines in a given file have not
//                          yet been used.  This function takes unused lines out
//                          of filelines and puts them into linesread so that
//                          some other function can then extract the lines
//                          themselves from the file and do something with them.
//
// Operation depends on ignoreStart setting:
//
// ignoreStart = -1: extract maxadd (or the number of lines available if this
//                   is lower) lines from filelines, at random, and put them
//                   into the linesread vector.
// ignoreStart >= 0: extract imax (or the number of lines available, not
//                   including the first ignoreStart lines, if this is lower)
//                   lines from filelines, skipping the first ignoreStart
//                   points, and put them into the linesread vector.
//
// - The string trainfile is set to the filename contained in filelines.
// - The number of lines left in filelines after extraction is written to
//   linesleft after extraction has finished.

void preExtractLinesFromFile(ofiletype &filelines, gentype &linesleft, std::string &trainfile, int ignoreStart, int imax, Vector<int> &linesread, int randorder = 0);

// Process kernel command
//
// ktype: 0 - standard kernel
//        1 - output kernel
//        2 - kernel not associated with an ML
// currcommandis overwrites currcomand(0)

void processKernel(ML_Base &kernML, MercerKernel &theKern, const std::string &currcommandis, const Vector<std::string> &currcomand, int ktype, SparseVector<SparseVector<gentype> > &argvariables, int &kernnum, int firstcall,
                   SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext);


// SVM testing functions
// =====================
//
// Function arguments follow from the relevant test operation in the SVM clss,
// with the additional arguments:
//
// logfile: the name of the logfile that the function writes results to.
// svmbase: the SVM being tested.
// firstsum: if 1 then the logfile is opened for writing and firstsum is set
//           to 0.  Otherwise the logfile is appended to.
// finalresult: the result of the test is written here.
// resfilter: this function is used to calculate finalresult, given
//            argvariables and additionally var(0,0) = result from test,
//            var(0,1) = class count vector, var(0,2) = confusion matrix.
// argvariables: general argvariables
//
// Arguments in the testFileVectors form copy that of addtrainingData.

void testLOO        (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                                                                                                        int &firstsum,              int startpoint,                                                                                                         gentype &finalresult,                                             const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num));
void testRecall     (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                                                                                                        int &firstsum,                                                                                                                                      gentype &finalresult,                                             const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num));
void testCross      (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                                                                                                        int &firstsum, int numreps, int startpoint, int randcross, int numfolds,                                                                            gentype &finalresult,                                             const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num));
void testSparSens   (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                                                                                                        int &firstsum, int minbad, int maxbad, double noisemean, double noisevar, int startpoint,                                                           gentype &finalresult,                                             const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num));
void testFileVectors(int binaryRelabel, int singleDrop, std::string &logfile, const ML_Mutable &svmbase, std::string &tfilename, int reverse, int ignoreStart, int imax, int &firstsum,                                                           int coercetosingle, int coercefromsingle, const gentype &fromsingletarget, gentype &finalresult, int uselinesvector, Vector<int> &linesread, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), const SparseVector<gentype> &xtemplate);
void testTest       (                                   std::string &logfile, const ML_Mutable &svmbase, const Vector<SparseVector<gentype> > &xtest, const Vector<gentype> &ytest, int &firstsum, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num));
void testnegloglike (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                                                                                                        int &firstsum,                                                                                                                                      gentype &finalresult,                                             const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables,                                int (*getsetExtVar)(gentype &res, const gentype &src, int num));

int loadDataFromMatlab(const std::string &xmatname, const std::string &ymatname, Vector<SparseVector<gentype> > &x, Vector<gentype> &dz, char targtype, int (*getsetExtVar)(gentype &res, const gentype &src, int num));

// Funciton to evaluate f(x) using external python script or exe
// =============================================================
//
// isscalar = nz if scalar
// isvector = nz if vector
// ispy     = 0 if exe
//          = 1 if python
// evalname = python/exe to be called
// sf       = scalar or function to be evaluated
// v        = vector

int pyorexeeval(int isscalar, int isvector, int ispy, const std::string &evalname, const gentype &sf, const Vector<gentype> &v, gentype &finalresult);

// Help function
// =============
//
// Prints help to stream given

void printhelp(std::ostream &output, int basic = 1, int advanced = 0);
void printhelpkernel(std::ostream &output, int basic = 1, int advanced = 0);
void printhelpvars(std::ostream &output, int basic = 1, int advanced = 0);
void printhelpgentype(std::ostream &output, int basic = 1, int advanced = 0);

void emptycommstack(Stack<awarestream *> &commstack);

void gridelmrun(gentype &res, Vector<gentype> &x, void *arg);
int gridelmMLreg(int ind, ML_Mutable *MLreg, void *arg);


// Global gentype ML_Base access function

const ML_Mutable &getMLrefconst(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext);

int getparamfull(int svmInd, int fnind, gentype &val, const gentype &xa, int ia, const gentype &xb, int ib, 
              SparseVector<int> *svmThreadOwner = NULL,
              SparseVector<ML_Mutable *> *svmbase = NULL,
              int *threadInd = NULL,
              SparseVector<SVMThreadContext *> *svmContext = NULL);
int getparamfull(int svmInd, int fnind, gentype &val, const gentype &xa, int ia, const gentype &xb, int ib, 
              SparseVector<int> *xsvmThreadOwner,
              SparseVector<ML_Mutable *> *xsvmbase,
              int *xthreadInd,
              SparseVector<SVMThreadContext *> *xsvmContext)
{
    static SparseVector<int> *svmThreadOwner = NULL;
    static SparseVector<ML_Mutable *> *svmbase = NULL;
    static int *threadInd = NULL;
    static SparseVector<SVMThreadContext *> *svmContext = NULL;

    svmThreadOwner = xsvmThreadOwner ? xsvmThreadOwner : svmThreadOwner;
    svmbase        = xsvmbase        ? xsvmbase        : svmbase;
    threadInd      = xthreadInd      ? xthreadInd      : threadInd;
    svmContext     = xsvmContext     ? xsvmContext     : svmContext;

    NiceAssert( svmThreadOwner );
    NiceAssert( svmbase );
    NiceAssert( threadInd );
    NiceAssert( svmContext );

    if ( svmInd >= 0 )
    {
        return getMLrefconst(*svmThreadOwner,*svmbase,*threadInd,svmInd,*svmContext).getparam(fnind,val,xa,ia,xb,ib);
    }

    else if ( ( svmInd == -1 ) && ( ia == 0 ) )
    {
        const Vector<double> &xxx = (const Vector<double> &) xa;

        evalTestFn(fnind,val.force_double(),xxx);

        return 0;
    }

    else if ( ( svmInd == -2 ) && ( ia == 0 ) && ( ib == 0 ) )
    {
        const Vector<double> &xxx = (const Vector<double> &) xa;
        const Matrix<double> &aaa = (const Matrix<double> &) xb;

        evalTestFn(fnind,val.force_double(),xxx,&aaa);

        return 0;
    }

    else if ( ( svmInd == -3 ) && ( ia == 0 ) && ( ib == 0 ) )
    {
        const Vector<double> &xxx = (const Vector<double> &) xa;
        int MM = (int) xb;

        double mooalpha = DEFAULT_MOOALPHA; // FIXME should be adjustable

        Vector<double> rrr(MM);

        evalTestFn(fnind,xxx.size(),MM,rrr,xxx,mooalpha);

        val = rrr;

        return 0;
    }

    return 1;
}

int egetparamfull(int svmInd, int fnind, Vector<gentype> &val, const Vector<gentype> &xa, int ia, const Vector<gentype> &xb, int ib, 
              SparseVector<int> *svmThreadOwner = NULL,
              SparseVector<ML_Mutable *> *svmbase = NULL,
              int *threadInd = NULL,
              SparseVector<SVMThreadContext *> *svmContext = NULL);
int egetparamfull(int svmInd, int fnind, Vector<gentype> &val, const Vector<gentype> &xa, int ia, const Vector<gentype> &xb, int ib, 
              SparseVector<int> *xsvmThreadOwner,
              SparseVector<ML_Mutable *> *xsvmbase,
              int *xthreadInd,
              SparseVector<SVMThreadContext *> *xsvmContext)
{
    static SparseVector<int> *svmThreadOwner = NULL;
    static SparseVector<ML_Mutable *> *svmbase = NULL;
    static int *threadInd = NULL;
    static SparseVector<SVMThreadContext *> *svmContext = NULL;

    svmThreadOwner = xsvmThreadOwner ? xsvmThreadOwner : svmThreadOwner;
    svmbase        = xsvmbase        ? xsvmbase        : svmbase;
    threadInd      = xthreadInd      ? xthreadInd      : threadInd;
    svmContext     = xsvmContext     ? xsvmContext     : svmContext;

    NiceAssert( svmThreadOwner );
    NiceAssert( svmbase );
    NiceAssert( threadInd );
    NiceAssert( svmContext );

    if ( svmInd >= 0 )
    {
        return getMLrefconst(*svmThreadOwner,*svmbase,*threadInd,svmInd,*svmContext).egetparam(fnind,val,xa,ia,xb,ib);
    }

    else if ( ( svmInd == -1 ) && ( ia == 0 ) )
    {
        int k;

        NiceAssert( xa.size() == xb.size() );

        val.resize(xa.size());

        for ( k = 0 ; k < xa.size() ; k++ )
        {
            const Vector<double> &xxx = (const Vector<double> &) xa(k);

            evalTestFn(fnind,val("&",k).force_double(),xxx);
        }

        return 0;
    }

    else if ( ( svmInd == -2 ) && ( ia == 0 ) && ( ib == 0 ) )
    {
        int k;

        NiceAssert( xa.size() == xb.size() );

        val.resize(xa.size());

        for ( k = 0 ; k < xa.size() ; k++ )
        {
            const Vector<double> &xxx = (const Vector<double> &) xa(k);
            const Matrix<double> &aaa = (const Matrix<double> &) xb(k);

            evalTestFn(fnind,val("&",k).force_double(),xxx,&aaa);
        }

        return 0;
    }

    else if ( ( svmInd == -3 ) && ( ia == 0 ) && ( ib == 0 ) )
    {
        int k;

        NiceAssert( xa.size() == xb.size() );

        val.resize(xa.size());

        for ( k = 0 ; k < xa.size() ; k++ )
        {
            const Vector<double> &xxx = (const Vector<double> &) xa(k);
            int MM = (int) xb(k);

            double mooalpha = DEFAULT_MOOALPHA; // FIXME should be adjustable

            Vector<double> rrr(MM);

            evalTestFn(fnind,xxx.size(),MM,rrr.resize(MM),xxx,mooalpha);

            val.castassign(rrr);
        }

        return 0;
    }

    return 1;
}

void getparam(int svmind, int fnind, gentype &val, const gentype &xa, int ia, const gentype &xb, int ib);
void getparam(int svmind, int fnind, gentype &val, const gentype &xa, int ia, const gentype &xb, int ib)
{
    if ( getparamfull(svmind,fnind,val,xa,ia,xb,ib) )
    {
        if ( !ia && !ib )
        {
            gentype tempval("fnC(x,y,z,var(0,3))");

            gentype gsvmind(svmind);
            gentype gfnind(fnind);

            val = tempval(gsvmind,gfnind,xa,xb);
        }

        else
        {
            gentype tempval("dfnC(x,y,z,var(0,3),var(0,4),var(0,5))");

            gentype gsvmind(svmind);
            gentype gfnind(fnind);
            gentype gia(ia);
            gentype gib(ib);

            val = tempval(gsvmind,gfnind,xa,gia,xb,gib);
        }
    }

    return;
}

void egetparam(int svmind, int fnind, Vector<gentype> &val, const Vector<gentype> &xa, int ia, const Vector<gentype> &xb, int ib);
void egetparam(int svmind, int fnind, Vector<gentype> &val, const Vector<gentype> &xa, int ia, const Vector<gentype> &xb, int ib)
{
    if ( egetparamfull(svmind,fnind,val,xa,ia,xb,ib) )
    {
        if ( !ia && !ib )
        {
            gentype tempval("efnC(x,y,z,var(0,3))");

            gentype gsvmind(svmind);
            gentype gfnind(fnind);

            gentype xxa(xa);
            gentype xxb(xb);

            val = tempval(gsvmind,gfnind,xxa,xxb);
        }

        else
        {
            gentype tempval("edfnC(x,y,z,var(0,3),var(0,4),var(0,5))");

            gentype gsvmind(svmind);
            gentype gfnind(fnind);
            gentype gia(ia);
            gentype gib(ib);

            gentype xxa(xa);
            gentype xxb(xb);

            val = tempval(gsvmind,gfnind,xxa,gia,xxb,gib);
        }
    }

    return;
}

int runsvmint(int threadInd,
           SparseVector<SVMThreadContext *> &svmContext,
           SparseVector<ML_Mutable *> &svmbase,
           SparseVector<int> &svmThreadOwner,
           Stack<awarestream *> *xxxcommstack,
           svmvolatile SparseVector<SparseVector<gentype> > &globargvariables,
           int (*getsetExtVar)(gentype &res, const gentype &src, int num),
           SparseVector<SparseVector<int> > &returntag);

int runsvm(int threadInd,
           SparseVector<SVMThreadContext *> &svmContext,
           SparseVector<ML_Mutable *> &svmbase,
           SparseVector<int> &svmThreadOwner,
           Stack<awarestream *> *xxxcommstack,
           svmvolatile SparseVector<SparseVector<gentype> > &globargvariables,
           int (*getsetExtVar)(gentype &res, const gentype &src, int num),
           SparseVector<SparseVector<int> > &returntag)
{
    // Install gentype global function

    gentype temp;
    Vector<gentype> etemp;

    getparamfull(0,0,temp,temp,0,temp,0,&svmThreadOwner,&svmbase,&threadInd,&svmContext);
    egetparamfull(0,0,etemp,etemp,0,etemp,0,&svmThreadOwner,&svmbase,&threadInd,&svmContext);

    setGenFunc(getparam);
    seteGenFunc(egetparam);

//FIXME
    return runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,xxxcommstack,globargvariables,getsetExtVar,returntag);
}









void theRoundFile(char c);
void theRoundFile(char c)
{
    (void) c;
    return;
}











// Write to both file and log

#define LOGVARPREFIX "svmh_"

template <class T>
int writeLog(const Vector<T> &resy, const Vector<Vector<T> > &resx, const std::string &resfilename); // just to file for this one
template <class T>
int writeLog(const Vector<T> &resy, const Vector<T> &rest, const Vector<Vector<T> > &resx, const std::string &resfilename); // just to file for this one
template <class T>
int writeLog(const Vector<T> &res, const std::string &resfilename, int (*getsetExtVar)(gentype &res, const gentype &src, int num));
template <class T>
int writeLog(const Matrix<T> &res, const std::string &resfilename, int (*getsetExtVar)(gentype &res, const gentype &src, int num));

template <class T>
int writeLog(const Vector<T> &res, const std::string &resfilename, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    (void) getsetExtVar;

    int i;
    int ires = 0;

    // Always write to logfiles
    {
        std::ofstream resfile;
        resfile.open(resfilename.c_str(),std::ofstream::out);

        if ( !resfile.is_open() )
        {
            STRTHROW("Unable to open result gridfile "+resfilename);
        }

        if ( res.size() )
        {
            for ( i = 0 ; i < res.size() ; i++ )
            {
                resfile << res(i) << "\n";
            }
        }

        resfile.close();
    }

    //  Also log to external variables if option available

    gentype dummya;
    gentype dummyb;

    if ( !(*getsetExtVar)(dummya,dummyb,-4) )
    {
        std::string extvarname(LOGVARPREFIX);
        extvarname += resfilename;

        if ( extvarname.length() )
        {
            for ( i = 0 ; i < (int) extvarname.length() ; i++ )
            {
                extvarname[i] = ( extvarname[i] == '.' ) ? '_' : extvarname[i];
            }
        }

        gentype resalt;

        resalt.force_vector().resize(res.size());

        if ( res.size() )
        {
            for ( i = 0 ; i < res.size() ; i++ )
            {
                resalt("&",i) = res(i);
            }
        }

        gentype extvar;

        extvar.makeString(extvarname);

        ires = (*getsetExtVar)(extvar,resalt,-2);
    }

    return ires;
}

template <class T>
int writeLog(const Vector<T> &resy, const Vector<Vector<T> > &resx, const std::string &resfilename)
{
    NiceAssert( resy.size() == resx.size() );

    int i,j;
    int ires = 0;

    // Always write to logfiles
    {
        std::ofstream resfile;
        resfile.open(resfilename.c_str(),std::ofstream::out);

        if ( !resfile.is_open() )
        {
            STRTHROW("Unable to open result gridfile "+resfilename);
        }

        if ( resy.size() )
        {
            for ( i = 0 ; i < resy.size() ; i++ )
            {
                resfile << resy(i);

                for ( j = 0 ; j < resx(i).size() ; j++ )
                {
                    resfile << "\t" << resx(i)(j);
                }

                resfile << "\n";
            }
        }

        resfile.close();
    }

    // Don't log external in this case.

    return ires;
}

#define MINVARREC 1e-12

template <class T>
int writeLog(const Vector<T> &resy, const Vector<T> &ress, const Vector<Vector<T> > &resx, const std::string &resfilename)
{
    NiceAssert( resy.size() == resx.size() );
    NiceAssert( ress.size() == resx.size() );

    int i,j;
    int ires = 0;

    // Always write to logfiles
    {
        std::ofstream resfile;
        resfile.open(resfilename.c_str(),std::ofstream::out);

        if ( !resfile.is_open() )
        {
            STRTHROW("Unable to open result gridfile "+resfilename);
        }

        if ( resy.size() )
        {
            for ( i = 0 ; i < resy.size() ; i++ )
            {
                double tempsup = ( ress(i).isCastableToRealWithoutLoss() ? ((double) ress(i)) : 1.0 );

                resfile << resy(i) << "\ts" << tempsup;

                for ( j = 0 ; j < resx(i).size() ; j++ )
                {
                    resfile << "\t" << resx(i)(j);
                }

                resfile << "\n";
            }
        }

        resfile.close();
    }

    // Don't log external in this case.

    return ires;
}

template <class T>
int writeLog(const Matrix<T> &res, const std::string &resfilename, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    (void) getsetExtVar;

    int ires = 0;

    {
        std::ofstream resfile;
        resfile.open(resfilename.c_str(),std::ofstream::out);

        if ( !resfile.is_open() )
        {
            STRTHROW("Unable to open result gridfile "+resfilename);
        }

        resfile << res << "\n";

        resfile.close();
    }

    gentype dummya;
    gentype dummyb;

    if ( !(*getsetExtVar)(dummya,dummyb,-4) )
    {
        std::string extvarname(LOGVARPREFIX);
        extvarname += resfilename;

        int i,j;

        if ( extvarname.length() )
        {
            for ( i = 0 ; i < (int) extvarname.length() ; i++ )
            {
                extvarname[i] = ( extvarname[i] == '.' ) ? '_' : extvarname[i];
            }
        }

        gentype resalt;

        resalt.force_matrix().resize(res.numRows(),res.numCols());

        if ( res.numRows() && res.numCols() )
        {
            for ( i = 0 ; i < res.numRows() ; i++ )
            {
                for ( j = 0 ; j < res.numCols() ; j++ )
                {
                    resalt("&",i,j) = res(i,j);
                }
            }
        }

        gentype extvar;

        extvar.makeString(extvarname);

        ires = (*getsetExtVar)(extvar,resalt,-2);
    }

    return ires;
}












const gentype &getmacro(int i)
{
    NiceAssert( i >= 0 );
    NiceAssert( i <= 31 );

    svmvolatile static svm_mutex eyelock;
    svm_mutex_lock(eyelock);

    svmvolatile static int firstrun = 1;
    svmvolatile static gentype svmMacroDefs[32];

    if ( firstrun )
    {
        firstrun = 0;

        const_cast<gentype &>(svmMacroDefs[0]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".norm\")     -Sna"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2grid2\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0) -fW 1045 fnA(h,824)"
        " -Zx -LL (var(0,765)+\".k3\")       -kt 3 -kg 1e-2"
        " -Zx -LL (var(0,765)+\".k3grid1\")  -g 2 \"-c var(0,1) -kg var(0,2) -tC 1 10\" \"-c var(0,1) -kg var(0,2)\" fl 1 1e-2 1e2 11 fl 2 1e-2 1e2 11"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k3grid2\")  -g 2 \"-c var(0,1) -kg var(0,2) -tC 1 10\" \"-c var(0,1) -kg var(0,2)\" fl 1 1e-2 1e2 11 fl 2 1e-2 1e2 11"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k3test\")   -tC 1 10 -fW  2042 var(1,1) -fW  2043 fnA(h,0) -fW  2044 derefv(fnB(h,856,0),0) -fW 2045 fnA(h,824)"
        " -Zx -LL (var(0,765)+\".k29\")      -kt 29 -kG 0"
        " -Zx -LL (var(0,765)+\".k29grid1\") -g 2 \"-c var(0,1) -kG var(0,2) -tC 1 10\" \"-c var(0,1) -kG var(0,2)\" fl 1 1e-2 1e2 11 zb 2 0 2 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k29grid2\") -g 2 \"-c var(0,1) -kG var(0,2) -tC 1 10\" \"-c var(0,1) -kG var(0,2)\" fl 1 1e-2 1e2 11 zb 2 0 2 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k29test\")  -tC 1 10 -fW  3042 var(1,1) -fW  3043 fnA(h,0) -fW  3044 derefv(fnB(h,857,0),0) -fW 3045 fnA(h,824)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -fW 2051 \"\"Best_RBF_Parameters\"\"        -echo var(0,2051) -echo var(0,2042) -echo var(0,2043) -echo var(0,2044)"
        " -Zx -fW 3051 \"\"Best_Cosine_Parameters\"\"     -echo var(0,3051) -echo var(0,3041) -echo var(0,3042) -echo var(0,3044)"
        " -Zx -fW 1050 \"\"Selected_polynomial_kernel\"\" "
        " -Zx -fW 2050 \"\"Selected_RBF_kernel\"\" "
        " -Zx -fW 3050 \"\"Selected_arc-cosine_kernel\"\" "
        " -Zx -LL (var(0,765)+\".sel1\")     -Zif land(le(var(0,1042),var(0,2042)),le(var(0,1042),var(0,3042))) { -echo var(0,1050) -kt 2  -c var(0,1043) -kd var(0,1044) -kI var(0,1045) }"
        " -Zx -LL (var(0,765)+\".sel2\")     -Zif land(le(var(0,2042),var(0,1042)),le(var(0,2042),var(0,3042))) { -echo var(0,2050) -kt 3  -c var(0,2043) -kg var(0,2044) -kI var(0,2045) }"
        " -Zx -LL (var(0,765)+\".sel3\")     -Zif land(le(var(0,3042),var(0,1042)),le(var(0,3042),var(0,2042))) { -echo var(0,3050) -kt 29 -c var(0,3043) -kG var(0,3044) -kI var(0,3045) }"
        );

        const_cast<gentype &>(svmMacroDefs[1]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".norm\")     -Sna"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0) -fW 1045 fnA(h,824)"
        " -Zx -LL (var(0,765)+\".k3\")       -kt 3 -kg 1e-2"
        " -Zx -LL (var(0,765)+\".k3grid1\")  -g 2 \"-c var(0,1) -kg var(0,2) -tC 1 10\" \"-c var(0,1) -kg var(0,2)\" fl 1 1e-2 1e2 11 fl 2 1e-2 1e2 11"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k3test\")   -tC 1 10 -fW  2042 var(1,1) -fW  2043 fnA(h,0) -fW  2044 derefv(fnB(h,856,0),0) -fW 2045 fnA(h,824)"
        " -Zx -LL (var(0,765)+\".k29\")      -kt 29 -kG 0"
        " -Zx -LL (var(0,765)+\".k29grid1\") -g 2 \"-c var(0,1) -kG var(0,2) -tC 1 10\" \"-c var(0,1) -kG var(0,2)\" fl 1 1e-2 1e2 11 zb 2 0 2 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k29test\")  -tC 1 10 -fW  3042 var(1,1) -fW  3043 fnA(h,0) -fW  3044 derefv(fnB(h,857,0),0) -fW 3045 fnA(h,824)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -fW 2051 \"\"Best_RBF_Parameters\"\"        -echo var(0,2051) -echo var(0,2042) -echo var(0,2043) -echo var(0,2044)"
        " -Zx -fW 3051 \"\"Best_Cosine_Parameters\"\"     -echo var(0,3051) -echo var(0,3041) -echo var(0,3042) -echo var(0,3044)"
        " -Zx -fW 1050 \"\"Selected_polynomial_kernel\"\" "
        " -Zx -fW 2050 \"\"Selected_RBF_kernel\"\" "
        " -Zx -fW 3050 \"\"Selected_arc-cosine_kernel\"\" "
        " -Zx -LL (var(0,765)+\".sel1\")     -Zif land(le(var(0,1042),var(0,2042)),le(var(0,1042),var(0,3042))) { -echo var(0,1050) -kt 2  -c var(0,1043) -kd var(0,1044) -kI var(0,1045) }"
        " -Zx -LL (var(0,765)+\".sel2\")     -Zif land(le(var(0,2042),var(0,1042)),le(var(0,2042),var(0,3042))) { -echo var(0,2050) -kt 3  -c var(0,2043) -kg var(0,2044) -kI var(0,2045) }"
        " -Zx -LL (var(0,765)+\".sel3\")     -Zif land(le(var(0,3042),var(0,1042)),le(var(0,3042),var(0,2042))) { -echo var(0,3050) -kt 29 -c var(0,3043) -kG var(0,3044) -kI var(0,3045) }"
        );

        const_cast<gentype &>(svmMacroDefs[2]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".norm\")     -Sna"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0)"
        " -Zx -LL (var(0,765)+\".k3\")       -kt 3 -kg 1e-2"
        " -Zx -LL (var(0,765)+\".k3grid1\")  -g 2 \"-c var(0,1) -kg var(0,2) -tC 1 10\" \"-c var(0,1) -kg var(0,2)\" fl 1 1e-2 1e2 11 fl 2 1e-2 1e2 11"
        " -Zx -LL (var(0,765)+\".k3test\")   -tC 1 10 -fW  2042 var(1,1) -fW  2043 fnA(h,0) -fW  2044 derefv(fnB(h,856,0),0)"
        " -Zx -LL (var(0,765)+\".k29\")      -kt 29 -kG 0"
        " -Zx -LL (var(0,765)+\".k29grid1\") -g 2 \"-c var(0,1) -kG var(0,2) -tC 1 10\" \"-c var(0,1) -kG var(0,2)\" fl 1 1e-2 1e2 11 zb 2 0 2 3"
        " -Zx -LL (var(0,765)+\".k29test\")  -tC 1 10 -fW  3042 var(1,1) -fW  3043 fnA(h,0) -fW  3044 derefv(fnB(h,857,0),0)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -fW 2051 \"\"Best_RBF_Parameters\"\"        -echo var(0,2051) -echo var(0,2042) -echo var(0,2043) -echo var(0,2044)"
        " -Zx -fW 3051 \"\"Best_Cosine_Parameters\"\"     -echo var(0,3051) -echo var(0,3041) -echo var(0,3042) -echo var(0,3044)"
        " -Zx -fW 1050 \"\"Selected_polynomial_kernel\"\" "
        " -Zx -fW 2050 \"\"Selected_RBF_kernel\"\" "
        " -Zx -fW 3050 \"\"Selected_arc-cosine_kernel\"\" "
        " -Zx -echo var(0,2050)"
        " -Zx -LL (var(0,765)+\".sel1\")     -Zif land(le(var(0,1042),var(0,2042)),le(var(0,1042),var(0,3042))) { -echo var(0,1050) -kt 2  -c var(0,1043) -kd var(0,1044) }"
        " -Zx -LL (var(0,765)+\".sel2\")     -Zif land(le(var(0,2042),var(0,1042)),le(var(0,2042),var(0,3042))) { -echo var(0,2050) -kt 3  -c var(0,2043) -kg var(0,2044) }"
        " -Zx -LL (var(0,765)+\".sel3\")     -Zif land(le(var(0,3042),var(0,1042)),le(var(0,3042),var(0,2042))) { -echo var(0,3050) -kt 29 -c var(0,3043) -kG var(0,3044) }"
        );

        const_cast<gentype &>(svmMacroDefs[3]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2grid2\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0) -fW 1045 fnA(h,824)"
        " -Zx -LL (var(0,765)+\".k3\")       -kt 3 -kg 1e-2"
        " -Zx -LL (var(0,765)+\".k3grid1\")  -g 2 \"-c var(0,1) -kg var(0,2) -tC 1 10\" \"-c var(0,1) -kg var(0,2)\" fl 1 1e-2 1e2 11 fl 2 1e-2 1e2 11"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k3grid2\")  -g 2 \"-c var(0,1) -kg var(0,2) -tC 1 10\" \"-c var(0,1) -kg var(0,2)\" fl 1 1e-2 1e2 11 fl 2 1e-2 1e2 11"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k3test\")   -tC 1 10 -fW  2042 var(1,1) -fW  2043 fnA(h,0) -fW  2044 derefv(fnB(h,856,0),0) -fW 2045 fnA(h,824)"
        " -Zx -LL (var(0,765)+\".k29\")      -kt 29 -kG 0"
        " -Zx -LL (var(0,765)+\".k29grid1\") -g 2 \"-c var(0,1) -kG var(0,2) -tC 1 10\" \"-c var(0,1) -kG var(0,2)\" fl 1 1e-2 1e2 11 zb 2 0 2 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k29grid2\") -g 2 \"-c var(0,1) -kG var(0,2) -tC 1 10\" \"-c var(0,1) -kG var(0,2)\" fl 1 1e-2 1e2 11 zb 2 0 2 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k29test\")  -tC 1 10 -fW  3042 var(1,1) -fW  3043 fnA(h,0) -fW  3044 derefv(fnB(h,857,0),0) -fW 3045 fnA(h,824)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -fW 2051 \"\"Best_RBF_Parameters\"\"        -echo var(0,2051) -echo var(0,2042) -echo var(0,2043) -echo var(0,2044)"
        " -Zx -fW 3051 \"\"Best_Cosine_Parameters\"\"     -echo var(0,3051) -echo var(0,3041) -echo var(0,3042) -echo var(0,3044)"
        " -Zx -fW 1050 \"\"Selected_polynomial_kernel\"\" "
        " -Zx -fW 2050 \"\"Selected_RBF_kernel\"\" "
        " -Zx -fW 3050 \"\"Selected_arc-cosine_kernel\"\" "
        " -Zx -LL (var(0,765)+\".sel1\")     -Zif land(le(var(0,1042),var(0,2042)),le(var(0,1042),var(0,3042))) { -echo var(0,1050) -kt 2  -c var(0,1043) -kd var(0,1044) -kI var(0,1045) }"
        " -Zx -LL (var(0,765)+\".sel2\")     -Zif land(le(var(0,2042),var(0,1042)),le(var(0,2042),var(0,3042))) { -echo var(0,2050) -kt 3  -c var(0,2043) -kg var(0,2044) -kI var(0,2045) }"
        " -Zx -LL (var(0,765)+\".sel3\")     -Zif land(le(var(0,3042),var(0,1042)),le(var(0,3042),var(0,2042))) { -echo var(0,3050) -kt 29 -c var(0,3043) -kG var(0,3044) -kI var(0,3045) }"
        );

        const_cast<gentype &>(svmMacroDefs[4]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0) -fW 1045 fnA(h,824)"
        " -Zx -LL (var(0,765)+\".k3\")       -kt 3 -kg 1e-2"
        " -Zx -LL (var(0,765)+\".k3grid1\")  -g 2 \"-c var(0,1) -kg var(0,2) -tC 1 10\" \"-c var(0,1) -kg var(0,2)\" fl 1 1e-2 1e2 11 fl 2 1e-2 1e2 11"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k3test\")   -tC 1 10 -fW  2042 var(1,1) -fW  2043 fnA(h,0) -fW  2044 derefv(fnB(h,856,0),0) -fW 2045 fnA(h,824)"
        " -Zx -LL (var(0,765)+\".k29\")      -kt 29 -kG 0"
        " -Zx -LL (var(0,765)+\".k29grid1\") -g 2 \"-c var(0,1) -kG var(0,2) -tC 1 10\" \"-c var(0,1) -kG var(0,2)\" fl 1 1e-2 1e2 11 zb 2 0 2 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k29test\")  -tC 1 10 -fW  3042 var(1,1) -fW  3043 fnA(h,0) -fW  3044 derefv(fnB(h,857,0),0) -fW 3045 fnA(h,824)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -fW 2051 \"\"Best_RBF_Parameters\"\"        -echo var(0,2051) -echo var(0,2042) -echo var(0,2043) -echo var(0,2044)"
        " -Zx -fW 3051 \"\"Best_Cosine_Parameters\"\"     -echo var(0,3051) -echo var(0,3041) -echo var(0,3042) -echo var(0,3044)"
        " -Zx -fW 1050 \"\"Selected_polynomial_kernel\"\" "
        " -Zx -fW 2050 \"\"Selected_RBF_kernel\"\" "
        " -Zx -fW 3050 \"\"Selected_arc-cosine_kernel\"\" "
        " -Zx -LL (var(0,765)+\".sel1\")     -Zif land(le(var(0,1042),var(0,2042)),le(var(0,1042),var(0,3042))) { -echo var(0,1050) -kt 2  -c var(0,1043) -kd var(0,1044) -kI var(0,1045) }"
        " -Zx -LL (var(0,765)+\".sel2\")     -Zif land(le(var(0,2042),var(0,1042)),le(var(0,2042),var(0,3042))) { -echo var(0,2050) -kt 3  -c var(0,2043) -kg var(0,2044) -kI var(0,2045) }"
        " -Zx -LL (var(0,765)+\".sel3\")     -Zif land(le(var(0,3042),var(0,1042)),le(var(0,3042),var(0,2042))) { -echo var(0,3050) -kt 29 -c var(0,3043) -kG var(0,3044) -kI var(0,3045) }"
        );

        const_cast<gentype &>(svmMacroDefs[5]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0)"
        " -Zx -LL (var(0,765)+\".k3\")       -kt 3 -kg 1e-2"
        " -Zx -LL (var(0,765)+\".k3grid1\")  -g 2 \"-c var(0,1) -kg var(0,2) -tC 1 10\" \"-c var(0,1) -kg var(0,2)\" fl 1 1e-2 1e2 11 fl 2 1e-2 1e2 11"
        " -Zx -LL (var(0,765)+\".k3test\")   -tC 1 10 -fW  2042 var(1,1) -fW  2043 fnA(h,0) -fW  2044 derefv(fnB(h,856,0),0)"
        " -Zx -LL (var(0,765)+\".k29\")      -kt 29 -kG 0"
        " -Zx -LL (var(0,765)+\".k29grid1\") -g 2 \"-c var(0,1) -kG var(0,2) -tC 1 10\" \"-c var(0,1) -kG var(0,2)\" fl 1 1e-2 1e2 11 zb 2 0 2 3"
        " -Zx -LL (var(0,765)+\".k29test\")  -tC 1 10 -fW  3042 var(1,1) -fW  3043 fnA(h,0) -fW  3044 derefv(fnB(h,857,0),0)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -fW 2051 \"\"Best_RBF_Parameters\"\"        -echo var(0,2051) -echo var(0,2042) -echo var(0,2043) -echo var(0,2044)"
        " -Zx -fW 3051 \"\"Best_Cosine_Parameters\"\"     -echo var(0,3051) -echo var(0,3041) -echo var(0,3042) -echo var(0,3044)"
        " -Zx -fW 1050 \"\"Selected_polynomial_kernel\"\" "
        " -Zx -fW 2050 \"\"Selected_RBF_kernel\"\" "
        " -Zx -fW 3050 \"\"Selected_arc-cosine_kernel\"\" "
        " -Zx -LL (var(0,765)+\".sel1\")     -Zif land(le(var(0,1042),var(0,2042)),le(var(0,1042),var(0,3042))) { -echo var(0,1050) -kt 2  -c var(0,1043) -kd var(0,1044) }"
        " -Zx -LL (var(0,765)+\".sel2\")     -Zif land(le(var(0,2042),var(0,1042)),le(var(0,2042),var(0,3042))) { -echo var(0,2050) -kt 3  -c var(0,2043) -kg var(0,2044) }"
        " -Zx -LL (var(0,765)+\".sel3\")     -Zif land(le(var(0,3042),var(0,1042)),le(var(0,3042),var(0,2042))) { -echo var(0,3050) -kt 29 -c var(0,3043) -kG var(0,3044) }"
        );





        const_cast<gentype &>(svmMacroDefs[6]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".norm\")     -Sna"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2grid2\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0) -fW 1045 fnA(h,824)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -LL (var(0,765)+\".sel1\")     -kt 2  -c var(0,1043) -kd var(0,1044) -kI var(0,1045)"
        );

        const_cast<gentype &>(svmMacroDefs[7]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".norm\")     -Sna"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0) -fW 1045 fnA(h,824)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -LL (var(0,765)+\".sel1\")     -kt 2  -c var(0,1043) -kd var(0,1044) -kI var(0,1045)"
        );

        const_cast<gentype &>(svmMacroDefs[8]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".norm\")     -Sna"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -LL (var(0,765)+\".sel1\")     -kt 2  -c var(0,1043) -kd var(0,1044)"
        );

        const_cast<gentype &>(svmMacroDefs[9]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2grid2\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0) -fW 1045 fnA(h,824)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -LL (var(0,765)+\".sel1\")     -kt 2  -c var(0,1043) -kd var(0,1044) -kI var(0,1045)"
        );

        const_cast<gentype &>(svmMacroDefs[10]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2feat1\")  -fsx -fSx"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0) -fW 1045 fnA(h,824)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -LL (var(0,765)+\".sel1\")     -kt 2  -c var(0,1043) -kd var(0,1044) -kI var(0,1045)"
        );

        const_cast<gentype &>(svmMacroDefs[11]).makeString( 
        " -Zx -fW 765 var(1,12)"
        " -Zx -LL (var(0,765)+\".k2\")       -kt 2 -kd 1"
        " -Zx -LL (var(0,765)+\".k2grid1\")  -g 2 \"-c var(0,1) -kd var(0,2) -tC 1 10\" \"-c var(0,1) -kd var(0,2)\" fl 1 1e-2 1e2 11 zb 2 1 3 3"
        " -Zx -LL (var(0,765)+\".k2test\")   -tC 1 10 -fW  1042 var(1,1) -fW  1043 fnA(h,0) -fW  1044 derefv(fnB(h,857,0),0)"
        " -Zx -???"
        " -Zx -fW 1051 \"\"Best_Polynomial_Parameters\"\" -echo var(0,1051) -echo var(0,1042) -echo var(0,1043) -echo var(0,1044)"
        " -Zx -LL (var(0,765)+\".sel1\")     -kt 2  -c var(0,1043) -kd var(0,1044)"
        );

        const_cast<gentype &>(svmMacroDefs[12]).makeString( 
        " -Zx -g 1 \"-c var(0,100) -tl\" \"-c var(0,100)\" fl 100 1e-2 1e2 21"
        );

        const_cast<gentype &>(svmMacroDefs[13]).makeString( 
        " -Zx -g 1 \"-c var(0,100) -tr\" \"-c var(0,100)\" fl 100 1e-2 1e2 21"
        );

        const_cast<gentype &>(svmMacroDefs[14]).makeString( 
        " -Zx -g 1 \"-c var(0,100) -tx\" \"-c var(0,100)\" fl 100 1e-2 1e2 21"
        );

        const_cast<gentype &>(svmMacroDefs[15]).makeString( 
        " -Zx -g 1 \"-c var(0,100) -tC 1 10\" \"-c var(0,100)\" fl 100 1e-2 1e2 21"
        );

        const_cast<gentype &>(svmMacroDefs[16]).makeString( 
        " -Zx -g 2 \"-c var(0,100) -kg var(0,200) -tl\" \"-c var(0,100) -kg var(0,200)\" fl 100 1e-2 1e2 11 fl 200 5e-2*sqrt(fnA(h,7)) 5e1*sqrt(fnA(h,7)) 11"
        );

        const_cast<gentype &>(svmMacroDefs[17]).makeString( 
        " -Zx -g 2 \"-c var(0,100) -kg var(0,200) -tr\" \"-c var(0,100) -kg var(0,200)\" fl 100 1e-2 1e2 11 fl 200 5e-2*sqrt(fnA(h,7)) 5e1*sqrt(fnA(h,7)) 11"
        );

        const_cast<gentype &>(svmMacroDefs[18]).makeString( 
        " -Zx -g 2 \"-c var(0,100) -kg var(0,200) -tx\" \"-c var(0,100) -kg var(0,200)\" fl 100 1e-2 1e2 11 fl 200 5e-2*sqrt(fnA(h,7)) 5e1*sqrt(fnA(h,7)) 11"
        );

        const_cast<gentype &>(svmMacroDefs[19]).makeString( 
        " -Zx -g 2 \"-c var(0,100) -kg var(0,200) -tC 1 10\" \"-c var(0,100) -kg var(0,200)\" fl 100 1e-2 1e2 11 fl 200 5e-2*sqrt(fnA(h,7)) 5e1*sqrt(fnA(h,7)) 11"
        );

        const_cast<gentype &>(svmMacroDefs[20]).makeString( 
        " -Zx -g 2 \"-c var(0,100) -kd var(0,200) -tl\" \"-c var(0,100) -kd var(0,200)\" fl 100 1e-2 1e2 11 zb 200 1 5 5"
        );

        const_cast<gentype &>(svmMacroDefs[21]).makeString( 
        " -Zx -g 2 \"-c var(0,100) -kd var(0,200) -tr\" \"-c var(0,100) -kd var(0,200)\" fl 100 1e-2 1e2 11 zb 200 1 5 5"
        );

        const_cast<gentype &>(svmMacroDefs[22]).makeString( 
        " -Zx -g 2 \"-c var(0,100) -kd var(0,200) -tx\" \"-c var(0,100) -kd var(0,200)\" fl 100 1e-2 1e2 11 zb 200 1 5 5"
        );

        const_cast<gentype &>(svmMacroDefs[23]).makeString( 
        " -Zx -g 2 \"-c var(0,100) -kd var(0,200) -tC 1 10\" \"-c var(0,100) -kd var(0,200)\" fl 100 1e-2 1e2 11 zb 200 1 5 5"
        );

        const_cast<gentype &>(svmMacroDefs[24]).makeString( 
        " -Zx -g 1 \"-kg var(0,200) -tl\" \"-kg var(0,200)\" fl 200 5e-2*sqrt(fnA(h,7)) 5e1*sqrt(fnA(h,7)) 20"
        );

        const_cast<gentype &>(svmMacroDefs[25]).makeString( 
        " -Zx -g 1 \"-kg var(0,200) -tr\" \"-kg var(0,200)\" fl 200 5e-2*sqrt(fnA(h,7)) 5e1*sqrt(fnA(h,7)) 21"
        );

        const_cast<gentype &>(svmMacroDefs[26]).makeString( 
        " -Zx -g 1 \"-kg var(0,200) -tx\" \"-kg var(0,200)\" fl 200 5e-2*sqrt(fnA(h,7)) 5e1*sqrt(fnA(h,7)) 21"
        );

        const_cast<gentype &>(svmMacroDefs[27]).makeString( 
        " -Zx -g 1 \"-kg var(0,200) -tC 1 10\" \"-kg var(0,200)\" fl 200 5e-2*sqrt(fnA(h,7)) 5e1*sqrt(fnA(h,7)) 21"
        );

        const_cast<gentype &>(svmMacroDefs[28]).makeString( 
        " -Zx -g 1 \"-kd var(0,200) -tl\" \"-kd var(0,200)\" zb 200 1 5 5"
        );

        const_cast<gentype &>(svmMacroDefs[29]).makeString( 
        " -Zx -g 1 \"-kd var(0,200) -tr\" \"-kd var(0,200)\" zb 200 1 5 5"
        );

        const_cast<gentype &>(svmMacroDefs[30]).makeString( 
        " -Zx -g 1 \"-kd var(0,200) -tx\" \"-kd var(0,200)\" zb 200 1 5 5"
        );

        const_cast<gentype &>(svmMacroDefs[31]).makeString( 
        " -Zx -g 1 \"-kd var(0,200) -tC 1 10\" \"-kd var(0,200)\" zb 200 1 5 5"
        );
    }

    svm_mutex_unlock(eyelock);

    return const_cast<gentype &>(svmMacroDefs[i]);
}













// Background training functionality

class bgTrainData;
class bgTrainData
{
public:
    ML_Mutable &svmbase;
    svmvolatile int &trainKillSwitch;
    svmvolatile int &traincontrolThreadInd;
};

void *brTrainRun(void *svmBGContext);
void *brTrainRun(void *svmBGContext)
{
    // Function called while waiting for input to train the SVM in a
    // background thread.

    ML_Mutable &svmbase = ((*((bgTrainData *) svmBGContext)).svmbase);
    svmvolatile int &trainKillSwitch = (*((bgTrainData *) svmBGContext)).trainKillSwitch;
    svmvolatile int &traincontrolThreadInd = (*((bgTrainData *) svmBGContext)).traincontrolThreadInd;

    // Train the SVM, with killswitch referred back to other thread. 

    errstream() << "\n\n*** Background training commenced in sub-thread " << traincontrolThreadInd << ".\n\n";

    int resdummy = 0;
    svmbase.train(resdummy,trainKillSwitch);

    if ( svmbase.isTrained() )
    {
        errstream() << "\n\n~~~ Background training successful in sub-thread " << traincontrolThreadInd << ".\n\n";
    }

    else
    {
        errstream() << "\n\n### Background training interrupted in sub-thread " << traincontrolThreadInd << ".\n\n";
    }

    // Signal training complete

    traincontrolThreadInd = -1;

    return NULL;
}

// grabsvm: get SVM with relevant index and *own* it in thread etc
// regsvm: register SVM whattoreg at first available index >= svmInd
//         returns index where registered

void grabsvm(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext);
int regsvm(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext, ML_Mutable &whattoreg);
//const ML_Mutable &getMLrefconst(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext);
ML_Mutable &getMLref(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext);

void killallthreads(SparseVector<SVMThreadContext *> &svmContext, int killmain)
{
    svmvolatile static svm_mutex eyelock;
    svm_mutex_lock(eyelock);

    // First need to set killswitch on potentially blocking shared stack
    // inter-thread comms

    awarestream dummy;

    dummy.killfifo();

    // Now continue with thread destruction

    if ( svmContext.indsize() )
    {
        int i,threadInd;

        for ( i = svmContext.indsize()-1 ; i >= 0 ; i-- )
        {
            threadInd = svmContext.ind(i);

            svmvolatile int &killswitch       = (*(svmContext.direref(i))).killswitch;
            svmvolatile int &controlThreadInd = (*(svmContext.direref(i))).controlThreadInd;
    
            killswitch = 1;

            while ( controlThreadInd != -1 )
            {
                svm_msleep(10);
            }

            if ( threadInd || killmain )
            {
                MEMDEL(svmContext.direref(i));
                svmContext.zero(threadInd);
            }
        }
    }

    svm_mutex_unlock(eyelock);

    return;
}

void deleteMLs(SparseVector<ML_Mutable *> &svmbase)
{
    svmvolatile static svm_mutex eyelock;
    svm_mutex_lock(eyelock);

    if ( svmbase.indsize() )
    {
        int i,svmInd;

        for ( i = svmbase.indsize()-1 ; i >= 0 ; i-- )
        {
            svmInd = svmbase.ind(i);

            MEMDEL(svmbase.direref(i));
            svmbase.zero(svmInd);
        }
    }

    svm_mutex_unlock(eyelock);

    return;
}

void grabsvm(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext)
{
    svmvolatile static svm_mutex eyelock;
    svm_mutex_lock(eyelock);

    // If svmInd does not exist we need to make it and own it

    if ( svmbase(svmInd) == NULL ) 
    { 
        MEMNEW(const_cast<svmvolatile ML_Mutable *&>(svmbase("&",svmInd)),ML_Mutable); 
    }

    if ( !svmThreadOwner.isindpresent(svmInd) )
    {
        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
    }

    int threadOwner = svmThreadOwner("&",svmInd);

    // If svmInd is owned be a dead thread we need to claim it

    if ( ( threadOwner == -1 ) || !svmContext.isindpresent(threadOwner) )
    {
        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
        threadOwner = threadInd;
    }

    // If another thread owns svmInd we need to kill that thread (or wait for it to die) and claim it

    if ( threadOwner != threadInd )
    {
        svmvolatile int &killswitch       = (*(svmContext("&",threadOwner))).killswitch;
        svmvolatile int &controlThreadInd = (*(svmContext("&",threadOwner))).controlThreadInd;

        int killmethod = (*(svmContext("&",threadInd))).killmethod;
    
        // Cannot proceed until we own this thread.

        int oldkillswitch = killswitch;

//FIXME add option to just wait
        killswitch = killmethod;

        while ( controlThreadInd != -1 )
        {
            svm_msleep(10);
        }

        killswitch = oldkillswitch;

        if ( threadOwner )
        {
            // Kill all but the main thread

            MEMDEL(svmContext("&",threadOwner));
            svmContext.zero(threadOwner);
        }

        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
        threadOwner = threadInd;
    }

    svm_mutex_unlock(eyelock);

    return;
}

int regsvm(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext, ML_Mutable *whattoreg)
{
    svmvolatile static svm_mutex eyelock;
    svm_mutex_lock(eyelock);

    // Put whattoreg at nearest available ind

    while ( svmbase(svmInd) != NULL )
    {
        svmInd++;
    }

    const_cast<svmvolatile ML_Mutable *&>(svmbase("&",svmInd)) = whattoreg;

    // The rest is basically the same as grabsvm

    if ( !svmThreadOwner.isindpresent(svmInd) )
    {
        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
    }

    int threadOwner = svmThreadOwner("&",svmInd);

    // If svmInd is owned be a dead thread we need to claim it

    if ( ( threadOwner == -1 ) || !svmContext.isindpresent(threadOwner) )
    {
        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
        threadOwner = threadInd;
    }

    // If another thread owns svmInd we need to kill that thread (or wait for it to die) and claim it

    if ( threadOwner != threadInd )
    {
        svmvolatile int &killswitch       = (*(svmContext("&",threadOwner))).killswitch;
        svmvolatile int &controlThreadInd = (*(svmContext("&",threadOwner))).controlThreadInd;

        int killmethod = (*(svmContext("&",threadInd))).killmethod;
    
        // Cannot proceed until we own this thread.

        int oldkillswitch = killswitch;

//FIXME add option to just wait
        killswitch = killmethod;

        while ( controlThreadInd != -1 )
        {
            svm_msleep(10);
        }

        killswitch = oldkillswitch;

        if ( threadOwner )
        {
            // Kill all but the main thread

            MEMDEL(svmContext("&",threadOwner));
            svmContext.zero(threadOwner);
        }

        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
        threadOwner = threadInd;
    }

    svm_mutex_unlock(eyelock);

    return svmInd;
}

const ML_Mutable &getMLrefconst(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext)
{
    grabsvm(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

    const ML_Mutable &res = *((svmbase)(svmInd));

    return res;
}

ML_Mutable &getMLref(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext)
{
    grabsvm(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

    return *((svmbase)("&",svmInd));
}























// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================

#define FILETESTRATE 10
#define DEFAULTLOGFILE "logfile"

// callsvm: gridargvars should be a copy of argvariables with any additional
// variables only required for the call written over.

int callsvm(int threadInd,
           SparseVector<SVMThreadContext *> &svmContext,
           SparseVector<ML_Mutable *> &svmbase,
           SparseVector<int> &svmThreadOwner,
           Stack<awarestream *> *xxxgridcommstack,
           svmvolatile SparseVector<SparseVector<gentype> > &globargvariables,
           int (*getsetExtVar)(gentype &res, const gentype &src, int num),
           SparseVector<SparseVector<gentype> > &gridargvars,
           int locverblevel,
           gentype &locfinalresult,
           std::string &loclogfile);

int callsvm(int threadInd,
           SparseVector<SVMThreadContext *> &svmContext,
           SparseVector<ML_Mutable *> &svmbase,
           SparseVector<int> &svmThreadOwner,
           Stack<awarestream *> *xxxgridcommstack,
           svmvolatile SparseVector<SparseVector<gentype> > &globargvariables,
           int (*getsetExtVar)(gentype &res, const gentype &src, int num),
           SparseVector<SparseVector<gentype> > &gridargvars,
           int locverblevel,
           gentype &locfinalresult,
           std::string &loclogfile)
{
    // Get variables

    Stack<awarestream *> &gridcommstack = *xxxgridcommstack;

    int                                  &verblevel         = (*(svmContext("&",threadInd))).verblevel;
    gentype                              &finalresult       = (*(svmContext("&",threadInd))).finalresult;
  //int                                  &svmInd            = (*(svmContext("&",threadInd))).svmInd;
  //gentype                              &biasdefault       = (*(svmContext("&",threadInd))).biasdefault;
  //SparseVector<gentype>                &xtemplate         = (*(svmContext("&",threadInd))).xtemplate;
    SparseVector<SparseVector<gentype> > &argvariables      = (*(svmContext("&",threadInd))).argvariables;
  //SparseVector<ofiletype>              &filevariables     = (*(svmContext("&",threadInd))).filevariables;
    int                                  &depthin           = (*(svmContext("&",threadInd))).depthin;
  //int                                  &bgTrainOn         = (*(svmContext("&",threadInd))).bgTrainOn;
    std::string                          &logfile           = (*(svmContext("&",threadInd))).logfile;
  //int                                  &binaryRelabel     = (*(svmContext("&",threadInd))).binaryRelabel;
  //int                                  &singleDrop        = (*(svmContext("&",threadInd))).singleDrop;
  //int                                  &updateargvars     = (*(svmContext("&",threadInd))).updateargvars;
  //int                                  &killmethod        = (*(svmContext("&",threadInd))).killmethod;
  //Stack<int>                           &MLindstack        = (*(svmContext("&",threadInd))).MLindstack;
  //svmvolatile int                      &thread_killswitch = (*(svmContext("&",threadInd))).killswitch;
  //svmvolatile int                      &controlThreadInd  = (*(svmContext("&",threadInd))).controlThreadInd;

    // Save copy of variables

    int tempverblevel = verblevel;
    gentype tempfinalresult = finalresult;

    verblevel = locverblevel;
    finalresult = 0.0;

    depthin++;

    qswap(argvariables,gridargvars);  // temporarily swap argvariables (which we want to preserve) and gridargvars (which will be used by the call)
    qswap(logfile,loclogfile);

    SparseVector<SparseVector<int> > returntag;

    int res = runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,&gridcommstack,globargvariables,getsetExtVar,returntag);

    locfinalresult = finalresult;

    qswap(logfile,loclogfile);
    qswap(argvariables,gridargvars); // swap them back

    depthin--;

    verblevel = tempverblevel;
    finalresult = tempfinalresult;

    // Retain variables that have been returned

    if ( returntag.size() )
    {
        int i,j,ii,jj;

        for ( i = 0 ; i < returntag.indsize() ; i++ )
        {
            ii = returntag.ind(i);

            if ( returntag(ii).size() )
            {
                for ( j = 0 ; j < returntag(ii).indsize() ; j++ )
                {
                    jj = returntag(ii).ind(j);
                    argvariables("&",ii)("&",jj) = gridargvars(ii)(jj);
                }
            }
        }
    }

    return res;
}

#define WAIT_BASESLEEP         100000
#define WAIT_ADDSLEEP_RAND     10000

int runsvmint(int threadInd,
           SparseVector<SVMThreadContext *> &svmContext,
           SparseVector<ML_Mutable *> &svmbase,
           SparseVector<int> &svmThreadOwner,
           Stack<awarestream *> *xxxcommstack,
           svmvolatile SparseVector<SparseVector<gentype> > &globargvariables,
           int (*getsetExtVar)(gentype &res, const gentype &src, int num),
           SparseVector<SparseVector<int> > &returntag)
{
    BLK_Generic::getsetExtVar = getsetExtVar;

    int doopt = 1;

    // Get variables

    Stack<awarestream *> &commstack = *xxxcommstack;

    int                                  &verblevel         = (*(svmContext("&",threadInd))).verblevel;
    gentype                              &finalresult       = (*(svmContext("&",threadInd))).finalresult;
    int                                  &svmInd            = (*(svmContext("&",threadInd))).svmInd;
    gentype                              &biasdefault       = (*(svmContext("&",threadInd))).biasdefault;
    SparseVector<gentype>                &xtemplate         = (*(svmContext("&",threadInd))).xtemplate;
    SparseVector<SparseVector<gentype> > &argvariables      = (*(svmContext("&",threadInd))).argvariables;
    SparseVector<ofiletype>              &filevariables     = (*(svmContext("&",threadInd))).filevariables;
    int                                  &depthin           = (*(svmContext("&",threadInd))).depthin;
    int                                  &bgTrainOn         = (*(svmContext("&",threadInd))).bgTrainOn;
    std::string                          &logfile           = (*(svmContext("&",threadInd))).logfile;
    int                                  &binaryRelabel     = (*(svmContext("&",threadInd))).binaryRelabel;
    int                                  &singleDrop        = (*(svmContext("&",threadInd))).singleDrop;
    int                                  &updateargvars     = (*(svmContext("&",threadInd))).updateargvars;
    int                                  &killmethod        = (*(svmContext("&",threadInd))).killmethod;
    Stack<int>                           &MLindstack        = (*(svmContext("&",threadInd))).MLindstack;
    svmvolatile int                      &thread_killswitch = (*(svmContext("&",threadInd))).killswitch;
    svmvolatile int                      &controlThreadInd  = (*(svmContext("&",threadInd))).controlThreadInd;

    svmvolatile static svm_mutex globargvariableslock;

    // Take control of this thread

    controlThreadInd = threadInd;

    // ...and go

    int iscomment = 0; // set to indicate processing comment /* ... */
    int stopnow   = 0; // set to cause exit
    int retval    = 0; // return value on exit
    std::string currentarg;
    int i;
    #ifdef ALLOW_SOCKETS
    int tcpfeedback = 0;
    #endif

    // Increment "depth" counter

    depthin++;

    // Time recording variables

    timediffunits loggingtime     = 0;
    timediffunits multiruntime    = 0;
    timediffunits svmsetuptime    = 0;
    timediffunits svmpresetuptime = 0;
    timediffunits preloadtime     = 0;
    timediffunits loadtime        = 0;
    timediffunits postloadtime    = 0;
    timediffunits learningtime    = 0;
    timediffunits kerneltime      = 0;
    timediffunits tuningtime      = 0;
    timediffunits gridtime        = 0;
    timediffunits xfertime        = 0;
    timediffunits featuretime     = 0;
    timediffunits fuzzytime       = 0;
    timediffunits boottime        = 0;
    timediffunits macrotime       = 0;
    timediffunits optimtime       = 0;
    timediffunits performtime     = 0;
    timediffunits reporttime      = 0;

    // Command processing vectors

    Vector<Vector<std::string> > loggingopt;
    Vector<Vector<std::string> > multirunopt;
    Vector<Vector<std::string> > svmsetupopt;
    Vector<Vector<std::string> > svmpresetupopt;
    Vector<Vector<std::string> > preloadopt;
    Vector<Vector<std::string> > loadopt;
    Vector<Vector<std::string> > postloadopt;
    Vector<Vector<std::string> > learningopt;
    Vector<Vector<std::string> > kernelopt;
    Vector<Vector<std::string> > tuningopt;
    Vector<Vector<std::string> > gridopt;
    Vector<Vector<std::string> > xferopt;
    Vector<Vector<std::string> > featureopt;
    Vector<Vector<std::string> > fuzzyopt;
    Vector<Vector<std::string> > bootopt;
    Vector<Vector<std::string> > macroopt;
    Vector<Vector<std::string> > optimopt;
    Vector<Vector<std::string> > performopt;
    Vector<Vector<std::string> > reportopt;

    int argbatchsize = 0;
    int skipon = 0;

    // Loop until stop or kill encountered

    while ( !stopnow && !thread_killswitch )
    {
        skipon = 0;

        // Set variables

        if ( updateargvars && !argbatchsize )
        {
            argvariables("&",42)("&",42) = svmInd;
            updateargvars = 0;
        }

        // Train the ML in the background if background training is on, no arguments have
        // yet been processed in this block, this is a top-level function, and ML isn't
        // trained already

        if ( !argbatchsize && ( depthin == 1 ) && !(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isTrained()) && bgTrainOn )
        {
            int newThreadInd = 10000; // default offset for non-user threads

            // Enter locked segment to prevent clashes in background thread assignment

            svmvolatile static svm_mutex eyelock;
            svm_mutex_lock(eyelock);

            // Find first free index to create new thread

            while ( svmContext.isindpresent(newThreadInd) )
            {
                newThreadInd++;
            }

            // Create thread data

            MEMNEW(const_cast<svmvolatile SVMThreadContext *&>((svmContext)("&",newThreadInd)),SVMThreadContext(*svmContext(threadInd),newThreadInd));

            // Have claimed thread handle, so can leave locked segment safely

            svm_mutex_unlock(eyelock);

            // Save some typing

            svmvolatile SVMThreadContext &locContext = (*(const_cast<svmvolatile SVMThreadContext *>((svmContext)("&",newThreadInd))));

            // Set up background training structure, lock the SVM to the new thread

            bgTrainData svmTrainOn = { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),locContext.killswitch,locContext.controlThreadInd};
            svmThreadOwner("&",svmInd) = newThreadInd;

            // Start thread running training function.  When completed controlThreadInd will
            // be set to -1.  To exit early use killswitch.

            svm_pthread_t optthread;

            if ( svm_pthread_create(&optthread,NULL,&brTrainRun,(void *) &svmTrainOn) )
            {
                errstream() << "ERROR: Thread creation failed, background training not available\n";
                retval  = 101;
                stopnow = 1;
            }

            // May now continue as per usual.  Any attempt to regrab
            // the SVM will result in appropriate action to terminate
            // the new thread.
        }

        // Get next command

        stopnow = grabnextarg(commstack,currentarg);

        argbatchsize++;

        // Only continue if not stopped

        if ( !stopnow )
        {
            // Process input, store for later use if necessary
            //
            // Note that asynchronous options are processed here

            if ( iscomment )
            {
                // If in comment mode then wait for comment close marker

                if ( currentarg == "*/" )
                {
                    iscomment = 0;
                }
            }

            else if ( currentarg == "/*"         ) { iscomment = 1; }
            else if ( currentarg == "-bike"      ) { ; /* -bike used to be an accelerator option, now does nothing, but still in a few scripts */ }
            else if ( currentarg == "-?"         ) { printhelp(outstream()); }
            else if ( currentarg == "-??"        ) { printhelp(outstream(),1,1); }
            else if ( currentarg == "-??k"       ) { printhelpkernel(outstream(),1,1); }
            else if ( currentarg == "-??v"       ) { printhelpvars(outstream(),1,1); }
            else if ( currentarg == "-??g"       ) { printhelpgentype(outstream(),1,1); }
            else if ( currentarg == "-???"       ) { errstream() << "\n\n\n\n\n\n\n\n\n\n"; }
            else if ( currentarg == "-Zinteract" ) { kbquitdet("Root",NULL,NULL,NULL,1); }
            else if ( currentarg == "-Zgod"      ) { enablekbquitdet(); }
            else if ( currentarg == "-Zdawkins"  ) { disablekbquitdet(); }
            else if ( currentarg == "-Zx"        ) { skipon = 1; goto processnow; }
            else if ( currentarg == ";"          ) { skipon = 1; goto processnow; }
            else if ( currentarg == "-ZZ"        ) { ZZeval: stopnow = 1; skipon = 1; goto processnow; }
            else if ( currentarg == "end"        ) { stopnow = 1; skipon = 1; goto processnow; }
            else if ( currentarg == "-ZZZZ"      ) { ZZZZeval: errstream() << "Halt encountered: exitting now.\n"; stopnow = 1; retval = -1; }
            else if ( currentarg == "exit"       ) { errstream() << "Halt encountered: exitting now.\n"; stopnow = 1; retval = -1; }
            else if ( currentarg == "-Zob"       ) { bgTrainOn = 0; }
            else if ( currentarg == "-ZoB"       ) { bgTrainOn = 1; killmethod = 1; }
            else if ( currentarg == "-ZoBB"      ) { bgTrainOn = 1; killmethod = 0; }
            else if ( currentarg == "-Zmute"     ) { static LoggingOstream gonowhere(theRoundFile); seterrstream(&gonowhere); }
            else if ( currentarg == "-Zunmute"   ) { static LoggingOstream gotocerr(NULL);          seterrstream(&gotocerr);  }
            else if ( currentarg == "-ZMute"     ) { static LoggingOstreamOut sendnowhere(theRoundFile); setoutstream(&sendnowhere); }
            else if ( currentarg == "-ZunMute"   ) { static LoggingOstreamOut gotocout(NULL);            setoutstream(&gotocout);    }
            else if ( currentarg == "-ZMUTE"     ) { static LoggingOstream gonowhere(theRoundFile); seterrstream(&gonowhere); static LoggingOstreamOut sendnowhere(theRoundFile); setoutstream(&sendnowhere); }
            else if ( currentarg == "-ZunMUTE"   ) { static LoggingOstream gotocerr(NULL);          seterrstream(&gotocerr);  static LoggingOstreamOut gotocout(NULL);            setoutstream(&gotocout);    }

            else if ( currentarg == "-Zs" )
            {
                // Seed random number generator

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    int sval = (int) time(NULL);

                    if ( currentarg != "time" )
                    {
                        sval = safeatoi(currentarg,argvariables);
                    }

                    svm_srand(sval);
                }

                else
                {
                    errstream() << "Syntax error: -Zs requires 1 argument (seed value or rand)\n";
                    retval  = 1;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zrep" )
            {
                // Reached rep command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(3,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zrep requires 2 argument\n";
                    retval  = 16;
                    stopnow = 1;
                }

                else
                {
                    // Strip curly braces off command

                    std::string evalarg = zifargs(zeroint())(2);

                    stripcurlybrackets(evalarg);

                    // want to repeat b times

                    int numreps = 0;

                    try
                    {
                        numreps = safeatoi(zifargs(zeroint())(1),argvariables);
                    }

                    catch ( ... )
                    {
                        errstream() << "Syntax error: -Zrep argument 1 must evaluate to integer\n";
                        retval  = 17;
                        stopnow = 1;
                    }

                    int iii;

                    for ( iii = 0 ; iii < numreps ; iii++ )
                    {
                        // Setup environment and run command

                        errstream() << "Running command " << evalarg << "\n";

                        std::stringstream *tmpcommand;
                        MEMNEW(tmpcommand,std::stringstream);
                        *tmpcommand << "-fV 1000 " << iii << " -Zx " << evalarg;
                        awarestream *gridbox;
                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                        Stack<awarestream *> *gridcommstack;
                        MEMNEW(gridcommstack,Stack<awarestream *>);
                        gridcommstack->push(gridbox);

                        runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,returntag);

                        errstream() << "Finished running command " << evalarg << "\n";
                    }
                }
            }

            else if ( ( currentarg == "-ZZif" ) || ( currentarg == "endif" ) )
            {
                // Reached if command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -ZZif requires 1 argument\n";
                    retval  = 18;
                    stopnow = 1;
                }

                else
                {
                    // Exit if test evaluated true

                    int iftestres = 0;

                    try
                    {
                        iftestres = safeatoi(zifargs(zeroint())(1),argvariables);
                    }

                    catch ( ... )
                    {
                        errstream() << "Syntax error: -ZZif argument 1 must evaluate to logical (integer)\n";
                        retval  = 19;
                        stopnow = 1;
                    }

                    if ( iftestres )
                    {
                        goto ZZeval;
                    }
                }
            }

            else if ( ( currentarg == "-ZZZZif" ) || ( currentarg == "exitif" ) )
            {
                // Reached if command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -ZZif requires 1 argument\n";
                    retval  = 20;
                    stopnow = 1;
                }

                else
                {
                    // Exit if test evaluated true

                    int iftestres = 0;

                    try
                    {
                        iftestres = safeatoi(zifargs(zeroint())(1),argvariables);
                    }

                    catch ( ... )
                    {
                        errstream() << "Syntax error: -ZZif argument 1 must evaluate to logical (integer)\n";
                        retval  = 21;
                        stopnow = 1;
                    }

                    if ( iftestres )
                    {
                        goto ZZZZeval;
                    }
                }
            }

            else if ( currentarg == "-Zifelse" )
            {
                // Reached if command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(4,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zif requires 2 argument\n";
                    retval  = 22;
                    stopnow = 1;
                }

                else
                {
                    // Strip curly braces off command

                    std::string evalargtrue = zifargs(zeroint())(2);

                    stripcurlybrackets(evalargtrue);

                    // Strip curly braces off command

                    std::string evalargfalse = zifargs(zeroint())(2);

                    stripcurlybrackets(evalargfalse);

                    // Only proceed if test evaluated true

                    int iftestres = 0;

                    try
                    {
                        iftestres = safeatoi(zifargs(zeroint())(1),argvariables);
                    }

                    catch ( ... )
                    {
                        errstream() << "Syntax error: -Zif argument 1 must evaluate to logical (integer)\n";
                        retval  = 23;
                        stopnow = 1;
                    }

                    if ( !stopnow && iftestres )
                    {
                        // Setup environment and run command

                        errstream() << "Running command " << evalargtrue << "\n";

                        std::stringstream *tmpcommand;
                        MEMNEW(tmpcommand,std::stringstream(evalargtrue));
                        awarestream *gridbox;
                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                        Stack<awarestream *> *gridcommstack;
                        MEMNEW(gridcommstack,Stack<awarestream *>);
                        gridcommstack->push(gridbox);

                        runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,returntag);

                        errstream() << "Finished running command " << evalargtrue << "\n";
                    }

                    if ( !stopnow && !iftestres )
                    {
                        // Setup environment and run command

                        errstream() << "Running command " << evalargfalse << "\n";

                        std::stringstream *tmpcommand;
                        MEMNEW(tmpcommand,std::stringstream(evalargfalse));
                        awarestream *gridbox;
                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                        Stack<awarestream *> *gridcommstack;
                        MEMNEW(gridcommstack,Stack<awarestream *>);
                        gridcommstack->push(gridbox);

                        runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,returntag);

                        errstream() << "Finished running command " << evalargfalse << "\n";
                    }
                }
            }

            else if ( currentarg == "-Zwhile" )
            {
                // Reached while command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(3,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zwhile requires 2 argument\n";
                    retval  = 24;
                    stopnow = 1;
                }

                else
                {
                    // Strip curly braces off command

                    std::string evalarg = zifargs(zeroint())(2);

                    stripcurlybrackets(evalarg);

                    // Only proceed if test evaluated true

                  try
                  {
                    errstream() << "While loop... " << evalarg << "...";

                    while ( !stopnow && safeatoi(zifargs(zeroint())(1),argvariables) )
                    {
                        errstream() << "@";

                        // Setup environment and run command

                        std::stringstream *tmpcommand;
                        MEMNEW(tmpcommand,std::stringstream(evalarg));
                        awarestream *gridbox;
                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                        Stack<awarestream *> *gridcommstack;
                        MEMNEW(gridcommstack,Stack<awarestream *>);
                        gridcommstack->push(gridbox);

                        runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,returntag);
                    }

                    errstream() << "Finished while loop... " << evalarg << "\n";
                  }

                  catch ( ... )
                  {
                    errstream() << "Syntax error: -Zwhile argument 1 must evaluate to logical (integer)\n";
                    retval  = 25;
                    stopnow = 1;
                  }
                }
            }

            else if ( currentarg == "-Zwait" )
            {
                // Reached wait command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zwait requires 1 argument\n";
                    retval  = 24;
                    stopnow = 1;
                }

                else
                {
                  try
                  {
                    errstream() << "Wait loop...";

                    while ( !stopnow && !safeatoi(zifargs(zeroint())(1),argvariables) )
                    {
                        svm_usleep(WAIT_BASESLEEP+(svm_rand()%WAIT_ADDSLEEP_RAND));
                    }

                    errstream() << "Finished wait loop...\n";
                  }

                  catch ( ... )
                  {
                    errstream() << "Syntax error: -Zwait argument 1 must evaluate to logical (integer)\n";
                    retval  = 25;
                    stopnow = 1;
                  }
                }
            }

            else if ( currentarg == "-Zusleep" )
            {
                // Reached usleep command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zusleep requires 1 argument\n";
                    retval  = 24;
                    stopnow = 1;
                }

                else
                {
                    svm_usleep((int) safeatof(zifargs(zeroint())(1),argvariables));
                }
            }

            else if ( currentarg == "-Zmsleep" )
            {
                // Reached msleep command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zmsleep requires 1 argument\n";
                    retval  = 24;
                    stopnow = 1;
                }

                else
                {
                    svm_usleep((int) 1000*safeatof(zifargs(zeroint())(1),argvariables));
                }
            }

            else if ( currentarg == "-Zsleep" )
            {
                // Reached sleep command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zsleep requires 1 argument\n";
                    retval  = 24;
                    stopnow = 1;
                }

                else
                {
                    svm_usleep((int) 1000000*safeatof(zifargs(zeroint())(1),argvariables));
                }
            }

            else if ( ( currentarg == "-Zw" ) || ( currentarg == "-Zaw" ) )
            {
                // Switch input from file, no feedback

                int popflag = 0;

                if ( currentarg == "-Zaw" )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    if ( popflag )
                    {
                        // Pop element of command stack

                        MEMDEL(commstack.accessTop());
                        commstack.pop();
                    }

                    std::ifstream *filein;

                    MEMNEW(filein,std::ifstream);

                    if ( filein )
                    {
                        filein->open(currentarg.c_str(),std::ifstream::in);

                        if ( filein->is_open() )
                        {
                            awarestream *fileinbox;

                            MEMNEW(fileinbox,awarestream(filein,1));

                            commstack.push(fileinbox);
                        }

                        else
                        {
                            errstream() << "Unable to open file -Zw " << currentarg << "\n";
                            retval  = 201;
                            stopnow = 1;
                        }
                    }

                    else
                    {
                        errstream() << "Unable to open file -Zw " << currentarg << "\n";
                        retval  = 202;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: -Zw and -Zaw require 1 argument\n";
                    retval  = 26;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-Zk" ) || ( currentarg == "-Zak" ) )
            {
                // Switch to cin without feedback

                if ( currentarg == "-Zak" )
                {
                    // Pop element of command stack

                    MEMDEL(commstack.accessTop());
                    commstack.pop();
                }

                awarestream *stdcinbox;

                MEMNEW(stdcinbox,awarestream(&instream(),0));

                commstack.push(stdcinbox);
            }

            else if ( ( currentarg == "-Zc" ) || ( currentarg == "-Zac" ) )
            {
                // Switch to shared stream without feedback

                int popflag = 0;

                if ( currentarg == "-Zac" )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    if ( popflag )
                    {
                        // Pop element of command stack

                        MEMDEL(commstack.accessTop());
                        commstack.pop();
                    }

                    awarestream *newsharestream;

                    MEMNEW(newsharestream,awarestream("&","&",safeatoi(currentarg,argvariables)));

                    commstack.push(newsharestream);
                }

                else
                {
                    errstream() << "Syntax error: -Zc, -Zu, -Zuf, -Zau and -Zauf require 1 argument\n";
                    retval  = 29;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-Zwf" ) || ( currentarg == "-Zawf" ) )
            {
                // Switch input from file with feedback to file

                int popflag = 0;

                if ( currentarg == "-Zawf" )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    std::ifstream *filein;

                    MEMNEW(filein,std::ifstream);

                    if ( filein )
                    {
                        filein->open(currentarg.c_str(),std::ifstream::in);

                        if ( filein->is_open() )
                        {
                            stopnow = grabnextarg(commstack,currentarg);

                            if ( !stopnow )
                            {
                                if ( popflag )
                                {
                                    // Pop element of command stack

                                    MEMDEL(commstack.accessTop());
                                    commstack.pop();
                                }

                                std::ofstream *fileout;

                                MEMNEW(fileout,std::ofstream);

                                if ( fileout )
                                {
                                    fileout->open(currentarg.c_str(),std::ofstream::out);

                                    if ( fileout->is_open() )
                                    {
                                        awarestream *fileiobox;

                                        MEMNEW(fileiobox,awarestream(filein,fileout,1,1));

                                        commstack.push(fileiobox);
                                    }

                                    else
                                    {
                                        errstream() << "Unable to open file -Zwf ... " << currentarg << "\n";
                                        retval  = 203;
                                        stopnow = 1;
                                    }
                                }

                                else
                                {
                                    errstream() << "Unable to open file -Zwf ... " << currentarg << "\n";
                                    retval  = 204;
                                    stopnow = 1;
                                }
                            }

                            else
                            {
                                errstream() << "Syntax error: -Zwf and -Zawf require 2 argument\n";
                                retval  = 27;
                                stopnow = 1;
                            }
                        }

                        else
                        {
                            errstream() << "Unable to open file -Zwf " << currentarg << " ...\n";
                            retval  = 205;
                            stopnow = 1;
                        }
                    }

                    else
                    {
                        errstream() << "Unable to open file -Zwf " << currentarg << " ...\n";
                        retval  = 206;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: -Zwf and -Zawf require 2 argument\n";
                    retval  = 28;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-Zkf" ) || ( currentarg == "-Zakf" ) )
            {
                // Switch input to cin with feedback to cout

                if ( currentarg == "-Zakf" )
                {
                    // Pop element of command stack

                    MEMDEL(commstack.accessTop());
                    commstack.pop();
                }

                awarestream *stdcinbox;

                MEMNEW(stdcinbox,awarestream(&instream(),&outstream(),0,0));

                commstack.push(stdcinbox);
            }

            else if ( currentarg == "-Za" )
            {
                // Pop element of command stack

                MEMDEL(commstack.accessTop());
                commstack.pop();

                if ( commstack.size() == 0 )
                {
                    // No more input streams, exiting

                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zcw" )
            {
                // push string onto shared stack

                Vector<Vector<std::string> > pushonopt;

                if ( grabargs(3,pushonopt,commstack,currentarg) )
                {
                    retval  = 39;
                    stopnow = 1;
                }

                int stacknum = safeatoi(pushonopt(zeroint())(1),argvariables);

                static awarestream streamdummy("&","&",stacknum);

                streamdummy.vogon(pushonopt(zeroint())(2));
            }

            else if ( currentarg == "-ZcW" )
            {
                // push string onto shared stack

                int stacknum = 0;

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    stacknum = safeatoi(currentarg,argvariables);

                    stopnow = grabnextarg(commstack,currentarg);
                }

                if ( !stopnow )
                {
                    static awarestream streamdummy("&","&",stacknum);

                    streamdummy.vogon(safeatog(currentarg,argvariables).cast_string(1));
                }

                else
                {
                    errstream() << "Syntax error: -Zcw and -ZcW require 2 argument\n";
                    retval  = 28;
                    stopnow = 1;
                }
            }

#ifdef ALLOW_SOCKETS
            else if ( ( currentarg == "-Zu"   ) ||
                      ( currentarg == "-Zuf"  ) ||
                      ( currentarg == "-Zau"  ) ||
                      ( currentarg == "-Zauf" )    )
            {
                // Switch input to UDP client

                tcpfeedback = 0;

                if ( ( currentarg == "-Zuf" ) || ( currentarg == "-Zauf" ) )
                {
                    // Feedback on

                    tcpfeedback = 1;
                }

                int popflag = 0;

                if ( ( currentarg == "-Zau" ) || ( currentarg == "-Zauf" ) )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    if ( popflag )
                    {
                        // Pop element of command stack

                        MEMDEL(commstack.accessTop());
                        commstack.pop();
                    }

                    awarestream *newudpstream;

                    MEMNEW(newudpstream,awarestream(safeatoi(currentarg,argvariables),SVM_SOCK_DGRAM,tcpfeedback));

                    commstack.push(newudpstream);
                }

                else
                {
                    errstream() << "Syntax error: -Zu, -Zuf, -Zau and -Zauf require 1 argument\n";
                    retval  = 29;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-Zt"   ) ||
                      ( currentarg == "-Ztf"  ) ||
                      ( currentarg == "-Zat"  ) ||
                      ( currentarg == "-Zatf" )    )
            {
                tcpfeedback = 0;

                // Switch input to TCP client

                if ( ( currentarg == "-Ztf" ) || ( currentarg == "-Zatf" ) )
                {
                    // Feedback on

                    tcpfeedback = 1;
                }

                int popflag = 0;

                if ( ( currentarg == "-Zat" ) || ( currentarg == "-Zatf" ) )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    if ( popflag )
                    {
                        // Pop element of command stack

                        MEMDEL(commstack.accessTop());
                        commstack.pop();
                    }

                    awarestream *newtcpstream;

                    MEMNEW(newtcpstream,awarestream(safeatoi(currentarg,argvariables),SVM_SOCK_STREAM,tcpfeedback));

                    commstack.push(newtcpstream);
                }

                else
                {
                    errstream() << "Syntax error: -Zt, -Ztf, -Zat and -Zatf require 1 argument\n";
                    retval  = 30;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-Zn"   ) ||
                      ( currentarg == "-Znf"  ) ||
                      ( currentarg == "-Zan"  ) ||
                      ( currentarg == "-Zanf" )    )
            {
                // Switch input to *nix client

                tcpfeedback = 0;

                if ( ( currentarg == "-Znf" ) || ( currentarg == "-Zanf" ) )
                {
                    // Feedback on

                    tcpfeedback = 1;
                }

                int popflag = 0;

                if ( ( currentarg == "-Zan" ) || ( currentarg == "-Zanf" ) )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    if ( popflag )
                    {
                        // Pop element of command stack

                        MEMDEL(commstack.accessTop());
                        commstack.pop();
                    }

                    awarestream *newtcpstream;

                    MEMNEW(newtcpstream,awarestream("&",currentarg,SVM_SOCK_STREAM,tcpfeedback,0));

                    commstack.push(newtcpstream);
                }

                else
                {
                    errstream() << "Syntax error: -Zn, -Znf, -Zan and -Zanf require 1 argument\n";
                    retval  = 31;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-ZU"   ) ||
                      ( currentarg == "-ZaU"  ) ||
                      ( currentarg == "-ZUf"  ) ||
                      ( currentarg == "-ZaUf" )    )
            {
                tcpfeedback = 0;

                // Switch input to UDP server

                if ( ( currentarg == "-ZUf" ) || ( currentarg == "-ZaUf" ) )
                {
                    // Feedback on

                    tcpfeedback = 1;
                }

                int popflag = 0;

                if ( ( currentarg == "-ZaU" ) || ( currentarg == "-ZaUf" ) )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    long portx = safeatoi(currentarg,argvariables);

                    stopnow = grabnextarg(commstack,currentarg);

                    if ( !stopnow )
                    {
                        if ( popflag )
                        {
                            // Pop element of command stack

                            MEMDEL(commstack.accessTop());
                            commstack.pop();
                        }

                        awarestream *newudpstream;

                        MEMNEW(newudpstream,awarestream(currentarg,portx,SVM_SOCK_DGRAM,tcpfeedback));

                        commstack.push(newudpstream);
                    }

                    else
                    {
                        errstream() << "Syntax error: -ZU, -ZUf, -ZaU and -ZaUf require 2 arguments\n";
                        retval  = 32;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: -ZU, -ZUf, -ZaU and -ZaUf require 2 arguments\n";
                    retval  = 33;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-ZT"   ) ||
                      ( currentarg == "-ZTf"  ) ||
                      ( currentarg == "-ZaTf" ) ||
                      ( currentarg == "-ZaT"  )    )
            {
                tcpfeedback = 0;

                // Switch input to TCP Server

                if ( ( currentarg == "-ZTf" ) || ( currentarg == "-ZaTf" ) )
                {
                    // Feedback on

                    tcpfeedback = 1;
                }

                int popflag = 0;

                if ( ( currentarg == "-ZaT" ) || ( currentarg == "-ZaTf" ) )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    long portx = safeatoi(currentarg,argvariables);

                    stopnow = grabnextarg(commstack,currentarg);

                    if ( !stopnow )
                    {
                        if ( popflag )
                        {
                            // Pop element of command stack

                            MEMDEL(commstack.accessTop());
                            commstack.pop();
                        }

                        awarestream *newudpstream;

                        MEMNEW(newudpstream,awarestream(currentarg,portx,tcpfeedback));

                        commstack.push(newudpstream);
                    }

                    else
                    {
                        errstream() << "Syntax error: -ZT, -ZTf, -ZaT and -ZaTf require 2 arguments\n";
                        retval  = 34;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: -ZT, -ZTf, -ZaT and -ZaTf require 2 arguments\n";
                    retval  = 35;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-ZN"   ) ||
                      ( currentarg == "-ZNf"  ) ||
                      ( currentarg == "-ZaN"  ) ||
                      ( currentarg == "-ZaNf" )    )
            {
                // Switch input to *nix server

                tcpfeedback = 0;

                if ( ( currentarg == "-ZNf" ) || ( currentarg == "-ZaNf" ) )
                {
                    // Feedback on

                    tcpfeedback = 1;
                }

                int popflag = 0;

                if ( ( currentarg == "-ZaN" ) || ( currentarg == "-ZaNf" ) )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    if ( popflag )
                    {
                        // Pop element of command stack

                        MEMDEL(commstack.accessTop());
                        commstack.pop();
                    }

                    awarestream *newtcpstream;

                    MEMNEW(newtcpstream,awarestream("&",currentarg,SVM_SOCK_STREAM,tcpfeedback,1));

                    commstack.push(newtcpstream);
                }

                else
                {
                    errstream() << "Syntax error: -ZN, -ZNf, -ZaN and -ZaNf require 1 argument\n";
                    retval  = 36;
                    stopnow = 1;
                }
            }
#endif

            else if ( currentarg == "-Zf" )
            {
                // Create an empty flag file

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    std::ofstream flagfile;
                    flagfile.open(currentarg.c_str(),std::ofstream::out);

                    if ( flagfile.is_open() )
                    {
                        flagfile.close();
                    }

                    else
                    {
                        errstream() << "Unable to open file -Zf " << currentarg << "\n";
                        retval  = 207;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: -Zf requires 1 argument\n";
                    retval  = 37;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zp" )
            {
                // Wait for flag file to be created

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    errstream() << "Waiting for " << currentarg << " flagfile to be created...";

                    int ispresent = 0;

                    while ( !ispresent )
                    {
                        std::ifstream sndflagfile(currentarg.c_str());

                        if ( !(sndflagfile.is_open()) )
                        {
                            svm_usleep(1000000/FILETESTRATE);
                        }

                        else
                        {
                            sndflagfile.close();
                            ispresent = 1;
                        }
                    }

                    errstream() << "done\n";
                }

                else
                {
                    errstream() << "Syntax error: -Zp requires 1 argument\n";
                    retval  = 38;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zff" )
            {
                // Create a file and write variable to it

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    std::ofstream flagfile;
                    flagfile.open(currentarg.c_str(),std::ofstream::out);

                    if ( flagfile.is_open() )
                    {
                        stopnow = grabnextarg(commstack,currentarg);

                        if ( !stopnow )
                        {
                            flagfile << safeatog(currentarg,argvariables) << "\n";
                            flagfile.close();
                        }

                        else
                        {
                            flagfile.close();

                            errstream() << "Syntax error: -Zff requires 2 arguments\n";
                            retval  = 37;
                            stopnow = 1;
                        }
                    }

                    else
                    {
                        stopnow = grabnextarg(commstack,currentarg);

                        errstream() << "Unable to open file -Zff " << currentarg << "\n";
                        retval  = 207;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: -Zff requires 2 arguments\n";
                    retval  = 37;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zpp" )
            {
                // Wait for flag file to be created and read variable from it

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    std::string zppnum;

                    stopnow = grabnextarg(commstack,zppnum);

                    if ( !stopnow )
                    {
                        int nn = safeatoi(zppnum,argvariables);

                        errstream() << "Waiting for " << currentarg << " flagfile to be created...";

                        int ispresent = 0;
                        std::ifstream sndflagfile;

                        while ( !ispresent )
                        {
                            sndflagfile.open(currentarg.c_str(),std::ifstream::in);

                            if ( !(sndflagfile.is_open()) )
                            {
                                svm_usleep(1000000/FILETESTRATE);
                            }

                            else
                            {
                                //sndflagfile.close();
                                ispresent = 1;
                            }
                        }

                        sndflagfile >> argvariables("&",0)("&",nn);
                        sndflagfile.close();

                        errstream() << "done\n";
                    }

                    else
                    {
                        goto zpperr;
                    }
                }

                else
                {
                    zpperr:
                    errstream() << "Syntax error: -Zpp requires 2 arguments\n";
                    retval  = 38;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zdel" )
            {
                // Remove file

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    remove(currentarg.c_str());
                }

                else
                {
                    errstream() << "Syntax error: -Zdel requires 1 argument\n";
                    retval  = 38;
                    stopnow = 1;
                }
            }







































































            else if ( ( currentarg == "-v"  ) ||
                      ( currentarg == "-L"  ) ||
                      ( currentarg == "-LL" )    )
            {
                // Logging options

                if ( grabargs(2,loggingopt,commstack,currentarg) )
                {
                    retval  = 39;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-qpop" )    )
            {
                // Setup options

                if ( grabargs(1,multirunopt,commstack,currentarg) )
                {
                    retval  = 40;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-qR"    ) ||
                      ( currentarg == "-qpush" ) ||
                      ( currentarg == "-qw"    )    )
            {
                // Setup options

                if ( grabargs(2,multirunopt,commstack,currentarg) )
                {
                    retval  = 40;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-qc" ) ||
                      ( currentarg == "-qs" )    )
            {
                // Setup options

                if ( grabargs(3,multirunopt,commstack,currentarg) )
                {
                    retval  = 41;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-z"  ) ||
                      ( currentarg == "-zc" ) ||
                      ( currentarg == "-zo" ) ||
                      ( currentarg == "-zd" ) ||
                      ( currentarg == "-zv" ) ||
                      ( currentarg == "-zl" )    )
            {
                // Setup options

                if ( grabargs(2,svmpresetupopt,commstack,currentarg) )
                {
                    retval  = 42;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-bv"  ) ||
                      ( currentarg == "-bz"  ) ||
                      ( currentarg == "-bgv" ) ||
                      ( currentarg == "-bgz" )    )
            {
                // Setup options

                if ( grabargs(1,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 43;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-ac"  ) ||
                      ( currentarg == "-B"   ) ||
                      ( currentarg == "-R"   ) ||
                      ( currentarg == "-mls" ) ||
                      ( currentarg == "-sR"  ) ||
                      ( currentarg == "-T"   ) ||
                      ( currentarg == "-N"   ) ||
                      ( currentarg == "-br"  ) ||
                      ( currentarg == "-bd"  ) ||
                      ( currentarg == "-XT"  )    )
            {
                // Setup options

                if ( grabargs(2,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 44;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fru" ) ||
                      ( currentarg == "-frn" ) ||
                      ( currentarg == "-fri" ) ||
                      ( currentarg == "-mc"  ) ||
                      ( currentarg == "-mcn" ) ||
                      ( currentarg == "-mbA" ) ||
                      ( currentarg == "-msn" ) ||
                      ( currentarg == "-msw" ) ||
                      ( currentarg == "-bat" ) ||
                      ( currentarg == "-bam" ) ||
                      ( currentarg == "-bac" ) ||
                      ( currentarg == "-bad" ) ||
                      ( currentarg == "-bav" ) ||
                      ( currentarg == "-baT" ) ||
                      ( currentarg == "-fat" )    )
            {
                // Setup options

                if ( grabargs(2,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 45;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fo"   ) ||
                      ( currentarg == "-foe"  ) ||
                      ( currentarg == "-fV"   ) ||
                      ( currentarg == "-mlR"  ) ||
                      ( currentarg == "-fW"   ) ||
                      ( currentarg == "-fWW"  ) ||
                      ( currentarg == "-fret" ) ||
                      ( currentarg == "-fVg"  ) ||
                      ( currentarg == "-fWg"  ) ||
                      ( currentarg == "-fVG"  ) ||
                      ( currentarg == "-fWG"  ) ||
                      ( currentarg == "-mbw"  ) ||
                      ( currentarg == "-mba"  )    )
            {
                // Setup options

                if ( grabargs(3,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 45;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fWm" ) ||
                      ( currentarg == "-fWM" )    )
            {
                // Setup options

                if ( grabargs(4,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 45;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fu"  ) ||
                      ( currentarg == "-fuG" )    )
            {
                // Setup options

                if ( grabargs(4,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 46;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-ft"   ) ||
                      ( currentarg == "-ftG"  ) ||
                      ( currentarg == "-fuu"  ) ||
                      ( currentarg == "-fuuG" )    )
            {
                // Setup options

                if ( grabargs(5,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 46;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fM"  )    )
            {
                // Setup options

                if ( grabargs(3,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 47;
                    stopnow = 1;
                }

                stripcurlybrackets(svmsetupopt("&",svmsetupopt.size()-1)("&",2));

                updateargvars = 1;
            }

            else if ( ( currentarg == "-prz" ) ||
                      ( currentarg == "-pR"  ) ||
                      ( currentarg == "-pRR" ) ||
                      ( currentarg == "-pS"  ) ||
                      ( currentarg == "-fic" )    )
            {
                // Preload options

                if ( grabargs(1,preloadopt,commstack,currentarg) )
                {
                    retval  = 50;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-pr"  ) ||
                      ( currentarg == "-pro" ) ||
                      ( currentarg == "-prm" ) ||
                      ( currentarg == "-pcs" ) ||
                      ( currentarg == "-pds" ) ||
                      ( currentarg == "-pws" ) ||
                      ( currentarg == "-pk"  ) ||
                      ( currentarg == "-ps"  )    )
            {
                // Preload options

                if ( grabargs(2,preloadopt,commstack,currentarg) )
                {
                    retval  = 51;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-pcw" ) ||
                      ( currentarg == "-pdw" ) ||
                      ( currentarg == "-pww" ) ||
                      ( currentarg == "-psd" ) ||
                      ( currentarg == "-psz" )    )
            {
                // Preload options

                if ( grabargs(3,preloadopt,commstack,currentarg) )
                {
                    retval  = 52;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Acd"   ) ||
                      ( currentarg == "-Aby"   ) ||
                      ( currentarg == "-ABy"   ) ||
                      ( currentarg == "-Abu"   ) ||
                      ( currentarg == "-ABu"   )    )
            {
                // Load options

                if ( grabargs(1,loadopt,commstack,currentarg) )
                {
                    retval  = 53;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Ac"    ) ||
                      ( currentarg == "-As"    ) ||
                      ( currentarg == "-Ad"    ) ||
                      ( currentarg == "-AD"    ) ||
                      ( currentarg == "-Ar"    ) ||
                      ( currentarg == "-AR"    ) ||
                      ( currentarg == "-AA"    ) ||
                      ( currentarg == "-AAe"   ) ||
                      ( currentarg == "-AAi"   ) ||
                      ( currentarg == "-AAI"   ) ||
                      ( currentarg == "-AAr"   ) ||
                      ( currentarg == "-AAR"   ) ||
                      ( currentarg == "-AAu"   ) ||
                      ( currentarg == "-AAeu"  ) ||
                      ( currentarg == "-AAiu"  ) ||
                      ( currentarg == "-AAIu"  ) ||
                      ( currentarg == "-AAru"  ) ||
                      ( currentarg == "-AARu"  ) ||
                      ( currentarg == "-ATA"   ) ||
                      ( currentarg == "-ATAe"  ) ||
                      ( currentarg == "-ATAi"  ) ||
                      ( currentarg == "-ATAI"  ) ||
                      ( currentarg == "-ATAr"  ) ||
                      ( currentarg == "-ATAR"  ) ||
                      ( currentarg == "-ATAu"  ) ||
                      ( currentarg == "-ATAeu" ) ||
                      ( currentarg == "-ATAiu" ) ||
                      ( currentarg == "-ATAIu" ) ||
                      ( currentarg == "-ATAru" ) ||
                      ( currentarg == "-ATARu" ) ||
                      ( currentarg == "-ATb"   ) ||
                      ( currentarg == "-ATa"   ) ||
                      ( currentarg == "-ATn"   ) ||
                      ( currentarg == "-ATx"   ) ||
                      ( currentarg == "-ATy"   ) ||
                      ( currentarg == "-Acz"   ) ||
                      ( currentarg == "-AeA"   ) ||
                      ( currentarg == "-AEA"   ) ||
                      ( currentarg == "-AeU"   ) ||
                      ( currentarg == "-AEU"   ) ||
                      ( currentarg == "-AGl"   ) ||
                      ( currentarg == "-AGu"   ) ||
                      ( currentarg == "-ID"    )    )
            {
                // Load options

                if ( grabargs(2,loadopt,commstack,currentarg) )
                {
                    retval  = 54;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-AU"  ) ||
                      ( currentarg == "-AY"  ) ||
                      ( currentarg == "-AV"  ) ||
                      ( currentarg == "-AVv" ) ||
                      ( currentarg == "-AeR" ) ||
                      ( currentarg == "-AER" ) ||
                      ( currentarg == "-AW"  )    )
            {
                // Load options

                if ( grabargs(3,loadopt,commstack,currentarg) )
                {
                    retval  = 55;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-AZ"  ) ||
                      ( currentarg == "-Aq"  ) ||
                      ( currentarg == "-AVV" )    )
            {
                // Load options

                if ( grabargs(4,loadopt,commstack,currentarg) )
                {
                    retval  = 56;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-AAl"   ) ||
                      ( currentarg == "-AAel"  ) ||
                      ( currentarg == "-AAil"  ) ||
                      ( currentarg == "-AAIl"  ) ||
                      ( currentarg == "-ATAl"  ) ||
                      ( currentarg == "-ATAel" ) ||
                      ( currentarg == "-ATAil" ) ||
                      ( currentarg == "-ATAIl" ) ||
                      ( currentarg == "-Aca"   )    )
            {
                // Load options

                if ( grabargs(3,loadopt,commstack,currentarg) )
                {
                    retval  = 57;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-AN"    ) ||
                      ( currentarg == "-ANe"   ) ||
                      ( currentarg == "-ANi"   ) ||
                      ( currentarg == "-ANI"   ) ||
                      ( currentarg == "-Ag"    ) ||
                      ( currentarg == "-AG"    ) ||
                      ( currentarg == "-ANr"   ) ||
                      ( currentarg == "-ANR"   ) ||
                      ( currentarg == "-ANu"   ) ||
                      ( currentarg == "-ANeu"  ) ||
                      ( currentarg == "-ANiu"  ) ||
                      ( currentarg == "-ANIu"  ) ||
                      ( currentarg == "-ANru"  ) ||
                      ( currentarg == "-ANRu"  ) ||
                      ( currentarg == "-ATN"   ) ||
                      ( currentarg == "-ATNe"  ) ||
                      ( currentarg == "-ATNi"  ) ||
                      ( currentarg == "-ATNI"  ) ||
                      ( currentarg == "-ATNr"  ) ||
                      ( currentarg == "-ATNR"  ) ||
                      ( currentarg == "-ATNu"  ) ||
                      ( currentarg == "-ATNeu" ) ||
                      ( currentarg == "-ATNiu" ) ||
                      ( currentarg == "-ATNIu" ) ||
                      ( currentarg == "-ATNru" ) ||
                      ( currentarg == "-ATNRu" )    )
            {
                // Load options

                if ( grabargs(5,loadopt,commstack,currentarg) )
                {
                    retval  = 58;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-ANl"   ) ||
                      ( currentarg == "-Agc"   ) ||
                      ( currentarg == "-AGc"   ) ||
                      ( currentarg == "-ANel"  ) ||
                      ( currentarg == "-ANil"  ) ||
                      ( currentarg == "-ANIl"  ) ||
                      ( currentarg == "-ANrl"  ) ||
                      ( currentarg == "-ANRl"  ) ||
                      ( currentarg == "-ATNl"  ) ||
                      ( currentarg == "-ATNel" ) ||
                      ( currentarg == "-ATNil" ) ||
                      ( currentarg == "-ATNIl" ) ||
                      ( currentarg == "-ATNrl" ) ||
                      ( currentarg == "-ATNRl" )    )
            {
                // Load options

                if ( grabargs(6,loadopt,commstack,currentarg) )
                {
                    retval  = 59;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Snx" ) ||
                      ( currentarg == "-Sna" ) ||
                      ( currentarg == "-Snb" ) ||
                      ( currentarg == "-Snc" ) ||
                      ( currentarg == "-SNa" ) ||
                      ( currentarg == "-SNb" ) ||
                      ( currentarg == "-SNc" ) ||
                      ( currentarg == "-SnA" ) ||
                      ( currentarg == "-SnB" ) ||
                      ( currentarg == "-SnC" ) ||
                      ( currentarg == "-SNA" ) ||
                      ( currentarg == "-SNB" ) ||
                      ( currentarg == "-SNC" ) ||
                      ( currentarg == "-Snt" )    )
            {
                // Postload options

                if ( grabargs(1,postloadopt,commstack,currentarg) )
                {
                    retval  = 60;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Sa"  ) ||
                      ( currentarg == "-Sb"  ) ||
                      ( currentarg == "-Sx"  ) ||
                      ( currentarg == "-Sra" )    )
            {
                // Postload options

                if ( grabargs(2,postloadopt,commstack,currentarg) )
                {
                    retval  = 61;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-St"  )    )
            {
                // Postload options

                if ( grabargs(4,postloadopt,commstack,currentarg) )
                {
                    retval  = 61;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Mn" ) ||
                      ( currentarg == "-Mi" ) ||
                      ( currentarg == "-Md" )    )
            {
                // Learning options

                if ( grabargs(1,learningopt,commstack,currentarg) )
                {
                    retval  = 62;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-c"    ) ||
                      ( currentarg == "-th"   ) ||
                      ( currentarg == "-thn"  ) ||
                      ( currentarg == "-c+"   ) ||
                      ( currentarg == "-c-"   ) ||
                      ( currentarg == "-c="   ) ||
                      ( currentarg == "-cs"   ) ||
                      ( currentarg == "-ds"   ) ||
                      ( currentarg == "-c+s"  ) ||
                      ( currentarg == "-c-s"  ) ||
                      ( currentarg == "-c=s"  ) ||
                      ( currentarg == "-j"    ) ||
                      ( currentarg == "-jc"   ) ||
                      ( currentarg == "-dd"   ) ||
                      ( currentarg == "-w"    ) ||
                      ( currentarg == "-w+"   ) ||
                      ( currentarg == "-w-"   ) ||
                      ( currentarg == "-w="   ) ||
                      ( currentarg == "-ws"   ) ||
                      ( currentarg == "-w+s"  ) ||
                      ( currentarg == "-w-s"  ) ||
                      ( currentarg == "-w=s"  ) ||
                      ( currentarg == "-jw"   ) ||
                      ( currentarg == "-nm"   ) ||
                      ( currentarg == "-Bf"   ) ||
                      ( currentarg == "-Tl"   ) ||
                      ( currentarg == "-Tq"   ) ||
                      ( currentarg == "-Nl"   ) ||
                      ( currentarg == "-sNl"  ) ||
                      ( currentarg == "-Nq"   ) ||
                      ( currentarg == "-Fi"   ) ||
                      ( currentarg == "-Flr"  ) ||
                      ( currentarg == "-Fzt"  ) ||
                      ( currentarg == "-mvi"  ) ||
                      ( currentarg == "-mvlr" ) ||
                      ( currentarg == "-mvzt" ) ||
                      ( currentarg == "-mvb"  ) ||
                      ( currentarg == "-Fc"   ) ||
                      ( currentarg == "-m"    ) ||
                      ( currentarg == "-blx"  ) ||
                      ( currentarg == "-bly"  ) ||
                      ( currentarg == "-blz"  ) ||
                      ( currentarg == "-bls"  ) ||
                      ( currentarg == "-bfx"  ) ||
                      ( currentarg == "-bfy"  ) ||
                      ( currentarg == "-bfxy" ) ||
                      ( currentarg == "-bfyx" ) ||
                      ( currentarg == "-bfr"  ) ||
                      ( currentarg == "-k"    ) ||
                      ( currentarg == "-ek"   ) ||
                      ( currentarg == "-K"    ) ||
                      ( currentarg == "-iz"   ) ||
                      ( currentarg == "-ie"   ) ||
                      ( currentarg == "-d"    ) ||
                      ( currentarg == "-nzs"  ) ||
                      ( currentarg == "-vlb"  ) ||
                      ( currentarg == "-vub"  )    )
            {
                // Learning options

                if ( grabargs(2,learningopt,commstack,currentarg) )
                {
                    retval  = 62;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-cd"   ) ||
                      ( currentarg == "-cw"   ) ||
                      ( currentarg == "-dw"   ) ||
                      ( currentarg == "-mlc"  ) ||
                      ( currentarg == "-cds"  ) ||
                      ( currentarg == "-wd"   ) ||
                      ( currentarg == "-ww"   ) ||
                      ( currentarg == "-wds"  ) ||
                      ( currentarg == "-Nld"  ) ||
                      ( currentarg == "-Nqd"  )    )
            {
                // Learning options

                if ( grabargs(3,learningopt,commstack,currentarg) )
                {
                    retval  = 63;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-kn"    ) ||
                      ( currentarg == "-ku"    ) ||
                      ( currentarg == "-knn"   ) ||
                      ( currentarg == "-kuu"   ) ||
                      ( currentarg == "-kc"    ) ||
                      ( currentarg == "-kuc"   ) ||
                      ( currentarg == "-km"    ) ||
                      ( currentarg == "-kum"   ) ||
                      ( currentarg == "-kS"    ) ||
                      ( currentarg == "-kuS"   ) ||
                      ( currentarg == "-kMS"   ) ||
                      ( currentarg == "-kMuS"  ) ||
                      ( currentarg == "-kU"    ) ||
                      ( currentarg == "-koz"   ) ||
                      ( currentarg == "-mtb"   ) ||
                      ( currentarg == "-bmx"   ) ||
                      ( currentarg == "-kOz"   ) ||
                      ( currentarg == "-ekn"   ) ||
                      ( currentarg == "-eku"   ) ||
                      ( currentarg == "-eknn"  ) ||
                      ( currentarg == "-ekuu"  ) ||
                      ( currentarg == "-ekc"   ) ||
                      ( currentarg == "-ekuc"  ) ||
                      ( currentarg == "-ekm"   ) ||
                      ( currentarg == "-ekum"  ) ||
                      ( currentarg == "-ekS"   ) ||
                      ( currentarg == "-ekuS"  ) ||
                      ( currentarg == "-ekMS"  ) ||
                      ( currentarg == "-ekMuS" ) ||
                      ( currentarg == "-ekU"   ) ||
                      ( currentarg == "-ekoz"  ) ||
                      ( currentarg == "-emtb"  ) ||
                      ( currentarg == "-ebmx"  ) ||
                      ( currentarg == "-ekOz"  )    )
            {
                // Kernel options

                if ( grabargs(1,kernelopt,commstack,currentarg) )
                {
                    retval  = 64;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-ks"    ) ||
                      ( currentarg == "-ki"    ) ||
                      ( currentarg == "-ka"    ) ||
                      ( currentarg == "-kb"    ) ||
                      ( currentarg == "-ke"    ) ||
                      ( currentarg == "-kw"    ) ||
                      ( currentarg == "-kt"    ) ||
                      ( currentarg == "-ktx"   ) ||
                      ( currentarg == "-ktk"   ) ||
                      ( currentarg == "-kg"    ) ||
                      ( currentarg == "-kgg"   ) ||
                      ( currentarg == "-kf"    ) ||
                      ( currentarg == "-kr"    ) ||
                      ( currentarg == "-kd"    ) ||
                      ( currentarg == "-kG"    ) ||
                      ( currentarg == "-kI"    ) ||
                      ( currentarg == "-kan"   ) ||
                      ( currentarg == "-eks"   ) ||
                      ( currentarg == "-eki"   ) ||
                      ( currentarg == "-eka"   ) ||
                      ( currentarg == "-ekb"   ) ||
                      ( currentarg == "-eke"   ) ||
                      ( currentarg == "-ekw"   ) ||
                      ( currentarg == "-ekt"   ) ||
                      ( currentarg == "-ektx"  ) ||
                      ( currentarg == "-ektk"  ) ||
                      ( currentarg == "-ekg"   ) ||
                      ( currentarg == "-ekgg"  ) ||
                      ( currentarg == "-ekf"   ) ||
                      ( currentarg == "-ekr"   ) ||
                      ( currentarg == "-ekd"   ) ||
                      ( currentarg == "-ekG"   ) ||
                      ( currentarg == "-ekI"   ) ||
                      ( currentarg == "-ekan"  )    )
            {
                // Kernel options

                if ( grabargs(2,kernelopt,commstack,currentarg) )
                {
                    retval  = 65;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-kv" ) || 
                      ( currentarg == "-kV" ) || 
                      ( currentarg == "-ko" ) || 
                      ( currentarg == "-kO" ) || 
                      ( currentarg == "-ekv" ) || 
                      ( currentarg == "-ekV" ) || 
                      ( currentarg == "-eko" ) || 
                      ( currentarg == "-ekO" )    )
            {
                // Kernel options

                if ( grabargs(3,kernelopt,commstack,currentarg) )
                {
                    retval  = 66;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-cua" ) ||
                      ( currentarg == "-cA"  ) ||
                      ( currentarg == "-cB"  ) ||
                      ( currentarg == "-cAN" ) ||
                      ( currentarg == "-cBN" ) ||
                      ( currentarg == "-bal" )    )
            {
                // Tuning options

                if ( grabargs(1,tuningopt,commstack,currentarg) )
                {
                    retval  = 67;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( currentarg == "-cX"  )
            {
                // Tuning options

                if ( grabargs(2,tuningopt,commstack,currentarg) )
                {
                    retval  = 68;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( currentarg == "-NlA" )
            {
                // Tuning options

                if ( grabargs(3,tuningopt,commstack,currentarg) )
                {
                    retval  = 69;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-g"  ) || ( currentarg == "-gd"  ) || ( currentarg == "-gN" ) || ( currentarg == "-gb"  ) )
            {
                // Gridsearch options
                //
                // To process we first read the number of arguments.  Then
                // there are 4a+2 arguments to come, where a is # args

                std::string storearg = currentarg;

                int a = 0;

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    a = safeatoi(currentarg,argvariables);

                    if ( a >= 0 )
                    {
                        int acnt = (5*a)+2;

                        currentarg = storearg;
                    
                        if ( grabargs(acnt+1,gridopt,commstack,currentarg) )
                        {
                            retval  = 70;
                            stopnow = 1;
                        }

                        //else
                        //{
                        //    stripquotes(gridopt("&",gridopt.size()-1)("&",1));
                        //    stripquotes(gridopt("&",gridopt.size()-1)("&",2));
                        //}
                    }

                    else
                    {
                        errstream() << "Syntax error: " << storearg << " argument count must be non-negative\n";
                        retval  = 71;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: " << storearg << " requires 3 arguments minimum\n";
                    retval  = 72;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-gmo" ) ||
                      ( currentarg == "-gms" ) ||
                      ( currentarg == "-gnp" ) ||
                      ( currentarg == "-gpb" ) ||
                      ( currentarg == "-gph" ) ||
                      ( currentarg == "-gnr" ) ||
                      ( currentarg == "-gphkn"   ) ||
                      ( currentarg == "-gphku"   ) ||
                      ( currentarg == "-gphknn"  ) ||
                      ( currentarg == "-gphkuu"  ) ||
                      ( currentarg == "-gphkc"   ) ||
                      ( currentarg == "-gphkuc"  ) ||
                      ( currentarg == "-gphkm"   ) ||
                      ( currentarg == "-gphkum"  ) ||
                      ( currentarg == "-gphkS"   ) ||
                      ( currentarg == "-gphkuS"  ) ||
                      ( currentarg == "-gphkMS"  ) ||
                      ( currentarg == "-gphkMuS" ) ||
                      ( currentarg == "-gphkU"   ) ||
                      ( currentarg == "-gphkoz"  ) ||
                      ( currentarg == "-gphmtb"  ) ||
                      ( currentarg == "-gphbmx"  ) ||
                      ( currentarg == "-gphkOz"  ) ||
                      ( currentarg == "-gPkn"    ) ||
                      ( currentarg == "-gPku"    ) ||
                      ( currentarg == "-gPknn"   ) ||
                      ( currentarg == "-gPkuu"   ) ||
                      ( currentarg == "-gPkc"    ) ||
                      ( currentarg == "-gPkuc"   ) ||
                      ( currentarg == "-gPkm"    ) ||
                      ( currentarg == "-gPkum"   ) ||
                      ( currentarg == "-gPkS"    ) ||
                      ( currentarg == "-gPkuS"   ) ||
                      ( currentarg == "-gPkMS"   ) ||
                      ( currentarg == "-gPkMuS"  ) ||
                      ( currentarg == "-gPkU"    ) ||
                      ( currentarg == "-gPkoz"   ) ||
                      ( currentarg == "-gPmtb"   ) ||
                      ( currentarg == "-gPbmx"   ) ||
                      ( currentarg == "-gPkOz"   )    )
            {
                // Learning options

                if ( grabargs(1,gridopt,commstack,currentarg) )
                {
                    retval  = 73;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-gy"    ) ||
                      ( currentarg == "-gr"    ) ||
                      ( currentarg == "-gpr"   ) ||
                      ( currentarg == "-gnr"   ) ||
                      ( currentarg == "-gp"    ) ||
                      ( currentarg == "-gP"    ) ||
                      ( currentarg == "-gpd"   ) ||
                      ( currentarg == "-gf"    ) ||
                      ( currentarg == "-gc"    ) ||
                      ( currentarg == "-gC"    ) ||
                      ( currentarg == "-g+"    ) ||
                      ( currentarg == "-gns"   ) ||
                      ( currentarg == "-gfm"   ) ||
                      ( currentarg == "-gfu"   ) ||
                      ( currentarg == "-gfM"   ) ||
                      ( currentarg == "-gfU"   ) ||
                      ( currentarg == "-gxs"   ) ||
                      ( currentarg == "-ggm"   ) ||
                      ( currentarg == "-ggi"   ) ||
                      ( currentarg == "-gdc"   ) ||
                      ( currentarg == "-gdf"   ) ||
                      ( currentarg == "-gde"   ) ||
                      ( currentarg == "-gda"   ) ||
                      ( currentarg == "-gbH"   ) ||
                      ( currentarg == "-gbs"   ) ||
                      ( currentarg == "-gbm"   ) ||
                      ( currentarg == "-gbj"   ) ||
                      ( currentarg == "-gba"   ) ||
                      ( currentarg == "-gbb"   ) ||
                      ( currentarg == "-gbl"   ) ||
                      ( currentarg == "-gbt"   ) ||
                      ( currentarg == "-gbe"   ) ||
                      ( currentarg == "-gbts"  ) ||
                      ( currentarg == "-gtp"   ) ||
                      ( currentarg == "-gtP"   ) ||
                      ( currentarg == "-gtx"   ) ||
                      ( currentarg == "-gbpp"  ) ||
                      ( currentarg == "-gbz"   ) ||
                      ( currentarg == "-gpB"   ) ||
                      ( currentarg == "-gmx"   ) ||
                      ( currentarg == "-gmxa"  ) ||
                      ( currentarg == "-gmxb"  ) ||
                      ( currentarg == "-gmy"   ) ||
                      ( currentarg == "-gmya"  ) ||
                      ( currentarg == "-gmyb"  ) ||
                      ( currentarg == "-gbD"   ) ||
                      ( currentarg == "-gbk"   ) ||
                      ( currentarg == "-gbx"   ) ||
                      ( currentarg == "-gbo"   ) ||
                      ( currentarg == "-gbB"   ) ||
                      ( currentarg == "-gbr"   ) ||
                      ( currentarg == "-gbu"   ) ||
                      ( currentarg == "-gbv"   ) ||
                      ( currentarg == "-gmw"   ) ||
                      ( currentarg == "-gmn"   ) ||
                      ( currentarg == "-gmt"   ) ||
                      ( currentarg == "-gmq"   ) ||
                      ( currentarg == "-gbim"  ) ||
                      ( currentarg == "-gbq"   ) ||
                      ( currentarg == "-gma"   ) ||
                      ( currentarg == "-gmT"   ) ||
                      ( currentarg == "-gmd"   ) ||
                      ( currentarg == "-gmg"   ) ||
                      ( currentarg == "-gmgg"  ) ||
                      ( currentarg == "-gmma"  ) ||
                      ( currentarg == "-gmmb"  ) ||
                      ( currentarg == "-gmmc"  ) ||
                      ( currentarg == "-gmmd"  ) ||
                      ( currentarg == "-gbG"   ) ||
                      ( currentarg == "-gbmm"  ) ||
                      ( currentarg == "-gbpd"  ) ||
                      ( currentarg == "-gbp"   ) ||
                      ( currentarg == "-gbpl"  ) ||
                      ( currentarg == "-gbpu"  ) ||
                      ( currentarg == "-gdy"   ) ||
                      ( currentarg == "-gNa"   ) ||
                      ( currentarg == "-gNb"   ) ||
                      ( currentarg == "-gNc"   ) ||
                      ( currentarg == "-gNd"   ) ||
                      ( currentarg == "-gNg"   ) ||
                      ( currentarg == "-gNe"   ) ||
                      ( currentarg == "-gNf"   ) ||
                      ( currentarg == "-gBy"   ) ||
                      ( currentarg == "-gBfm"  ) ||
                      ( currentarg == "-gBfM"  ) ||
                      ( currentarg == "-gBfU"  ) ||
                      ( currentarg == "-gBdc"  ) ||
                      ( currentarg == "-gBdf"  ) ||
                      ( currentarg == "-gBde"  ) ||
                      ( currentarg == "-gBda"  ) ||
                      ( currentarg == "-gBdy"  ) ||
                      ( currentarg == "-gBbj"  ) ||
                      ( currentarg == "-gBbt"  ) ||
                      ( currentarg == "-gmW"   ) ||
                      ( currentarg == "-gBbH"  ) ||
                      ( currentarg == "-gbsp"  ) ||
                      ( currentarg == "-gbsP"  ) ||
                      ( currentarg == "-gbsA"  ) ||
                      ( currentarg == "-gbsB"  ) ||
                      ( currentarg == "-gbsF"  ) ||
                      ( currentarg == "-gbsr"  ) ||
                      ( currentarg == "-gbsz"  ) ||
                      ( currentarg == "-gbss"  ) ||
                      ( currentarg == "-gbst"  ) ||
                      ( currentarg == "-gbuu"  ) ||
                      ( currentarg == "-gbuk"  ) ||
                      ( currentarg == "-gbuS"  ) ||
                      ( currentarg == "-gphks"   ) ||
                      ( currentarg == "-gphki"   ) ||
                      ( currentarg == "-gphka"   ) ||
                      ( currentarg == "-gphkb"   ) ||
                      ( currentarg == "-gphke"   ) ||
                      ( currentarg == "-gphkw"   ) ||
                      ( currentarg == "-gphkt"   ) ||
                      ( currentarg == "-gphktx"  ) ||
                      ( currentarg == "-gphktk"  ) ||
                      ( currentarg == "-gphkg"   ) ||
                      ( currentarg == "-gphkgg"  ) ||
                      ( currentarg == "-gphkf"   ) ||
                      ( currentarg == "-gphkr"   ) ||
                      ( currentarg == "-gphkd"   ) ||
                      ( currentarg == "-gphkG"   ) ||
                      ( currentarg == "-gphkI"   ) ||
                      ( currentarg == "-gphkan"  ) ||
                      ( currentarg == "-gPks"    ) ||
                      ( currentarg == "-gPki"    ) ||
                      ( currentarg == "-gPka"    ) ||
                      ( currentarg == "-gPkb"    ) ||
                      ( currentarg == "-gPke"    ) ||
                      ( currentarg == "-gPkw"    ) ||
                      ( currentarg == "-gPkt"    ) ||
                      ( currentarg == "-gPktx"   ) ||
                      ( currentarg == "-gPktk"   ) ||
                      ( currentarg == "-gPkg"    ) ||
                      ( currentarg == "-gPkgg"   ) ||
                      ( currentarg == "-gPkf"    ) ||
                      ( currentarg == "-gPkr"    ) ||
                      ( currentarg == "-gPkd"    ) ||
                      ( currentarg == "-gPkG"    ) ||
                      ( currentarg == "-gPkI"    ) ||
                      ( currentarg == "-gPkan"   )    )
            {
                // Learning options

                if ( grabargs(2,gridopt,commstack,currentarg) )
                {
                    retval  = 74;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-gphkv" ) ||
                      ( currentarg == "-gphkV" ) ||
                      ( currentarg == "-gphko" ) ||
                      ( currentarg == "-gphkO" ) ||
                      ( currentarg == "-gPkv"  ) ||
                      ( currentarg == "-gPkV"  ) ||
                      ( currentarg == "-gPko"  ) ||
                      ( currentarg == "-gPkO"  )    )
            {
                // Learning options

                if ( grabargs(3,gridopt,commstack,currentarg) )
                {
                    retval  = 74;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-xi" ) ||
                      ( currentarg == "-xt" ) ||
                      ( currentarg == "-xs" )    )
            {
                // Kernel transfer options

                if ( grabargs(2,xferopt,commstack,currentarg) )
                {
                    retval  = 75;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( currentarg == "-x" )
            {
                // Kernel transfer options

                if ( grabargs(3,xferopt,commstack,currentarg) )
                {
                    retval  = 75;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fsx"   ) ||
                      ( currentarg == "-fsxz"  ) ||
                      ( currentarg == "-fsxB"  ) ||
                      ( currentarg == "-fsxzB" ) ||
                      ( currentarg == "-fsr"   ) ||
                      ( currentarg == "-fsrB"  ) ||
                      ( currentarg == "-fSx"   ) ||
                      ( currentarg == "-fSxz"  ) ||
                      ( currentarg == "-fSxB"  ) ||
                      ( currentarg == "-fSxzB" ) ||
                      ( currentarg == "-fSr"   ) ||
                      ( currentarg == "-fSrB"  ) ||
                      ( currentarg == "-fsd"   ) ||
                      ( currentarg == "-fsD"   )    )
            {
                // Feature selection options

                if ( grabargs(1,featureopt,commstack,currentarg) )
                {
                    retval  = 75;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fsc"    ) ||
                      ( currentarg == "-fscz"   ) ||
                      ( currentarg == "-fscB"   ) ||
                      ( currentarg == "-fsczB"  ) ||
                      ( currentarg == "-fSc"    ) ||
                      ( currentarg == "-fScz"   ) ||
                      ( currentarg == "-fScB"   ) ||
                      ( currentarg == "-fSczB"  ) ||
                      ( currentarg == "-fsf"    ) ||
                      ( currentarg == "-fsfe"   ) ||
                      ( currentarg == "-fsfi"   ) ||
                      ( currentarg == "-fsfI"   ) ||
                      ( currentarg == "-fsfu"   ) ||
                      ( currentarg == "-fsfeu"  ) ||
                      ( currentarg == "-fsfiu"  ) ||
                      ( currentarg == "-fsfIu"  ) ||
                      ( currentarg == "-fsfB"   ) ||
                      ( currentarg == "-fsfeB"  ) ||
                      ( currentarg == "-fsfiB"  ) ||
                      ( currentarg == "-fsfIB"  ) ||
                      ( currentarg == "-fsfuB"  ) ||
                      ( currentarg == "-fsfeuB" ) ||
                      ( currentarg == "-fsfiuB" ) ||
                      ( currentarg == "-fsfIuB" ) ||
                      ( currentarg == "-fSf"    ) ||
                      ( currentarg == "-fSfe"   ) ||
                      ( currentarg == "-fSfi"   ) ||
                      ( currentarg == "-fSfI"   ) ||
                      ( currentarg == "-fSfu"   ) ||
                      ( currentarg == "-fSfeu"  ) ||
                      ( currentarg == "-fSfiu"  ) ||
                      ( currentarg == "-fSfIu"  ) ||
                      ( currentarg == "-fSfB"   ) ||
                      ( currentarg == "-fSfeB"  ) ||
                      ( currentarg == "-fSfiB"  ) ||
                      ( currentarg == "-fSfIB"  ) ||
                      ( currentarg == "-fSfuB"  ) ||
                      ( currentarg == "-fSfeuB" ) ||
                      ( currentarg == "-fSfiuB" ) ||
                      ( currentarg == "-fSfIuB" ) ||
                      ( currentarg == "-fss"    )    )
            {
                // Feature selection options

                if ( grabargs(2,featureopt,commstack,currentarg) )
                {
                    retval  = 76;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fsC"    ) ||
                      ( currentarg == "-fsCz"   ) ||
                      ( currentarg == "-fsCB"   ) ||
                      ( currentarg == "-fsCzB"  ) ||
                      ( currentarg == "-fsfl"   ) ||
                      ( currentarg == "-fsfel"  ) ||
                      ( currentarg == "-fsfil"  ) ||
                      ( currentarg == "-fsfIl"  ) ||
                      ( currentarg == "-fsflB"  ) ||
                      ( currentarg == "-fsfelB" ) ||
                      ( currentarg == "-fsfilB" ) ||
                      ( currentarg == "-fsfIlB" ) ||
                      ( currentarg == "-fSC"    ) ||
                      ( currentarg == "-fSCz"   ) ||
                      ( currentarg == "-fSCB"   ) ||
                      ( currentarg == "-fSCzB"  ) ||
                      ( currentarg == "-fSfl"   ) ||
                      ( currentarg == "-fSfel"  ) ||
                      ( currentarg == "-fSfil"  ) ||
                      ( currentarg == "-fSfIl"  ) ||
                      ( currentarg == "-fSflB"  ) ||
                      ( currentarg == "-fSfelB" ) ||
                      ( currentarg == "-fSfilB" ) ||
                      ( currentarg == "-fSfIlB" )    )
            {
                // Feature selection options

                if ( grabargs(3,featureopt,commstack,currentarg) )
                {
                    retval  = 77;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fsF"    ) ||
                      ( currentarg == "-fsFe"   ) ||
                      ( currentarg == "-fsFi"   ) ||
                      ( currentarg == "-fsFI"   ) ||
                      ( currentarg == "-fsFr"   ) ||
                      ( currentarg == "-fsFR"   ) ||
                      ( currentarg == "-fsFu"   ) ||
                      ( currentarg == "-fsFeu"  ) ||
                      ( currentarg == "-fsFiu"  ) ||
                      ( currentarg == "-fsFIu"  ) ||
                      ( currentarg == "-fsFru"  ) ||
                      ( currentarg == "-fsFRu"  ) ||
                      ( currentarg == "-fsFB"   ) ||
                      ( currentarg == "-fsFeB"  ) ||
                      ( currentarg == "-fsFiB"  ) ||
                      ( currentarg == "-fsFIB"  ) ||
                      ( currentarg == "-fsFrB"  ) ||
                      ( currentarg == "-fsFRB"  ) ||
                      ( currentarg == "-fsFuB"  ) ||
                      ( currentarg == "-fsFeuB" ) ||
                      ( currentarg == "-fsFiuB" ) ||
                      ( currentarg == "-fsFIuB" ) ||
                      ( currentarg == "-fsFruB" ) ||
                      ( currentarg == "-fsFRuB" ) ||
                      ( currentarg == "-fSF"    ) ||
                      ( currentarg == "-fSFe"   ) ||
                      ( currentarg == "-fSFi"   ) ||
                      ( currentarg == "-fSFI"   ) ||
                      ( currentarg == "-fSFr"   ) ||
                      ( currentarg == "-fSFR"   ) ||
                      ( currentarg == "-fSFu"   ) ||
                      ( currentarg == "-fSFeu"  ) ||
                      ( currentarg == "-fSFiu"  ) ||
                      ( currentarg == "-fSFIu"  ) ||
                      ( currentarg == "-fSFru"  ) ||
                      ( currentarg == "-fSFRu"  ) ||
                      ( currentarg == "-fSFB"   ) ||
                      ( currentarg == "-fSFeB"  ) ||
                      ( currentarg == "-fSFiB"  ) ||
                      ( currentarg == "-fSFIB"  ) ||
                      ( currentarg == "-fSFrB"  ) ||
                      ( currentarg == "-fSFRB"  ) ||
                      ( currentarg == "-fSFuB"  ) ||
                      ( currentarg == "-fSFeuB" ) ||
                      ( currentarg == "-fSFiuB" ) ||
                      ( currentarg == "-fSFIuB" ) ||
                      ( currentarg == "-fSFruB" ) ||
                      ( currentarg == "-fSFRuB" )    )
            {
                // Feature selection options

                if ( grabargs(4,featureopt,commstack,currentarg) )
                {
                    retval  = 78;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fsFl"   ) ||
                      ( currentarg == "-fsFel"  ) ||
                      ( currentarg == "-fsFil"  ) ||
                      ( currentarg == "-fsFIl"  ) ||
                      ( currentarg == "-fsFrl"  ) ||
                      ( currentarg == "-fsFRl"  ) ||
                      ( currentarg == "-fsFlB"  ) ||
                      ( currentarg == "-fsFelB" ) ||
                      ( currentarg == "-fsFilB" ) ||
                      ( currentarg == "-fsFIlB" ) ||
                      ( currentarg == "-fsFrlB" ) ||
                      ( currentarg == "-fsFRlB" ) ||
                      ( currentarg == "-fSFl"   ) ||
                      ( currentarg == "-fSFel"  ) ||
                      ( currentarg == "-fSFil"  ) ||
                      ( currentarg == "-fSFIl"  ) ||
                      ( currentarg == "-fSFrl"  ) ||
                      ( currentarg == "-fSFRl"  ) ||
                      ( currentarg == "-fSFlB"  ) ||
                      ( currentarg == "-fSFelB" ) ||
                      ( currentarg == "-fSFilB" ) ||
                      ( currentarg == "-fSFIlB" ) ||
                      ( currentarg == "-fSFrlB" ) ||
                      ( currentarg == "-fSFRlB" )    )
            {
                // Feature selection options

                if ( grabargs(5,featureopt,commstack,currentarg) )
                {
                    retval  = 79;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fzt"    ) ||
                      ( currentarg == "-fzs"    ) ||
                      ( currentarg == "-fztf"   ) ||
                      ( currentarg == "-fztm"   ) ||
                      ( currentarg == "-fztNlA" ) ||
                      ( currentarg == "-fzsf"   ) ||
                      ( currentarg == "-fzsm"   ) ||
                      ( currentarg == "-fzsNlA" )    )
            {
                // Fuzzy options

                if ( grabargs(2,fuzzyopt,commstack,currentarg) )
                {
                    retval  = 80;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fztkn"   ) ||
                      ( currentarg == "-fztku"   ) ||
                      ( currentarg == "-fztknn"  ) ||
                      ( currentarg == "-fztkuu"  ) ||
                      ( currentarg == "-fztkc"   ) ||
                      ( currentarg == "-fztkuc"  ) ||
                      ( currentarg == "-fztkm"   ) ||
                      ( currentarg == "-fztkum"  ) ||
                      ( currentarg == "-fztkS"   ) ||
                      ( currentarg == "-fztkuS"  ) ||
                      ( currentarg == "-fztkMS"  ) ||
                      ( currentarg == "-fztkMuS" ) ||
                      ( currentarg == "-fztkU"   ) ||
                      ( currentarg == "-fztkoz"  ) ||
                      ( currentarg == "-fztkmtb" ) ||
                      ( currentarg == "-fztkbmx" ) ||
                      ( currentarg == "-fztkOz"  ) ||
                      ( currentarg == "-fzskn"   ) ||
                      ( currentarg == "-fzsku"   ) ||
                      ( currentarg == "-fzsknn"  ) ||
                      ( currentarg == "-fzskuu"  ) ||
                      ( currentarg == "-fzskc"   ) ||
                      ( currentarg == "-fzskuc"  ) ||
                      ( currentarg == "-fzskm"   ) ||
                      ( currentarg == "-fzskum"  ) ||
                      ( currentarg == "-fzskS"   ) ||
                      ( currentarg == "-fzskuS"  ) ||
                      ( currentarg == "-fzskMS"  ) ||
                      ( currentarg == "-fzskMuS" ) ||
                      ( currentarg == "-fzskU"   ) ||
                      ( currentarg == "-fzskoz"  ) ||
                      ( currentarg == "-fzskmtb" ) ||
                      ( currentarg == "-fzskbmx" ) ||
                      ( currentarg == "-fzskOz"  )    )
            {
                // Fuzzy options

                if ( grabargs(1,fuzzyopt,commstack,currentarg) )
                {
                    retval  = 81;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fztks"   ) ||
                      ( currentarg == "-fztki"   ) ||
                      ( currentarg == "-fztka"   ) ||
                      ( currentarg == "-fztkb"   ) ||
                      ( currentarg == "-fztke"   ) ||
                      ( currentarg == "-fztkw"   ) ||
                      ( currentarg == "-fztkt"   ) ||
                      ( currentarg == "-fztktx"  ) ||
                      ( currentarg == "-fztktk"  ) ||
                      ( currentarg == "-fztkg"   ) ||
                      ( currentarg == "-fztkgg"  ) ||
                      ( currentarg == "-fztkf"   ) ||
                      ( currentarg == "-fztkr"   ) ||
                      ( currentarg == "-fztkd"   ) ||
                      ( currentarg == "-fztkG"   ) ||
                      ( currentarg == "-fztkI"   ) ||
                      ( currentarg == "-fztkan"  ) ||
                      ( currentarg == "-fzsks"   ) ||
                      ( currentarg == "-fzski"   ) ||
                      ( currentarg == "-fzska"   ) ||
                      ( currentarg == "-fzskb"   ) ||
                      ( currentarg == "-fzske"   ) ||
                      ( currentarg == "-fzskw"   ) ||
                      ( currentarg == "-fzskt"   ) ||
                      ( currentarg == "-fzsktx"  ) ||
                      ( currentarg == "-fzsktk"  ) ||
                      ( currentarg == "-fzskg"   ) ||
                      ( currentarg == "-fzskgg"  ) ||
                      ( currentarg == "-fzskf"   ) ||
                      ( currentarg == "-fzskr"   ) ||
                      ( currentarg == "-fzskd"   ) ||
                      ( currentarg == "-fzskG"   ) ||
                      ( currentarg == "-fzskI"   ) ||
                      ( currentarg == "-fzskan"  )    )
            {
                // Fuzzy options

                if ( grabargs(2,fuzzyopt,commstack,currentarg) )
                {
                    retval  = 82;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fztkv" ) ||
                      ( currentarg == "-fztkV" ) ||
                      ( currentarg == "-fztko" ) ||
                      ( currentarg == "-fztkO" ) ||
                      ( currentarg == "-fzskv" ) ||
                      ( currentarg == "-fzskV" ) ||
                      ( currentarg == "-fzsko" ) ||
                      ( currentarg == "-fzskO" )    )
            {
                // Fuzzy options

                if ( grabargs(3,fuzzyopt,commstack,currentarg) )
                {
                    retval  = 84;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-boot" ) )
            {
                // Bootstrap options

                if ( grabargs(1,bootopt,commstack,currentarg) )
                {
                    retval  = 85;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-M0"  ) ||
                      ( currentarg == "-M1"  ) ||
                      ( currentarg == "-M2"  ) ||
                      ( currentarg == "-M3"  ) ||
                      ( currentarg == "-M4"  ) ||
                      ( currentarg == "-M5"  ) ||
                      ( currentarg == "-M6"  ) ||
                      ( currentarg == "-M7"  ) ||
                      ( currentarg == "-M8"  ) ||
                      ( currentarg == "-M9"  ) ||
                      ( currentarg == "-M10" ) ||
                      ( currentarg == "-M11" ) ||
                      ( currentarg == "-M12" ) ||
                      ( currentarg == "-M13" ) ||
                      ( currentarg == "-M14" ) ||
                      ( currentarg == "-M15" ) ||
                      ( currentarg == "-M16" ) ||
                      ( currentarg == "-M17" ) ||
                      ( currentarg == "-M18" ) ||
                      ( currentarg == "-M19" ) ||
                      ( currentarg == "-M20" ) ||
                      ( currentarg == "-M21" ) ||
                      ( currentarg == "-M22" ) ||
                      ( currentarg == "-M23" ) ||
                      ( currentarg == "-M24" ) ||
                      ( currentarg == "-M25" ) ||
                      ( currentarg == "-M26" ) ||
                      ( currentarg == "-M27" ) ||
                      ( currentarg == "-M28" ) ||
                      ( currentarg == "-M29" ) ||
                      ( currentarg == "-M30" ) ||
                      ( currentarg == "-M31" )    )
            {
                // Macro options

                if ( grabargs(1,macroopt,commstack,currentarg) )
                {
                    retval  = 85;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-MM" ) ||
                      ( currentarg == "-MF" )    )
            {
                // Macro options

                if ( grabargs(2,macroopt,commstack,currentarg) )
                {
                    retval  = 86;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-oo"  ) ||
                      ( currentarg == "-ofy" ) ||
                      ( currentarg == "-ofn" ) ||
                      ( currentarg == "-oO"  )    )
            {
                // Optimisation options

                if ( grabargs(1,optimopt,commstack,currentarg) )
                {
                    retval  = 87;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-om"  ) ||
                      ( currentarg == "-oM"  ) ||
                      ( currentarg == "-oe"  ) ||
                      ( currentarg == "-oz"  ) ||
                      ( currentarg == "-ot"  ) ||
                      ( currentarg == "-oy"  ) ||
                      ( currentarg == "-oge" ) ||
                      ( currentarg == "-ogm" ) ||
                      ( currentarg == "-ogr" ) ||
                      ( currentarg == "-ogs" ) ||
                      ( currentarg == "-ogt" ) ||
                      ( currentarg == "-ogT" ) ||
                      ( currentarg == "-ofa" ) ||
                      ( currentarg == "-ofe" ) ||
                      ( currentarg == "-ofm" ) ||
                      ( currentarg == "-ofr" ) ||
                      ( currentarg == "-ofs" ) ||
                      ( currentarg == "-oft" ) ||
                      ( currentarg == "-ofM" ) ||
                      ( currentarg == "-omr" ) ||
                      ( currentarg == "-ome" ) ||
                      ( currentarg == "-oms" ) ||
                      ( currentarg == "-olr" )    )
            {
                // Optimisation options

                if ( grabargs(2,optimopt,commstack,currentarg) )
                {
                    retval  = 88;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tx"    ) ||
                      ( currentarg == "-tl"    ) ||
                      ( currentarg == "-tQ"    ) ||
                      ( currentarg == "-tvar"  ) ||
                      ( currentarg == "-txz"   ) ||
                      ( currentarg == "-txB"   ) ||
                      ( currentarg == "-txzB"  ) ||
                      ( currentarg == "-tr"    ) ||
                      ( currentarg == "-trB"   )    )
            {
                // Testing options

                if ( grabargs(1,performopt,commstack,currentarg) )
                {
                    retval  = 89;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tm"    ) ||
                      ( currentarg == "-tn"    ) ||
                      ( currentarg == "-tM"    ) ||
                      ( currentarg == "-tMv"   ) ||
                      ( currentarg == "-tc"    ) ||
                      ( currentarg == "-tcZ"   ) ||
                      ( currentarg == "-tcz"   ) ||
                      ( currentarg == "-tcB"   ) ||
                      ( currentarg == "-tczB"  ) ||
                      ( currentarg == "-tf"    ) ||
                      ( currentarg == "-tfe"   ) ||
                      ( currentarg == "-tfi"   ) ||
                      ( currentarg == "-tfI"   ) ||
                      ( currentarg == "-tfu"   ) ||
                      ( currentarg == "-tfeu"  ) ||
                      ( currentarg == "-tfiu"  ) ||
                      ( currentarg == "-tfIu"  ) ||
                      ( currentarg == "-tfB"   ) ||
                      ( currentarg == "-tfeB"  ) ||
                      ( currentarg == "-tfiB"  ) ||
                      ( currentarg == "-tfIB"  ) ||
                      ( currentarg == "-tfuB"  ) ||
                      ( currentarg == "-tfeuB" ) ||
                      ( currentarg == "-tfiuB" ) ||
                      ( currentarg == "-tfIuB" )    )
            {
                // Testing options

                if ( grabargs(2,performopt,commstack,currentarg) )
                {
                    retval  = 90;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tC"     ) ||
                      ( currentarg == "-tCZ"    ) ||
                      ( currentarg == "-tCz"    ) ||
                      ( currentarg == "-tCB"    ) ||
                      ( currentarg == "-tCzB"   ) ||
                      ( currentarg == "-tMpy"   ) ||
                      ( currentarg == "-tMpyv"  ) ||
                      ( currentarg == "-tMpyf"  ) ||
                      ( currentarg == "-tMxpy"  ) ||
                      ( currentarg == "-tMxpyv" ) ||
                      ( currentarg == "-tMxpyf" ) ||
                      ( currentarg == "-tMypy"  ) ||
                      ( currentarg == "-tMypyv" ) ||
                      ( currentarg == "-tMypyf" ) ||
                      ( currentarg == "-tMexe"  ) ||
                      ( currentarg == "-tMexev" ) ||
                      ( currentarg == "-tMexef" ) ||
                      ( currentarg == "-tfl"    ) ||
                      ( currentarg == "-tfel"   ) ||
                      ( currentarg == "-tfil"   ) ||
                      ( currentarg == "-tfIl"   ) ||
                      ( currentarg == "-tflB"   ) ||
                      ( currentarg == "-tfelB"  ) ||
                      ( currentarg == "-tfilB"  ) ||
                      ( currentarg == "-tfIlB"  ) ||
                      ( currentarg == "-tV"     ) ||
                      ( currentarg == "-tW"     )    )
            {
                // Testing options

                if ( grabargs(3,performopt,commstack,currentarg) )
                {
                    retval  = 91;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tF"    ) ||
                      ( currentarg == "-tMd"   ) ||
                      ( currentarg == "-tMD"   ) ||
                      ( currentarg == "-tFe"   ) ||
                      ( currentarg == "-tFi"   ) ||
                      ( currentarg == "-tFI"   ) ||
                      ( currentarg == "-tFr"   ) ||
                      ( currentarg == "-tFR"   ) ||
                      ( currentarg == "-tFu"   ) ||
                      ( currentarg == "-tFeu"  ) ||
                      ( currentarg == "-tFiu"  ) ||
                      ( currentarg == "-tFIu"  ) ||
                      ( currentarg == "-tFru"  ) ||
                      ( currentarg == "-tFRu"  ) ||
                      ( currentarg == "-tFB"   ) ||
                      ( currentarg == "-tFeB"  ) ||
                      ( currentarg == "-tFiB"  ) ||
                      ( currentarg == "-tFIB"  ) ||
                      ( currentarg == "-tFrB"  ) ||
                      ( currentarg == "-tFRB"  ) ||
                      ( currentarg == "-tFuB"  ) ||
                      ( currentarg == "-tFeuB" ) ||
                      ( currentarg == "-tFiuB" ) ||
                      ( currentarg == "-tFIuB" ) ||
                      ( currentarg == "-tFruB" ) ||
                      ( currentarg == "-tFRuB" )    )
            {
                // Testing options

                if ( grabargs(4,performopt,commstack,currentarg) )
                {
                    retval  = 92;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tFl"   ) ||
                      ( currentarg == "-tFel"  ) ||
                      ( currentarg == "-tFil"  ) ||
                      ( currentarg == "-tFIl"  ) ||
                      ( currentarg == "-tFrl"  ) ||
                      ( currentarg == "-tFRl"  ) ||
                      ( currentarg == "-tFlB"  ) ||
                      ( currentarg == "-tFelB" ) ||
                      ( currentarg == "-tb"    ) ||
                      ( currentarg == "-tg"    ) ||
                      ( currentarg == "-tG"    ) ||
                      ( currentarg == "-tFilB" ) ||
                      ( currentarg == "-tFIlB" ) ||
                      ( currentarg == "-tFrlB" ) ||
                      ( currentarg == "-tFRlB" )    )
            {
                // Testing options

                if ( grabargs(5,performopt,commstack,currentarg) )
                {
                    retval  = 93;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tgc" ) ||
                      ( currentarg == "-tGc" )    )
            {
                // Testing options

                if ( grabargs(6,performopt,commstack,currentarg) )
                {
                    retval  = 93;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-hX"  ) ||
                      ( currentarg == "-K0"  ) ||
                      ( currentarg == "-hXv" )    )

            {
                // Reporting options

                if ( grabargs(1,reportopt,commstack,currentarg) )
                {
                    retval  = 94;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-a"    ) ||
                      ( currentarg == "-b"    ) ||
                      ( currentarg == "-s"    ) ||
                      ( currentarg == "-hU"   ) ||
                      ( currentarg == "-K1"   ) ||
                      ( currentarg == "-phi2" ) ||
                      ( currentarg == "-hY"   ) ||
                      ( currentarg == "-hV"   ) ||
                      ( currentarg == "-hW"   ) ||
                      ( currentarg == "-hUv"  ) ||
                      ( currentarg == "-hYv"  ) ||
                      ( currentarg == "-hVv"  ) ||
                      ( currentarg == "-hVV"  ) ||
                      ( currentarg == "-hWv"  ) ||
                      ( currentarg == "-echo" ) ||
                      ( currentarg == "-ECHO" )    )
            {
                // Reporting options

                if ( grabargs(2,reportopt,commstack,currentarg) )
                {
                    retval  = 95;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-hM"   ) ||
                      ( currentarg == "-hN"   ) ||
                      ( currentarg == "-hZ"   ) ||
                      ( currentarg == "-hUc"  ) ||
                      ( currentarg == "-K2"   ) ||
                      ( currentarg == "-hYc"  ) ||
                      ( currentarg == "-hWc"  ) ||
                      ( currentarg == "-hZv"  ) ||
                      ( currentarg == "-echosock" ) ||
                      ( currentarg == "-ECHOsock" )    )
            {
                // Reporting options

                if ( grabargs(3,reportopt,commstack,currentarg) )
                {
                    retval  = 96;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-hZc"  ) ||
                      ( currentarg == "-K3"   )    )
            {
                // Reporting options

                if ( grabargs(4,reportopt,commstack,currentarg) )
                {
                    retval  = 97;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-K4"   ) ||
                      ( currentarg == "-hp"   ) ||
                      ( currentarg == "-hpi"  )    )
            {
                // Reporting options

                if ( grabargs(5,reportopt,commstack,currentarg) )
                {
                    retval  = 97;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Km" )    )
            {
                // Reporting options
                //
                // To process we first read the number of arguments.  Then
                // there are a arguments to come, where a is # args

                std::string storearg = currentarg;

                int a = 0;
                int b = reportopt.size();

                stopnow = grabnextarg(commstack,currentarg);

                std::string mlarg = currentarg;

                if ( !stopnow )
                {
                    a = safeatoi(currentarg,argvariables);

                    if ( ( a >= 0 ) && ( grabargs(a+1,reportopt,commstack,( currentarg = storearg )) ) )
                    {
                        retval  = 70;
                        stopnow = 1;
                    }

                    // I was using "else if" here, and a nested structure above, but...
                    // "compiler limit : blocks nested too deeply"
                    // ...spake microsoft c-c++ compiler driver (in mex)

                    if ( a < 0 )
                    {
                        errstream() << "Syntax error: " << storearg << " argument count must be non-negative\n";
                        retval  = 71;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: " << storearg << " requires 1 argument minimum\n";
                    retval  = 72;
                    stopnow = 1;
                }

                reportopt("&",b).add(1);
                reportopt("&",b)("&",1) = mlarg;

                updateargvars = 1;
            }

            else if ( ( currentarg == "-hP"   ) ||
                      ( currentarg == "-hPi"  )    )
            {
                // Reporting options

                if ( grabargs(6,reportopt,commstack,currentarg) )
                {
                    retval  = 97;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else
            {
                // Unknown command

                errstream() << "Syntax error: command " << currentarg << " unknown.\n";
                retval  = 98;
                stopnow = 1;
            }
        }







































// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================





        else if ( !retval && ( skipon   ||
                  loggingopt.size()     ||
                  multirunopt.size()    ||
                  svmsetupopt.size()    ||
                  svmpresetupopt.size() ||
                  preloadopt.size()     ||
                  loadopt.size()        ||
                  postloadopt.size()    ||
                  learningopt.size()    ||
                  kernelopt.size()      ||
                  tuningopt.size()      ||
                  gridopt.size()        ||
                  xferopt.size()        ||
                  featureopt.size()     ||
                  fuzzyopt.size()       ||
                  bootopt.size()        ||
                  macroopt.size()       ||
                  optimopt.size()       ||
                  performopt.size()     ||
                  reportopt.size()         ) )
        {
        processnow:

          skipon       = 0;
          argbatchsize = 0;

          Vector<std::string> currcommand;

          try
          {
            multiruntime    = 0;
            svmsetuptime    = 0;
            svmpresetuptime = 0;
            preloadtime     = 0;
            loadtime        = 0;
            postloadtime    = 0;
            learningtime    = 0;
            kerneltime      = 0;
            tuningtime      = 0;
            gridtime        = 0;
            xfertime        = 0;
            featuretime     = 0;
            fuzzytime       = 0;
            optimtime       = 0;
            performtime     = 0;
            reporttime      = 0;

            // Run through relevant SVM tests

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Logging steps

            if ( loggingopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing logging setup operations... ";

                while ( loggingopt.size() )
                {
                    currcommand = loggingopt(zeroint());
                    loggingopt.remove(zeroint());

                    if ( currcommand(zeroint()) == "-v" )
                    {
                        // Change verbosity level

                        verblevel = safeatoi(currcommand(1),argvariables);

                        argvariables("&",1)("&",13) = verblevel;

                        if ( ( verblevel != 0 ) && ( verblevel != 1 ) )
                        {
                            STRTHROW("Syntax error: -v options are 0,1");
                        }
                    }

                    else if ( currcommand(zeroint()) == "-L" )
                    {
                        // Change log file name

                        logfile = currcommand(1);
                        argvariables("&",1)("&",12).makeString(logfile);
                    }

                    else if ( currcommand(zeroint()) == "-LL" )
                    {
                        // Change log file name

                        gentype temparg;

                        safeatowhatever(temparg,currcommand(1),argvariables);
                        logfile = (const std::string &) temparg;
                        argvariables("&",1)("&",12).makeString(logfile);
                    }
                }

                time_used endtime = TIMECALL;
                loggingtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << loggingtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Multirun steps

            if ( multirunopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing multi-run setup operations... ";

                while ( multirunopt.size() )
                {
                    currcommand = multirunopt(zeroint());
                    multirunopt.remove(zeroint());

                    if ( currcommand(zeroint()) == "-qR" )
                    {
                        // Delete SVM completely

                        int nInd = safeatoi(currcommand(1),argvariables);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive SVM index "+currcommand(1)+" in -qR");
                        }

                        // First claim the ML in question for the current thread...

                        grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);

                        // ...delete it...

                        MEMDEL(svmbase("&",nInd));

                        // ...and remove all traces

                        svmbase.zero(nInd);
                        svmThreadOwner.zero(nInd);

                        // NB: if nInd = svmInd then the SVM will be automatically
                        // re-created in restarted state on first dereference
                        // using svmbase(svmInd).

                        if ( nInd == svmInd )
                        {
                            argvariables("&",42)("&",42) = svmInd;
                        }
                    }

                    else if ( currcommand(zeroint()) == "-qc" )
                    {
                        // copy SVM - overwrite n with m

                        int nInd = safeatoi(currcommand(1),argvariables);
                        int mInd = safeatoi(currcommand(2),argvariables);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive first SVM index "+currcommand(1)+" in -qs");
                        }

                        if ( mInd <= 0 )
                        {
                            STRTHROW("Non-positive second SVM index "+currcommand(2)+" in -qs");
                        }

                        if ( nInd != mInd )
                        {
                            // Claim both SVMs for current thread...

                            grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);
                            grabsvm(svmThreadOwner,svmbase,threadInd,mInd,svmContext);

                            // ...overwrite...

                            getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext) = getMLrefconst(svmThreadOwner,svmbase,threadInd,mInd,svmContext);

                            // ...and disowm both as/if required.

                            svmThreadOwner("&",nInd) = ( svmInd == nInd ) ? threadInd : -1;
                            svmThreadOwner("&",mInd) = ( svmInd == mInd ) ? threadInd : -1;

                            if ( nInd == svmInd )
                            {
                                argvariables("&",42)("&",42) = svmInd;
                            }
                        }
                    }

                    else if ( currcommand(zeroint()) == "-qs" )
                    {
                        // swap SVMs

                        int nInd = safeatoi(currcommand(1),argvariables);
                        int mInd = safeatoi(currcommand(2),argvariables);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive first SVM index "+currcommand(1)+" in -qs");
                        }

                        if ( mInd <= 0 )
                        {
                            STRTHROW("Non-positive second SVM index "+currcommand(2)+" in -qs");
                        }

                        if ( nInd != mInd )
                        {
                            // Claim both SVMs for current thread...

                            grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);
                            grabsvm(svmThreadOwner,svmbase,threadInd,mInd,svmContext);

                            // ...swap...

                            svmbase.squareswap(nInd,mInd);

                            // ...and disowm both as/if required.

                            svmThreadOwner("&",nInd) = ( svmInd == nInd ) ? threadInd : -1;
                            svmThreadOwner("&",mInd) = ( svmInd == mInd ) ? threadInd : -1;

                            if ( ( nInd == svmInd ) || ( mInd == svmInd ) )
                            {
                                argvariables("&",42)("&",42) = svmInd;
                            }
                        }
                    }

                    else if ( currcommand(zeroint()) == "-qw" )
                    {
                        // set working SVM

                        int nInd = safeatoi(currcommand(1),argvariables);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive SVM index "+currcommand(1)+" in -qw");
                        }

                        if ( nInd != svmInd )
                        {
                            // Disown current ML...

                            svmThreadOwner("&",svmInd) = -1;

                            // ...claim new ML...

                            grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);

                            // ...and update indexes

                            svmInd = nInd;

                            argvariables("&",42)("&",42) = svmInd;
                        }
                    }

                    else if ( currcommand(zeroint()) == "-qpush" )
                    {
                        // set working SVM after pushing old one onto the stack

                        MLindstack.push(svmInd);

                        int nInd = safeatoi(currcommand(1),argvariables);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive SVM index "+currcommand(1)+" in -qpush");
                        }

                        if ( nInd != svmInd )
                        {
                            // Disown current ML...

                            svmThreadOwner("&",svmInd) = -1;

                            // ...claim new ML...

                            grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);

                            // ...and update indexes

                            svmInd = nInd;

                            argvariables("&",42)("&",42) = svmInd;
                        }
                    }

                    else if ( currcommand(zeroint()) == "-qpop" )
                    {
                        // set working SVM to one popped off stack

                        int nInd = -1;

                        MLindstack.pop(nInd);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive SVM index "+currcommand(1)+" in -qpush");
                        }

                        if ( nInd != svmInd )
                        {
                            // Disown current ML...

                            svmThreadOwner("&",svmInd) = -1;

                            // ...claim new ML...

                            grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);

                            // ...and update indexes

                            svmInd = nInd;

                            argvariables("&",42)("&",42) = svmInd;
                        }
                    }
                }

                time_used endtime = TIMECALL;
                multiruntime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << multiruntime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Pre-Setup steps

            if ( svmpresetupopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing setup operations... ";

                while ( svmpresetupopt.size() )
                {
                    currcommand = svmpresetupopt(zeroint());
                    svmpresetupopt.remove(zeroint());

                    if ( currcommand(zeroint()) == "-zl" )
                    {
                        // Load SVM from file

                        argvariables("&",1)("&",14).makeString(currcommand(1));

                        std::ifstream loadfile(currcommand(1).c_str());

                        if ( !loadfile.is_open() )
                        {
                            STRTHROW("Unable to open SVM file "+currcommand(1));
                        }

                        loadfile >> getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        loadfile.close();
                    }

                    else if ( currcommand(zeroint()) == "-z" )
                    {
                        // Set SVM type, and reset SVM

                             if ( currcommand(1) == "r"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(0);   }
                        else if ( currcommand(1) == "c"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(1);   }
                        else if ( currcommand(1) == "s"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(2);   }
                        else if ( currcommand(1) == "m"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(3);   }
                        else if ( currcommand(1) == "v"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(4);   }
                        else if ( currcommand(1) == "a"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(5);   }
                        else if ( currcommand(1) == "e"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(6);   }
                        else if ( currcommand(1) == "p"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(7);   }
                        else if ( currcommand(1) == "t"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(8);   }
                        else if ( currcommand(1) == "l"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(12);  }
                        else if ( currcommand(1) == "o"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(13);  }
                        else if ( currcommand(1) == "g"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(15);  }
                        else if ( currcommand(1) == "i"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(16);  }
                        else if ( currcommand(1) == "h"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(17);  }
                        else if ( currcommand(1) == "j"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(18);  }
                        else if ( currcommand(1) == "b"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(19);  }
                        else if ( currcommand(1) == "u"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(20);  }

                        else if ( currcommand(1) == "onr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(100); }
                        else if ( currcommand(1) == "onv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(101); }
                        else if ( currcommand(1) == "ona" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(102); }
                        else if ( currcommand(1) == "onc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(103); }
                        else if ( currcommand(1) == "one" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(104); }
                        else if ( currcommand(1) == "ong" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(105); }

                        else if ( currcommand(1) == "knp" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(300); }
                        else if ( currcommand(1) == "knc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(301); }
                        else if ( currcommand(1) == "kng" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(302); }
                        else if ( currcommand(1) == "knr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(303); }
                        else if ( currcommand(1) == "knv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(304); }
                        else if ( currcommand(1) == "kna" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(305); }
                        else if ( currcommand(1) == "kne" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(306); }
                        else if ( currcommand(1) == "knm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(307); }

                        else if ( currcommand(1) == "gpr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(400); }
                        else if ( currcommand(1) == "gpv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(401); }
                        else if ( currcommand(1) == "gpa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(402); }
                        else if ( currcommand(1) == "gpg" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(408); }
                        else if ( currcommand(1) == "gpc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(409); }

                        else if ( currcommand(1) == "mlr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(800); }
                        else if ( currcommand(1) == "mlc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(801); }
                        else if ( currcommand(1) == "mlv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(802); }

                        else if ( currcommand(1) == "lsr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(500); }
                        else if ( currcommand(1) == "lsv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(501); }
                        else if ( currcommand(1) == "lsa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(502); }
                        else if ( currcommand(1) == "lso" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(505); }
                        else if ( currcommand(1) == "lse" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(507); }
                        else if ( currcommand(1) == "lsg" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(508); }
                        else if ( currcommand(1) == "lsi" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(509); }
                        else if ( currcommand(1) == "lsh" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(510); }

                        else if ( currcommand(1) == "nop" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(200); }
                        else if ( currcommand(1) == "con" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(201); }
                        else if ( currcommand(1) == "fna" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(203); }
                        else if ( currcommand(1) == "io"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(204); }
                        else if ( currcommand(1) == "avr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(202); }
                        else if ( currcommand(1) == "avv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(205); }
                        else if ( currcommand(1) == "ava" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(206); }
                        else if ( currcommand(1) == "fnb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(207); }
                        else if ( currcommand(1) == "fcb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(208); }
                        else if ( currcommand(1) == "mxa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(209); }
                        else if ( currcommand(1) == "mxb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(210); }
                        else if ( currcommand(1) == "mer" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(211); }
                        else if ( currcommand(1) == "mba" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(212); }
                        else if ( currcommand(1) == "sys" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(213); }
                        else if ( currcommand(1) == "ker" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(214); }
                        else if ( currcommand(1) == "ber" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(215); }
                        else if ( currcommand(1) == "bat" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(216); }

                        else if ( currcommand(1) == "ei"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(600); }
                        else if ( currcommand(1) == "svm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(601); }

                        else if ( currcommand(1) == "ssr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(700); }
                        else if ( currcommand(1) == "ssc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(701); }
                        else if ( currcommand(1) == "sss" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(702); }

                        else if ( currcommand(1) == "ser" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(-2);  }
                        else if ( currcommand(1) == "par" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(-3);  }

                        else { STRTHROW("Syntax error: -z options are many, but this is not one of them"); }
                    }

                    else if ( currcommand(zeroint()) == "-zd" )
                    {
                        // Set SVM type, try to transition data

                             if ( currcommand(1) == "r"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(0);   }
                        else if ( currcommand(1) == "c"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(1);   }
                        else if ( currcommand(1) == "s"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(2);   }
                        else if ( currcommand(1) == "m"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(3);   }
                        else if ( currcommand(1) == "v"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(4);   }
                        else if ( currcommand(1) == "a"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(5);   }
                        else if ( currcommand(1) == "e"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(6);   }
                        else if ( currcommand(1) == "p"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(7);   }
                        else if ( currcommand(1) == "t"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(8);   }
                        else if ( currcommand(1) == "l"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(12);  }
                        else if ( currcommand(1) == "o"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(13);  }
                        else if ( currcommand(1) == "g"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(15);  }
                        else if ( currcommand(1) == "i"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(16);  }
                        else if ( currcommand(1) == "h"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(17);  }
                        else if ( currcommand(1) == "j"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(18);  }
                        else if ( currcommand(1) == "b"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(19);  }
                        else if ( currcommand(1) == "u"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(20);  }

                        else if ( currcommand(1) == "onr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(100); }
                        else if ( currcommand(1) == "onv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(101); }
                        else if ( currcommand(1) == "ona" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(102); }
                        else if ( currcommand(1) == "onc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(103); }
                        else if ( currcommand(1) == "one" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(104); }
                        else if ( currcommand(1) == "ong" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(105); }

                        else if ( currcommand(1) == "knp" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(300); }
                        else if ( currcommand(1) == "knc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(301); }
                        else if ( currcommand(1) == "kng" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(302); }
                        else if ( currcommand(1) == "knr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(303); }
                        else if ( currcommand(1) == "knv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(304); }
                        else if ( currcommand(1) == "kna" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(305); }
                        else if ( currcommand(1) == "kne" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(306); }
                        else if ( currcommand(1) == "knm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(307); }

                        else if ( currcommand(1) == "gpr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(400); }
                        else if ( currcommand(1) == "gpv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(401); }
                        else if ( currcommand(1) == "gpa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(402); }
                        else if ( currcommand(1) == "gpg" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(408); }
                        else if ( currcommand(1) == "gpc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(409); }

                        else if ( currcommand(1) == "mlr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(800); }
                        else if ( currcommand(1) == "mlc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(801); }
                        else if ( currcommand(1) == "mlv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(802); }

                        else if ( currcommand(1) == "lsr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(500); }
                        else if ( currcommand(1) == "lsv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(501); }
                        else if ( currcommand(1) == "lsa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(502); }
                        else if ( currcommand(1) == "lso" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(505); }
                        else if ( currcommand(1) == "lse" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(507); }
                        else if ( currcommand(1) == "lsg" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(508); }
                        else if ( currcommand(1) == "lsi" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(509); }
                        else if ( currcommand(1) == "lsh" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(510); }

                        else if ( currcommand(1) == "nop" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(200); }
                        else if ( currcommand(1) == "con" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(201); }
                        else if ( currcommand(1) == "fna" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(203); }
                        else if ( currcommand(1) == "io"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(204); }
                        else if ( currcommand(1) == "avr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(202); }
                        else if ( currcommand(1) == "avv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(205); }
                        else if ( currcommand(1) == "ava" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(206); }
                        else if ( currcommand(1) == "fnb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(207); }
                        else if ( currcommand(1) == "fcb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(208); }
                        else if ( currcommand(1) == "mxa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(209); }
                        else if ( currcommand(1) == "mxb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(210); }
                        else if ( currcommand(1) == "mer" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(211); }
                        else if ( currcommand(1) == "mba" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(212); }
                        else if ( currcommand(1) == "sys" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(213); }
                        else if ( currcommand(1) == "ker" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(214); }
                        else if ( currcommand(1) == "ber" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(215); }
                        else if ( currcommand(1) == "bat" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(216); }

                        else if ( currcommand(1) == "ei"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(600); }
                        else if ( currcommand(1) == "svm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(601); }

                        else if ( currcommand(1) == "ssr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(700); }
                        else if ( currcommand(1) == "ssc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(701); }
                        else if ( currcommand(1) == "sss" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(702); }

                        else if ( currcommand(1) == "ser" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(-2);  }
                        else if ( currcommand(1) == "par" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(-3);  }

                        else { STRTHROW("Syntax error: -zd options are a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z"); }
                    }

                    else if ( currcommand(zeroint()) == "-zv" )
                    {
                        // Set SVM type, try to transition data

                        if      ( currcommand(1) == "once" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setatonce(); }
                        else if ( currcommand(1) == "red"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setredbin(); }
                        else { STRTHROW("Syntax error: -zv options are atonce (once) and redbin (red)"); }
                    }

                    else if ( currcommand(zeroint()) == "-zc" )
                    {
                        // Set multiclsas SVM type

                        if      ( currcommand(1) == "1vsA"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).set1vsA();    }
                        else if ( currcommand(1) == "1vs1"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).set1vs1();    }
                        else if ( currcommand(1) == "DAG"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setDAGSVM();  }
                        else if ( currcommand(1) == "MOC"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMOC();     }
                        else if ( currcommand(1) == "maxwin" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmaxwins(); }
                        else if ( currcommand(1) == "recdiv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setrecdiv();  }
                        else { STRTHROW("Syntax error: -zc options are 1vsA,1vs1,DAG,MOC,maxwin,recdiv"); }
                    }

                    else if ( currcommand(zeroint()) == "-zo" )
                    {
                        // Set 1-class SVM type

                        if      ( currcommand(1) == "sch" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsingmethod(0); }
                        else if ( currcommand(1) == "tax" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsingmethod(1); }
                        else { STRTHROW("Syntax error: -zo options are sch,tax"); }
                    }
                }

                time_used endtime = TIMECALL;
                svmpresetuptime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << svmpresetuptime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Setup steps

            if ( svmsetupopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing setup operations... ";

                double mooalpha = DEFAULT_MOOALPHA;

                while ( svmsetupopt.size() )
                {
                    currcommand = svmsetupopt(zeroint());
                    svmsetupopt.remove(zeroint());

                    if ( ( currcommand(zeroint()) == "-fo" ) || ( currcommand(zeroint()) == "-foe" ) )
                    {
                        int filenum;
                        int targpos = ( currcommand(zeroint()) == "-fo" ) ? 0 : 1;

                        filenum = safeatoi(currcommand(1),argvariables);

                        if ( filenum < 0 )
                        {
                            STRTHROW("Syntax error: file number in -fo must be non-negative.");
                        }

                        // Open the file

                        std::ifstream datfile(currcommand(2).c_str());

                        if ( !datfile.is_open() )
                        {
                            STRTHROW("Unable to open file -fo "+currcommand(2));
                        }

                        ofiletype templace(filenum,currcommand(2),targpos,datfile);
                        filevariables("&",filenum) = templace;

                        datfile.close();

                        argvariables("&",0)("&",filenum) = templace.getlinecnt();
                    }

                    else if ( currcommand(zeroint()) == "-fV"   ) { argvariables("&",0)("&",safeatoi(currcommand(1),argvariables)) = currcommand(2); }
                    else if ( currcommand(zeroint()) == "-fW"   ) { argvariables("&",0)("&",safeatoi(currcommand(1),argvariables)) = safeatog(currcommand(2),argvariables); argvariables("&",0)("&",safeatoi(currcommand(1),argvariables)).finalise(); }
                    else if ( currcommand(zeroint()) == "-fWW"  ) { argvariables("&",0)("&",safeatoi(currcommand(1),argvariables)) = safeatog(currcommand(2),argvariables); }
                    else if ( currcommand(zeroint()) == "-fret" ) { returntag("&",safeatoi(currcommand(1),argvariables))("&",safeatoi(currcommand(2),argvariables)) = 1; }

                    else if ( currcommand(zeroint()) == "-fWm" )
                    {
                        // Set integer variable to result of mex function

                        int nn = safeatoi(currcommand(1),argvariables);

                        const gentype srcvar = safeatog(currcommand(3),argvariables);
                        gentype &resvar = argvariables("&",0)("&",nn);

                        resvar.makeString(currcommand(2));

                        (*getsetExtVar)(resvar,srcvar,-3);
                    }

                    else if ( currcommand(zeroint()) == "-fWM" )
                    {
                        // Set integer variable

errstream() << "phantomxy 0\n";
                        int nn = safeatoi(currcommand(1),argvariables);
errstream() << "phantomxy 1\n";
                        int ii = safeatoi(currcommand(2),argvariables);
errstream() << "phantomxy 2\n";

                        const gentype srcvar = safeatog(currcommand(3),argvariables);
                        gentype &resvar = argvariables("&",0)("&",nn);

                        (*getsetExtVar)(resvar,srcvar,ii);
                    }

                    else if ( currcommand(zeroint()) == "-fru" ) { randfill(argvariables("&",0)("&",safeatoi(currcommand(1),argvariables)));                  }
                    else if ( currcommand(zeroint()) == "-frn" ) { randnfill(argvariables("&",0)("&",safeatoi(currcommand(1),argvariables)));                 }
                    else if ( currcommand(zeroint()) == "-fri" ) { (argvariables("&",0)("&",safeatoi(currcommand(1),argvariables))).force_int() = svm_rand(); }

                    else if ( currcommand(zeroint()) == "-mc"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmercachesize(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(zeroint()) == "-mcn" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmercachenorm(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(zeroint()) == "-mba" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmlqlist(safeatoi(currcommand(1),argvariables),getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(2),argvariables),svmContext)); }
                    else if ( currcommand(zeroint()) == "-mbw" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmlqweight(safeatoi(currcommand(1),argvariables),safeatog(currcommand(2),argvariables)); }
                    else if ( currcommand(zeroint()) == "-mbA" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removemlqlist(safeatoi(currcommand(1),argvariables));   }

                    else if ( currcommand(zeroint()) == "-msn" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBernDegree(safeatog(currcommand(1),argvariables)); }
                    else if ( currcommand(zeroint()) == "-msw" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBernIndex(safeatog(currcommand(1),argvariables));  }

                    else if ( currcommand(zeroint()) == "-bat" ) { Vector<gentype> tmp; getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbattparam(safeatoVector(tmp,currcommand(1),argvariables));  }
                    else if ( currcommand(zeroint()) == "-bam" ) {                      getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbatttmax(safeatof(currcommand(1),argvariables));            }
                    else if ( currcommand(zeroint()) == "-bac" ) {                      getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbattImax(safeatof(currcommand(1),argvariables));            }
                    else if ( currcommand(zeroint()) == "-bad" ) {                      getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbatttdelta(safeatof(currcommand(1),argvariables));          }
                    else if ( currcommand(zeroint()) == "-bav" ) {                      getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbattVstart(safeatof(currcommand(1),argvariables));          }
                    else if ( currcommand(zeroint()) == "-baT" ) {                      getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbattthetaStart(safeatof(currcommand(1),argvariables));      }

                    else if ( currcommand(zeroint()) == "-fu" )
                    {
                        // Single-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);

                        Vector<double> xxx;

                        safeatoVector(xxx,currcommand(3),argvariables);

                        double rrr;

                        evalTestFn(fnnum,rrr,xxx);

                        argvariables("&",0)("&",nn) = rrr;
                    }

                    else if ( currcommand(zeroint()) == "-fuu" )
                    {
                        // Single-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);

                        Vector<double> xxx;
                        Matrix<double> aaa;

                        safeatoVector(xxx,currcommand(3),argvariables);
                        safeatoMatrix(aaa,currcommand(4),argvariables);

                        double rrr;

                        evalTestFn(fnnum,rrr,xxx,&aaa);

                        argvariables("&",0)("&",nn) = rrr;
                    }

                    else if ( currcommand(zeroint()) == "-ft" )
                    {
                        // Multi-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);
                        int MM = safeatoi(currcommand(3),argvariables);

                        Vector<double> xxx;

                        safeatoVector(xxx,currcommand(4),argvariables);

                        Vector<double> rrr(MM);

                        evalTestFn(fnnum,xxx.size(),MM,rrr,xxx,mooalpha);

                        argvariables("&",0)("&",nn) = rrr;
                    }

                    else if ( currcommand(zeroint()) == "-fat" )
                    {
                        // Set something

                        mooalpha = safeatof(currcommand(1),argvariables);
                    }

                    else if ( currcommand(zeroint()) == "-fVg" )
                    {
                        // Set integer variable

                        int nn = safeatoi(currcommand(1),argvariables);

                        svm_mutex_lock(globargvariableslock);
                        argvariables("&",0)("&",nn) = currcommand(2);
                        svm_mutex_unlock(globargvariableslock);
                    }

                    else if ( currcommand(zeroint()) == "-fWg" )
                    {
                        // Set integer variable

                        int nn = safeatoi(currcommand(1),argvariables);

                        svm_mutex_lock(globargvariableslock);
                        argvariables("&",0)("&",nn) = safeatog(currcommand(2),const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables));
                        argvariables("&",0)("&",nn).finalise();
                        svm_mutex_unlock(globargvariableslock);
                    }

                    else if ( currcommand(zeroint()) == "-fVG" )
                    {
                        // Set integer variable

                        int nn = safeatoi(currcommand(1),argvariables);

                        svm_mutex_lock(globargvariableslock);
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn) = currcommand(2);
                        svm_mutex_unlock(globargvariableslock);
                    }

                    else if ( currcommand(zeroint()) == "-fWG" )
                    {
                        // Set integer variable

                        int nn = safeatoi(currcommand(1),argvariables);

                        svm_mutex_lock(globargvariableslock);
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn) = safeatog(currcommand(2),argvariables);
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn).finalise();
                        svm_mutex_unlock(globargvariableslock);
                    }

                    else if ( currcommand(zeroint()) == "-fuG" )
                    {
                        // Single-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);

                        Vector<double> xxx;

                        safeatoVector(xxx,currcommand(3),argvariables);

                        double rrr;

                        evalTestFn(fnnum,rrr,xxx);

                        svm_mutex_lock(globargvariableslock);
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn) = rrr;
                        svm_mutex_unlock(globargvariableslock);
                    }

                    else if ( currcommand(zeroint()) == "-fuuG" )
                    {
                        // Single-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);

                        Vector<double> xxx;
                        Matrix<double> aaa;

                        safeatoVector(xxx,currcommand(3),argvariables);
                        safeatoMatrix(aaa,currcommand(3),argvariables);

                        double rrr;

                        evalTestFn(fnnum,rrr,xxx,&aaa);

                        svm_mutex_lock(globargvariableslock);
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn) = rrr;
                        svm_mutex_unlock(globargvariableslock);
                    }

                    else if ( currcommand(zeroint()) == "-ftG" )
                    {
                        // Multi-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);
                        int MM = safeatoi(currcommand(3),argvariables);

                        Vector<double> xxx;

                        safeatoVector(xxx,currcommand(4),argvariables);

                        Vector<double> rrr(MM);

                        evalTestFn(fnnum,xxx.size(),MM,rrr,xxx,mooalpha);

                        svm_mutex_lock(globargvariableslock);
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn) = rrr;
                        svm_mutex_unlock(globargvariableslock);
                    }

                    else if ( currcommand(zeroint()) == "-fM" )
                    {
                        // Set macro variable

                        int nn = safeatoi(currcommand(1),argvariables);

                        std::string newmacro = currcommand(2);

                        // Curly brackets already stripped at this point
                        argvariables("&",130)("&",nn).makeString(newmacro);
                    }

                    else if ( currcommand(zeroint()) == "-ac" )
                    {
                        // Set classifier SVM type

                        if      ( currcommand(1) == "svc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setClassifyViaSVM(); }
                        else if ( currcommand(1) == "svr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setClassifyViaSVR(); }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -ac mode."); }
                    }

                    else if ( currcommand(zeroint()) == "-bv"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setVardelta();   }
                    else if ( currcommand(zeroint()) == "-bz"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setZerodelta();  }
                    else if ( currcommand(zeroint()) == "-bgv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setVarmuBias();  }
                    else if ( currcommand(zeroint()) == "-bgz" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setZeromuBias(); }

                    else if ( currcommand(zeroint()) == "-B" )
                    {
                        // Set bias type

                        if      ( currcommand(1) == "f" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setFixedBias(biasdefault); }
                        else if ( currcommand(1) == "v" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setVarBias();              }
                        else if ( currcommand(1) == "p" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setPosBias();              }
                        else if ( currcommand(1) == "n" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setNegBias();              }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -B mode."); }
                    }

                    else if ( currcommand(zeroint()) == "-R" )
                    {
                        // Set empirical risk type

                        if      ( currcommand(1) == "l" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setLinearCost();    getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setusefuzzt(0); }
                        else if ( currcommand(1) == "q" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setQuadraticCost(); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setusefuzzt(0); }
                        else if ( currcommand(1) == "o" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).set1NormCost();     getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setusefuzzt(0); }
                        else if ( currcommand(1) == "g" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setLinearCost();    getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setusefuzzt(1); }
                        else if ( currcommand(1) == "G" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setQuadraticCost(); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setusefuzzt(1); }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -R mode."); }
                    }

                    else if ( currcommand(zeroint()) == "-mls" )
                    {
                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).settsize(safeatoi(currcommand(1),argvariables));
                    }

                    else if ( currcommand(zeroint()) == "-mlR" )
                    {
                        // Set empirical risk type

                        if      ( currcommand(2) == "l" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setregtype(safeatoi(currcommand(1),argvariables),1); }
                        else if ( currcommand(2) == "q" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setregtype(safeatoi(currcommand(1),argvariables),2); }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -mlR mode."); }
                    }

                    else if ( currcommand(zeroint()) == "-sR" )
                    {
                        // Set empirical risk type

                        if      ( currcommand(1) == "l" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setLinRegul();  }
                        else if ( currcommand(1) == "q" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setQuadRegul(); }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -sR mode."); }
                    }

                    else if ( currcommand(zeroint()) == "-T" )
                    {
                        // Set tube type

                        if      ( currcommand(1) == "f" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setFixedTube();  }
                        else if ( currcommand(1) == "s" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setShrinkTube(); }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -T mode."); }
                    }

                    else if ( currcommand(zeroint()) == "-N"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).prealloc(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(zeroint()) == "-br" ) { binaryRelabel = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-bd" ) { singleDrop = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-XT" ) { std::stringstream xstr(currcommand(1)); SparseVector<gentype> xt; streamItIn(xstr,xt,0); xtemplate = xt; }
                }

                time_used endtime = TIMECALL;
                svmsetuptime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << svmsetuptime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Preload steps

            if ( preloadopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing preload operations... ";

                while ( preloadopt.size() )
                {
                    currcommand = preloadopt(zeroint());
                    preloadopt.remove(zeroint());

                    if      ( currcommand(zeroint()) == "-pr"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removeTrainingVector(safeatoi(currcommand(1),argvariables));                               }
                    else if ( currcommand(zeroint()) == "-pcw" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables));   }
                    else if ( currcommand(zeroint()) == "-pcs" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).scaleCweight(safeatof(currcommand(1),argvariables));                                       }
                    else if ( currcommand(zeroint()) == "-pww" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(zeroint()) == "-pws" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).scaleepsweight(safeatof(currcommand(1),argvariables));                                     }
                    else if ( currcommand(zeroint()) == "-pS"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).scale(1/abs2(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).alpha()));       }
                    else if ( currcommand(zeroint()) == "-ps"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).scale(safeatof(currcommand(1),argvariables));                                              }
                    else if ( currcommand(zeroint()) == "-psz" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).sety(safeatoi(currcommand(1),argvariables),safeatog(currcommand(2),argvariables));         }
                    else if ( currcommand(zeroint()) == "-pR"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).reset();                                                                                   }
                    else if ( currcommand(zeroint()) == "-pRR" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).restart();                                                                                 }
                    else if ( currcommand(zeroint()) == "-pro" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removeTrainingVector(0,safeatoi(currcommand(1),argvariables));                             }
                    else if ( currcommand(zeroint()) == "-fic" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).fillCache();                                                                               }

                    else if ( currcommand(zeroint()) == "-pk" )
                    {
                        std::ifstream datfile(currcommand(1).c_str());

                        if ( !datfile.is_open() )
                        {
                            STRTHROW("Unable to open kernel file "+currcommand(1));
                        }

                        Matrix<gentype> kernmat;

                        datfile >> kernmat;

                        datfile.close();

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K2bypass(kernmat);
                    }

                    else if ( currcommand(zeroint()) == "-pdw" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigmaweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables));   }
                    else if ( currcommand(zeroint()) == "-pds" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).scalesigmaweight(safeatof(currcommand(1),argvariables));                                       }

                    else if ( currcommand(zeroint()) == "-prz" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removeNonSupports();                                                               }
                    else if ( currcommand(zeroint()) == "-prm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).trimTrainingSet(safeatoi(currcommand(1),argvariables));                            }
                    else if ( currcommand(zeroint()) == "-psd" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setd(safeatoi(currcommand(1),argvariables),safeatoi(currcommand(2),argvariables)); }
                }

                time_used endtime = TIMECALL;
                preloadtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << preloadtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Load steps

            if ( loadopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing load operations... ";

                double adist  = 1.0;
                double nadist = 1.0;
                int Nantrig = 0;
                int daclass = 0;
                int addVectsToML = 3;
                gentype AGlb(0.0);
                gentype AGub(1.0);

                while ( loadopt.size() )
                {
                    currcommand = loadopt(zeroint());
                    loadopt.remove(zeroint());

                         if ( currcommand(zeroint()) == "-ATa"  ) { adist        = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-ATb"  ) { nadist       = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-ATn"  ) { Nantrig      = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-ATx"  ) { daclass      = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-ATy"  ) { addVectsToML = safeatoi(currcommand(1),argvariables); }

                    else if ( currcommand(zeroint()) == "-Ad"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).settspaceDim(safeatoi(currcommand(1),argvariables));    }
                    else if ( currcommand(zeroint()) == "-AD"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setorder(safeatoi(currcommand(1),argvariables));        }
                    else if ( currcommand(zeroint()) == "-Ac"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addclass(safeatoi(currcommand(1),argvariables));        }
                    else if ( currcommand(zeroint()) == "-As"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setanomalyclass(safeatoi(currcommand(1),argvariables)); }

                    else if ( currcommand(zeroint()) == "-Acz" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addclass(safeatoi(currcommand(1),argvariables),1);                                      }
                    else if ( currcommand(zeroint()) == "-Aca" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).anomalyOn(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(zeroint()) == "-Acd" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).anomalyOff();                                                                           }

                    else if ( currcommand(zeroint()) == "-Aby" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisYUU();                                                                }
                    else if ( currcommand(zeroint()) == "-Abu" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisUUU();                                                                }
                    else if ( currcommand(zeroint()) == "-AeU" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addToBasisUU(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).NbasisUU(),safeatog(currcommand(1),argvariables)); }
                    else if ( currcommand(zeroint()) == "-AeR" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisUU(safeatoi(currcommand(1),argvariables),safeatoi(currcommand(2),argvariables)); }
                    else if ( currcommand(zeroint()) == "-Ar"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removeFromBasisUU(safeatoi(currcommand(1),argvariables));                     }

                    else if ( currcommand(zeroint()) == "-ABy" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisYVV();                                                                }
                    else if ( currcommand(zeroint()) == "-ABu" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisUVV();                                                                }
                    else if ( currcommand(zeroint()) == "-AEU" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addToBasisVV(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).NbasisVV(),safeatog(currcommand(1),argvariables)); }
                    else if ( currcommand(zeroint()) == "-AER" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisVV(safeatoi(currcommand(1),argvariables),safeatoi(currcommand(2),argvariables)); }
                    else if ( currcommand(zeroint()) == "-AR"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removeFromBasisVV(safeatoi(currcommand(1),argvariables));                     }

                    else if ( currcommand(zeroint()) == "-AGl"  ) { AGlb = safeatog(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-AGu"  ) { AGub = safeatog(currcommand(1),argvariables); }

                    else if ( ( currcommand(zeroint()) == "-Ag" ) || ( currcommand(zeroint()) == "-AG" ) || ( currcommand(zeroint()) == "-Agc" ) || ( currcommand(zeroint()) == "-AGc" ) )
                    {
                        int N = safeatoi(currcommand(1),argvariables);
                        int d = safeatoi(currcommand(2),argvariables);
                        gentype f(currcommand(3)); // No processing, deliberately
                        double v = safeatof(currcommand(4),argvariables);
                        double nadd = 0;
                        int gorG = ( currcommand(zeroint()) == "-Ag" ) ? 0 : 1;

                        std::string cfn("1");

                        if ( ( currcommand(zeroint()) == "-Agc" ) || ( currcommand(zeroint()) == "-AGc" ) )
                        {
                            cfn = currcommand(5);
                        }

                        gentype cf(cfn);

                        int jj,kk;
                        Vector<SparseVector<gentype> > xdata(N);
                        Vector<gentype> ydata(N);
                        SparseVector<SparseVector<gentype> > z;
                        Vector<double> Qweight(N);

                        Qweight = 1.0;

                        // Generate x data

                        for ( jj = 0 ; jj < N ; jj++ )
                        {
                            for ( kk = 0 ; kk < d ; kk++ )
                            {
                                if ( !gorG )
                                {
                                    randnfill(xdata("&",jj)("&",kk)); // Gaussian, zero mean, unit variance.
                                }

                                else
                                {
                                    double lb = 0;
                                    double ub = 1;

                                    if ( AGlb.isValVector() )
                                    {
                                        lb = (double) ((const Vector<gentype> &) AGlb)(kk);
                                    }

                                    else
                                    {
                                        lb = (double) AGlb;
                                    }

                                    if ( AGub.isValVector() )
                                    {
                                        ub = (double) ((const Vector<gentype> &) AGub)(kk);
                                    }

                                    else
                                    {
                                        ub = (double) AGub;
                                    }

                                    randfill(xdata("&",jj)("&",kk)); // Uniform 0,1

                                    xdata("&",jj)("&",kk) *= ub-lb; // Uniform 0,ub-lb
                                    xdata("&",jj)("&",kk) += lb;    // Uniform lb,ub
                                }
                            }
                        
                            z("&",zeroint()) = xdata(jj);

                            randnfill(nadd);

                            ydata("&",jj) = f(z) + (nadd*v);

                            if ( ( (int) cf(z) ) != 1 )
                            {
                                jj--;
                            }
                        }

                        addtemptox(xdata,xtemplate);

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addTrainingVector(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N(),ydata,xdata,Qweight,Qweight);
                    }

                    else if ( currcommand(zeroint()) == "-Aq" )
                    {
                        int nbad = safeatoi(currcommand(1),argvariables);

                        double nmean = safeatof(currcommand(2),argvariables);
                        double nvar  = safeatof(currcommand(3),argvariables);

                        double nvadd;

                        int jj,kk;
                        int xdim = (getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).indKey())(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).indKey().size()-1)+1;

                        for ( jj = 0 ; jj < getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() ; jj++ )
                        {
                            SparseVector<gentype> xj = (getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).x())(jj);

                            for ( kk = 0 ; kk < nbad ; kk++ )
                            {
                                randnfill(nvadd);

                                xj("[]",xdim+kk) = nmean+(nvadd*nvar);
                            }

                            getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setx(jj,xj);
                        }
                    }

                    else if ( ( (currcommand(zeroint())).substr(0,3) == "-AA" ) || ( (currcommand(zeroint())).substr(0,3) == "-AN" ) )
                    {
                        std::string subcom = (currcommand(zeroint())).substr(3,((currcommand(zeroint())).length())-3); // Contains suffixes only
                        int isANtype = ( (currcommand(zeroint())).substr(0,3) == "-AN" );                      // Set if i j {k} suffixes present
                        int fileargpos = isANtype ? 4 : 1;                                             // position of filename/number
                        int setibase = 1;                                                              // set if ibase (k) present

                        int reverse = 0;              // set 1 if -AAe used.
                        int ignoreStart = 0;          // number to ignore at start
                        int imax = -1;                // max number to add, or -1 if no limit
                        int ibase = -1;               // where to start adding points, or -1 if end.
                        int uselinesvector = 0;       // if 1 then use linesread vector
                        int israw = 0;                // set if output is to be saved in raw format (not used here)
                        int startpoint = 0;           // set if reoptimisation should start clean-slate (not used here)
                        int coercetosingle = 0;       // if 1 then class label / target is read but disgarded and
                        int coercefromsingle = 0;     // if 1 then class label / target is given and file is assumed unlabelled
                        gentype fromsingletarget;     // see above
                        std::string trainfile;        // name of training file
                        Vector<int> linesread;        // vector containing lines to be read (if uselinesvector is set)

                        argvariables("&",1)("&",11).makeString(trainfile);

                        xlateDataSourceSuffixes(isANtype,fileargpos,setibase,currcommand,subcom,argvariables,filevariables,reverse,ignoreStart,imax,ibase,uselinesvector,israw,startpoint,coercetosingle,coercefromsingle,fromsingletarget,trainfile,linesread);

                        if ( logfile.length() == 0 )
                        {
                            logfile = logfile+trainfile; // This will be logfile+filenum when uselinesvector is set
                            argvariables("&",1)("&",12).makeString(logfile);
                        }

                        if ( uselinesvector )
                        {
                            std::string indexfilename = logfile+".index."+((currcommand(zeroint())).substr(1,((currcommand(zeroint())).length())-1));

                            writeLog(linesread,indexfilename,getsetExtVar);
                        }

                        int pointsadded = addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,trainfile,reverse,ignoreStart,imax,ibase,coercetosingle,coercefromsingle,fromsingletarget,binaryRelabel,singleDrop,uselinesvector,linesread);

                        errstream() << "Added " << pointsadded << " training vectors/";
                    }

                    else if ( ( (currcommand(zeroint())).substr(0,4) == "-ATA" ) || ( (currcommand(zeroint())).substr(0,4) == "-ATN" ) )
                    {
                        std::string subcom = (currcommand(zeroint())).substr(4,((currcommand(zeroint())).length())-4); // Contains suffixes only
                        int isANtype = ( (currcommand(zeroint())).substr(0,4) == "-ATN" );                     // Set if i j {k} suffixes present
                        int fileargpos = isANtype ? 4 : 1;                                             // position of filename/number
                        int setibase = 1;                                                              // set if ibase (k) present

                        int reverse = 0;              // set 1 if -AAe used.
                        int ignoreStart = 0;          // number to ignore at start
                        int imax = -1;                // max number to add, or -1 if no limit
                        int ibase = -1;               // where to start adding points, or -1 if end.
                        int uselinesvector = 0;       // if 1 then use linesread vector
                        int israw = 0;                // set if output is to be saved in raw format (not used here)
                        int startpoint = 0;           // set if reoptimisation should start clean-slate (not used here)
                        int coercetosingle = 0;       // if 1 then class label / target is read but disgarded and
                        int coercefromsingle = 0;     // if 1 then class label / target is given and file is assumed unlabelled
                        gentype fromsingletarget;     // see above
                        std::string trainfile;        // name of training file
                        Vector<int> linesread;        // vector containing lines to be read (if uselinesvector is set)

                        argvariables("&",1)("&",11).makeString(trainfile);

                        xlateDataSourceSuffixes(isANtype,fileargpos,setibase,currcommand,subcom,argvariables,filevariables,reverse,ignoreStart,imax,ibase,uselinesvector,israw,startpoint,coercetosingle,coercefromsingle,fromsingletarget,trainfile,linesread);

                        if ( logfile.length() == 0 )
                        {
                            logfile = logfile+trainfile; // This will be logfile+filenum when uselinesvector is set
                            argvariables("&",1)("&",12).makeString(logfile);
                        }

                        if ( uselinesvector )
                        {
                            std::string indexfilename = logfile+".index."+((currcommand(zeroint())).substr(1,((currcommand(zeroint())).length())-1));

                            writeLog(linesread,indexfilename,getsetExtVar);
                        }

                        int anomaliesRelabelled = 0;

                        Vector<SparseVector<gentype> > xxx;
                        Vector<gentype> yyy;

                        Vector<gentype> zassign;
                        Vector<int> trigVect;
                        Vector<int> anomVect;
                        Vector<int> addVect;

                        loadFileForHillClimb(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,trainfile,reverse,ignoreStart,imax,coercetosingle,coercefromsingle,fromsingletarget,binaryRelabel,singleDrop,uselinesvector,linesread,xxx,yyy);
                        anomaliesRelabelled = anAnomalyCreate(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xxx,zassign,trigVect,anomVect,addVect,Nantrig,adist,nadist,daclass,addVectsToML,0);

                        errstream() << "Added " << addVect.size() << " out of " << xxx.size() << " training vectors (" << anomaliesRelabelled << " relabels, " << anomVect.size() << " ignored)/";
                    }

                    else if ( currcommand(zeroint()) == "-AU" )
                    {
                        std::stringstream dstr(currcommand(1));
                        std::stringstream xstr(currcommand(2));

                        Vector<gentype> dz(1);
                        Vector<SparseVector<gentype> > x(1);
                        gentype temp;

                        streamItIn(xstr,x("&",0),0);
                        dz("&",0) = safeatog(currcommand(1),argvariables);

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,dz,-1,0,0,temp);
                    }

                    else if ( currcommand(zeroint()) == "-AY" )
                    {
                        Vector<gentype> dz(1);
                        Vector<SparseVector<gentype> > x(1);
                        gentype temp;

                        dz("&",0) = safeatog(currcommand(1),argvariables);
                        x("&",0)  = safeatog(currcommand(2),argvariables).cast_vector(1);

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,dz,-1,0,0,temp);
                    }

                    else if ( currcommand(zeroint()) == "-AZ" )
                    {
                        Vector<gentype> dz(1);
                        Vector<SparseVector<gentype> > x(1);
                        gentype temp;
                        int nInd = safeatoi(currcommand(3),argvariables);

                        if ( nInd < 0 )
                        {
                            STRTHROW("Negative SVM index "+currcommand(2)+" in -AZ");
                        }

                        dz("&",0) = safeatog(currcommand(1),argvariables);
                        x("&",0)  = safeatog(currcommand(2),argvariables).cast_vector(1);

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext),xtemplate,x,dz,-1,0,0,temp);
                    }

                    else if ( currcommand(zeroint()) == "-AV" )
                    {
                        std::stringstream dstr(currcommand(1));
                        std::stringstream xstr(currcommand(2));

                        Vector<gentype> dz;
                        Vector<SparseVector<gentype> > x;
                        gentype temp;

                        dstr >> dz;
                        streamItIn(xstr,x,0);

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,dz,-1,0,0,temp);
                    }

                    else if ( currcommand(zeroint()) == "-AVv" )
                    {
                        gentype dd = safeatog(currcommand(1),argvariables);
                        gentype xx = safeatog(currcommand(2),argvariables);
                        gentype temp;

                        Vector<SparseVector<gentype> > x(xx.size());

                        int ij;

                        const Vector<gentype> &ghgh = (const Vector<gentype> &) xx;

                        for ( ij = 0 ; ij < xx.size() ; ij++ )
                        {
                             x("&",ij) = ghgh(ij);
                        }

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,(const Vector<gentype> &) dd,-1,0,0,temp);
                    }

                    else if ( currcommand(zeroint()) == "-AVV" )
                    {
                        gentype dd = safeatog(currcommand(1),argvariables);
                        gentype xx = safeatog(currcommand(2),argvariables);
                        gentype ss = safeatog(currcommand(3),argvariables);
                        gentype temp;

                        Vector<SparseVector<gentype> > x(xx.size());

                        int ij;

                        const Vector<gentype> &ghgh = (const Vector<gentype> &) xx;

                        for ( ij = 0 ; ij < xx.size() ; ij++ )
                        {
                             x("&",ij) = (const Vector<gentype> &) ghgh(ij);
                        }

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,(const Vector<gentype> &) dd,(const Vector<gentype> &) ss,-1,0,0,temp);
                    }

                    else if ( currcommand(zeroint()) == "-AW" )
                    {
                        Vector<gentype> dz;
                        Vector<SparseVector<gentype> > x;
                        gentype temp;

                        int pointsadded = loadDataFromMatlab(currcommand(2),currcommand(1),x,dz,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).targType(),getsetExtVar);

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,dz,-1,0,0,temp);

                        errstream() << "Added " << pointsadded << " vectors from Matlab/";
                    }

                    else if ( currcommand(zeroint()) == "-AeA" ) 
                    {
                        int pointsadded = addbasisdataUU(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),currcommand(1));

                        errstream() << "Added " << pointsadded << " U basis vectors/";
                    }

                    else if ( currcommand(zeroint()) == "-AEA" ) 
                    {
                        int pointsadded = addbasisdataVV(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),currcommand(1));

                        errstream() << "Added " << pointsadded << " V basis vectors/";
                    }
                }

                time_used endtime = TIMECALL;
                loadtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << loadtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Post-load steps

            if ( postloadopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing post-load operations... ";

                while ( postloadopt.size() )
                {
                    currcommand = postloadopt(zeroint());
                    postloadopt.remove(zeroint());

                    if ( currcommand(zeroint()) == "-Sa" )
                    {
                        std::ifstream datfile(currcommand(1).c_str());

                        argvariables("&",1)("&",15).makeString(currcommand(1));

                        if ( !datfile.is_open() )
                        {
                            STRTHROW("Unable to open file -Sa "+currcommand(1));
                        }

                        Vector<gentype> alpha;
                        datfile >> alpha;
                        datfile.close();
                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setAlpha(alpha);
                    }

                    else if ( currcommand(zeroint()) == "-Sb" )
                    {
                        argvariables("&",1)("&",16).makeString(currcommand(1));

                        std::ifstream datfile(currcommand(1).c_str());

                        if ( !datfile.is_open() )
                        {
                            STRTHROW("Unable to open file -Sb "+currcommand(1));
                        }

                        gentype bias;
                        datfile >> bias;
                        datfile.close();
                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBias(bias);
                    }

                    else if ( currcommand(zeroint()) == "-Snx" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelNone();                                     }
                    else if ( currcommand(zeroint()) == "-Sna" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMeanUnitVariance();                     }
                    else if ( currcommand(zeroint()) == "-Snb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMedianUnitVariance();                   }
                    else if ( currcommand(zeroint()) == "-Snc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelUnitRange();                                }
                    else if ( currcommand(zeroint()) == "-SNa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMeanUnitVariance(0,1);                  }
                    else if ( currcommand(zeroint()) == "-SNb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMedianUnitVariance(0,1);                }
                    else if ( currcommand(zeroint()) == "-SNc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelUnitRange(0,1);                             }
                    else if ( currcommand(zeroint()) == "-SnA" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMeanUnitVariance(1,0);                  }
                    else if ( currcommand(zeroint()) == "-SnB" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMedianUnitVariance(1,0);                }
                    else if ( currcommand(zeroint()) == "-SnC" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelUnitRange(1,0);                             }
                    else if ( currcommand(zeroint()) == "-SNA" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMeanUnitVariance(1,1);                  }
                    else if ( currcommand(zeroint()) == "-SNB" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMedianUnitVariance(1,1);                }
                    else if ( currcommand(zeroint()) == "-SNC" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelUnitRange(1,1);                             }
                    else if ( currcommand(zeroint()) == "-Sra" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).randomise(safeatof(currcommand(1),argvariables));     }

                    else if ( currcommand(zeroint()) == "-Sx"  ) 
                    {
                        ML_Base &model = getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        gentype f = safeatog(currcommand(1),argvariables);

                        Vector<gentype> y(model.y());

                        int i;

                        for ( i = 0 ; i < y.size() ; i++ )
                        {
                            y("&",i) = f.evalyonly(model.y()(i));
                        }

                        model.sety(y);
                    }

                    else if ( currcommand(zeroint()) == "-St"  ) 
                    { 
                        Vector<gentype> xmin,xmax;

                        safeatoVector(xmin,currcommand(1),argvariables);
                        safeatoVector(xmax,currcommand(2),argvariables);

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setSampleMode(1,xmin,xmax,safeatoi(currcommand(3),argvariables));
                    }

                    else if ( currcommand(zeroint()) == "-Snt"  ) 
                    {
                        Vector<gentype> xmin,xmax;

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setSampleMode(0,xmin,xmax,safeatof(currcommand(1),argvariables));
                    }
                }

                time_used endtime = TIMECALL;
                postloadtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << postloadtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Learning steps

            if ( learningopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Setting Learning parameters... ";

                std::string tfilename;

                while ( learningopt.size() )
                {
                    currcommand = learningopt(zeroint());
                    learningopt.remove(zeroint());

                         if ( currcommand(zeroint()) == "-c"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setC(safeatof(currcommand(1),argvariables));                                                                                                    }
                    else if ( currcommand(zeroint()) == "-th"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).settheta(safeatof(currcommand(1),argvariables));                                                                                                }
                    else if ( currcommand(zeroint()) == "-thn" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsimnorm(safeatoi(currcommand(1),argvariables));                                                                                                }
                    else if ( currcommand(zeroint()) == "-c+"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(+1,safeatof(currcommand(1),argvariables));                                                                                            }
                    else if ( currcommand(zeroint()) == "-c-"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(-1,safeatof(currcommand(1),argvariables));                                                                                            }
                    else if ( currcommand(zeroint()) == "-c="  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(2,safeatof(currcommand(1),argvariables));                                                                                             }
                    else if ( currcommand(zeroint()) == "-cd"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables));                                                         }
                    else if ( currcommand(zeroint()) == "-cs"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setC(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).C()*safeatof(currcommand(1),argvariables));                                                              }
                    else if ( currcommand(zeroint()) == "-c+s" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(+1,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Cclass(+1)*safeatof(currcommand(1),argvariables));                                               }
                    else if ( currcommand(zeroint()) == "-c-s" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(-1,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Cclass(-1)*safeatof(currcommand(1),argvariables));                                               }
                    else if ( currcommand(zeroint()) == "-c=s" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(2,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Cclass(2)*safeatof(currcommand(1),argvariables));                                                 }
                    else if ( currcommand(zeroint()) == "-cds" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(safeatoi(currcommand(1),argvariables),getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Cclass(safeatoi(currcommand(1),argvariables))*safeatof(currcommand(2),argvariables));   }
                    else if ( currcommand(zeroint()) == "-j"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(+1,safeatof(currcommand(1),argvariables)); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(-1,1);                                                      }
                    else if ( currcommand(zeroint()) == "-jc"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(+1,safeatof(currcommand(1),argvariables)); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(-1,1);                                                      }
                    else if ( currcommand(zeroint()) == "-dd"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setRejectThreshold(safeatof(currcommand(1),argvariables));                                                                                                  }
                    else if ( currcommand(zeroint()) == "-w"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).seteps(safeatof(currcommand(1),argvariables));                                                                                                  }
                    else if ( currcommand(zeroint()) == "-w+"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(+1,safeatof(currcommand(1),argvariables));                                                                                          }
                    else if ( currcommand(zeroint()) == "-w-"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(-1,safeatof(currcommand(1),argvariables));                                                                                          }
                    else if ( currcommand(zeroint()) == "-w="  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(2,safeatof(currcommand(1),argvariables));                                                                                           }
                    else if ( currcommand(zeroint()) == "-wd"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables));                                                       }
                    else if ( currcommand(zeroint()) == "-ws"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).seteps(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eps()*safeatof(currcommand(1),argvariables));                                                          }
                    else if ( currcommand(zeroint()) == "-w+s" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(+1,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).epsclass(+1)*safeatof(currcommand(1),argvariables));                                           }
                    else if ( currcommand(zeroint()) == "-w-s" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(-1,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).epsclass(-1)*safeatof(currcommand(1),argvariables));                                           }
                    else if ( currcommand(zeroint()) == "-w=s" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(2,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).epsclass(2)*safeatof(currcommand(1),argvariables));                                             }
                    else if ( currcommand(zeroint()) == "-wds" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(safeatoi(currcommand(1),argvariables),getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Cclass(safeatoi(currcommand(1),argvariables))*safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(zeroint()) == "-jw"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(+1,safeatof(currcommand(1),argvariables)); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(-1,1);                                                  }
                    else if ( currcommand(zeroint()) == "-cw"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables));                                                        }
                    else if ( currcommand(zeroint()) == "-ww"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables));                                                      }
                    else if ( currcommand(zeroint()) == "-mvb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbetarank(safeatof(currcommand(1),argvariables));                                                                                             }

                    else if ( currcommand(zeroint()) == "-ds"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigma(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).sigma()*safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(zeroint()) == "-dw"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigmaweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }

                    else if ( currcommand(zeroint()) == "-mlc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setregC(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }

                    else if ( currcommand(zeroint()) == "-Mn"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setNoMonotonicConstraints(); }
                    else if ( currcommand(zeroint()) == "-Mi"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setForcedMonotonicIncreasing(); }
                    else if ( currcommand(zeroint()) == "-Md"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setForcedMonotonicDecreasing(); }
                    else if ( currcommand(zeroint()) == "-Bf"   ) { biasdefault = safeatog(currcommand(1),argvariables); if ( getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isFixedBias() ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setFixedBias(biasdefault); } }
                    else if ( currcommand(zeroint()) == "-nm"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setm(safeatoi(currcommand(1),argvariables));                                                                                                    }
                    else if ( currcommand(zeroint()) == "-Tl"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setnu(safeatof(currcommand(1),argvariables));                                                                                                   }
                    else if ( currcommand(zeroint()) == "-Tq"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setnuQuad(safeatof(currcommand(1),argvariables));                                                                                               }
                    else if ( currcommand(zeroint()) == "-Nl"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setLinBiasForce(-2,safeatof(currcommand(1),argvariables));                                                                                      }
                    else if ( currcommand(zeroint()) == "-sNl"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBiasForce(safeatof(currcommand(1),argvariables));                                                                                      }
                    else if ( currcommand(zeroint()) == "-Nq"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setQuadBiasForce(-2,safeatof(currcommand(1),argvariables));                                                                                     }
                    else if ( currcommand(zeroint()) == "-Nld"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setLinBiasForce(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables));                                                   }
                    else if ( currcommand(zeroint()) == "-Nqd"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setQuadBiasForce(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables));                                                  }
                    else if ( currcommand(zeroint()) == "-mvi"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmaxitermvrank(safeatoi(currcommand(1),argvariables));                                                                                        }
                    else if ( currcommand(zeroint()) == "-mvlr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlrmvrank(safeatof(currcommand(1),argvariables));                                                                                             }
                    else if ( currcommand(zeroint()) == "-mvzt" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setztmvrank(safeatof(currcommand(1),argvariables));                                                                                             }
                    else if ( currcommand(zeroint()) == "-Fi"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmaxiterfuzzt(safeatoi(currcommand(1),argvariables));                                                                                         }
                    else if ( currcommand(zeroint()) == "-Flr"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlrfuzzt(safeatof(currcommand(1),argvariables));                                                                                              }
                    else if ( currcommand(zeroint()) == "-Fzt"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setztfuzzt(safeatof(currcommand(1),argvariables));                                                                                              }
                    else if ( currcommand(zeroint()) == "-Fc"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setcostfnfuzzt(currcommand(1));                                                                                                                 }
                    else if ( currcommand(zeroint()) == "-m"    )
                    {
                        if      ( currcommand(1) == "r" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setKreal();   }
                        else if ( currcommand(1) == "m" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setKunreal(); }
                        else { STRTHROW("Error: -m arguments are {r,m}"); }
                    }

                    else if ( currcommand(zeroint()) == "-blx" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutfn(currcommand(1));                            }
                    else if ( currcommand(zeroint()) == "-bly" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmexcall(currcommand(1));                          }
                    else if ( currcommand(zeroint()) == "-blz" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmexcallid(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(zeroint()) == "-bls" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsyscall(currcommand(1));                          }
                    else if ( currcommand(zeroint()) == "-bfx" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setxfilename(currcommand(1));                        }
                    else if ( currcommand(zeroint()) == "-bfy" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setyfilename(currcommand(1));                        }
                    else if ( currcommand(zeroint()) == "-bfxy") { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setxyfilename(currcommand(1));                       }
                    else if ( currcommand(zeroint()) == "-bfyx") { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setyxfilename(currcommand(1));                       }
                    else if ( currcommand(zeroint()) == "-bfr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setrfilename(currcommand(1));                        }
                    else if ( currcommand(zeroint()) == "-k"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setk(safeatoi(currcommand(1),argvariables));         }
                    else if ( currcommand(zeroint()) == "-K"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setktp(safeatoi(currcommand(1),argvariables));       }
                    else if ( currcommand(zeroint()) == "-d"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigma(safeatof(currcommand(1),argvariables));     }

                    else if ( currcommand(zeroint()) == "-iz"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setzref(safeatof(currcommand(1),argvariables));      }
                    else if ( currcommand(zeroint()) == "-ie"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setehimethod(safeatoi(currcommand(1),argvariables)); }

                    else if ( currcommand(zeroint()) == "-nzs" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setNzs(safeatoi(currcommand(1),argvariables));                                                                                                   }

                    if ( currcommand(zeroint()) == "-vlb"  ) 
                    {
                        SparseVector<double> xlb;
                        std::stringstream xstr(currcommand(1));
                        streamItIn(xstr,xlb,0);

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setzmin(xlb);
                    }

                    if ( currcommand(zeroint()) == "-vub"  ) 
                    {
                        SparseVector<double> xub;
                        std::stringstream xstr(currcommand(1));
                        streamItIn(xstr,xub,0);

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setzmax(xub);
                    }
                }

                time_used endtime = TIMECALL;
                learningtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << learningtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Kernel steps

            if ( kernelopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Setting Kernel parameters... ";

                int kernnum = 0;
                int firstcall = 1;

                int ekernnum = 0;
                int efirstcall = 1;

                while ( kernelopt.size() )
                {
                    currcommand = kernelopt(zeroint());
                    kernelopt.remove(zeroint());

                    if ( currcommand(zeroint())[1] == 'e' )
                    {
                        // Process output kernel

                        std::string currcommandis = "-" + ((currcommand(zeroint())).substr(2));

                        ML_Base &kernML = getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);
                        MercerKernel &theKern = kernML.getUUOutputKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,1,argvariables,ekernnum,efirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        efirstcall = 0;
                    }

                    else
                    {
                        // Process main kernel

                        std::string currcommandis = currcommand(zeroint());

                        ML_Base &kernML = getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);
                        MercerKernel &theKern = kernML.getKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,0,argvariables,kernnum,firstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        firstcall = 0;
                    }
                }

                time_used endtime = TIMECALL;
                kerneltime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << kerneltime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Tuning steps

            if ( tuningopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Tuning SVM parameters... ";

                std::string tfilename;

                while ( tuningopt.size() )
                {
                    currcommand = tuningopt(zeroint());
                    tuningopt.remove(zeroint());

                    if      ( currcommand(zeroint()) == "-cA"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetCNKmean();                                                                                 }
                    else if ( currcommand(zeroint()) == "-bal" ) { balc(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext));                                                                                            }
                    else if ( currcommand(zeroint()) == "-cB"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetCNKmedian();                                                                               }
                    else if ( currcommand(zeroint()) == "-cAN" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetCKmean();                                                                                  }
                    else if ( currcommand(zeroint()) == "-cBN" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetCKmedian();                                                                                }
                    else if ( currcommand(zeroint()) == "-cX"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetCscaled(safeatof(currcommand(1),argvariables));                                            }
                    else if ( currcommand(zeroint()) == "-cua" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetOff();                                                                                     }
                    else if ( currcommand(zeroint()) == "-NlA" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetLinBiasForce(safeatof(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                }

                time_used endtime = TIMECALL;
                tuningtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << tuningtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Grid-search steps

            if ( gridopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Grid-search and direct SVM parameter selection... ";

                std::string tfilename;
                std::string interstring;
                std::string prestring;
                std::string midstring;

                // NB: these are rather large, so don't put them on the stack!

                GridOptions   *xgopts;
                DIRectOptions *xdopts;
                NelderOptions *xnopts;
                BayesOptions  *xbopts;

                MEMNEW(xgopts,GridOptions );
                MEMNEW(xdopts,DIRectOptions);
                MEMNEW(xnopts,NelderOptions);
                MEMNEW(xbopts,BayesOptions );

                GridOptions   &gopts = *xgopts;
                DIRectOptions &dopts = *xdopts;
                NelderOptions &nopts = *xnopts;
                BayesOptions  &bopts = *xbopts;

                gentype xfnis; xfnis.makeNull();

                int bayesModelNum = -1;
                int numOptReps = 1;

                Vector<gentype> defxres;

                // RKHS kernel stuff

                int gphkernnum   = 0;
                int gphfirstcall = 1;

                int gPkernnum   = 0;
                int gPfirstcall = 1;

                while ( gridopt.size() )
                {
                    currcommand = gridopt(zeroint());
                    gridopt.remove(zeroint());

                         if ( currcommand(zeroint()) == "-gy"  ) { bopts.goptssingleobj.maxtraintime  = ( nopts.maxtraintime  = ( gopts.maxtraintime  = ( dopts.maxtraintime  = ( bopts.maxtraintime  = safeatof(currcommand(1),argvariables) ) ) ) ); }
                    else if ( currcommand(zeroint()) == "-gfm" ) { bopts.goptssingleobj.hardmin       = ( nopts.hardmin       = ( gopts.hardmin       = ( dopts.hardmin       = ( bopts.hardmin       = safeatof(currcommand(1),argvariables) ) ) ) ); }
                    else if ( currcommand(zeroint()) == "-gfM" ) { bopts.goptssingleobj.softmin       = ( nopts.softmin       = ( gopts.softmin       = ( dopts.softmin       = ( bopts.softmin       = safeatof(currcommand(1),argvariables) ) ) ) ); }
                    else if ( currcommand(zeroint()) == "-gfu" ) { bopts.goptssingleobj.hardmax       = ( nopts.hardmax       = ( gopts.hardmax       = ( dopts.hardmax       = ( bopts.hardmax       = safeatof(currcommand(1),argvariables) ) ) ) ); }
                    else if ( currcommand(zeroint()) == "-gfU" ) { bopts.goptssingleobj.softmax       = ( nopts.softmax       = ( gopts.softmax       = ( dopts.softmax       = ( bopts.softmax       = safeatof(currcommand(1),argvariables) ) ) ) ); }

                    else if ( currcommand(zeroint()) == "-gBy"  ) { bopts.goptsmultiobj.maxtraintime = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-gBfm" ) { bopts.goptsmultiobj.hardmin      = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-gBfN" ) { bopts.goptsmultiobj.softmin      = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-gBfU" ) { bopts.goptsmultiobj.softmax      = safeatof(currcommand(1),argvariables); }

                    else if ( currcommand(zeroint()) == "-gpr" ) { nopts.randReproject = ( gopts.randReproject = ( dopts.randReproject = ( bopts.randReproject = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(zeroint()) == "-gnp" ) { nopts.isProjection  = ( gopts.isProjection  = ( dopts.isProjection  = ( bopts.isProjection  = 0                                     ) ) ); }
                    else if ( currcommand(zeroint()) == "-gpb" ) { nopts.isProjection  = ( gopts.isProjection  = ( dopts.isProjection  = ( bopts.isProjection  = 3                                     ) ) ); }
                    else if ( currcommand(zeroint()) == "-gpB" ) { nopts.isProjection  = ( gopts.isProjection  = ( dopts.isProjection  = ( bopts.isProjection  = 4                                     ) ) );
                                                                   nopts.bernstart     = ( gopts.bernstart     = ( dopts.bernstart     = ( bopts.bernstart     = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(zeroint()) == "-gph" ) { nopts.isProjection  = ( gopts.isProjection  = ( dopts.isProjection  = ( bopts.isProjection  = 5                                     ) ) ); }
                    else if ( currcommand(zeroint()) == "-gpd" ) { nopts.fnDim         = ( gopts.fnDim         = ( dopts.fnDim         = ( bopts.fnDim         = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(zeroint()) == "-gf"  ) { nopts.useScalarFn   = ( gopts.useScalarFn   = ( dopts.useScalarFn   = ( bopts.useScalarFn   = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(zeroint()) == "-gc"  ) { nopts.includeConst  = ( gopts.includeConst  = ( dopts.includeConst  = ( bopts.includeConst  = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(zeroint()) == "-gC"  ) { nopts.whatConst     = ( gopts.whatConst     = ( dopts.whatConst     = ( bopts.whatConst     = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(zeroint()) == "-gns" ) { nopts.xNsamp        = ( gopts.xNsamp        = ( dopts.xNsamp        = ( bopts.xNsamp        = safeatoi(currcommand(1),argvariables) ) ) ); }

                    else if ( currcommand(zeroint()) == "-gp"  ) 
                    { 
                        nopts.isProjection = 1; nopts.defrandDirtemplateVec.setoutfn(currcommand(1)); 
                        gopts.isProjection = 1; gopts.defrandDirtemplateVec.setoutfn(currcommand(1)); 
                        dopts.isProjection = 1; dopts.defrandDirtemplateVec.setoutfn(currcommand(1)); 
                        bopts.isProjection = 1; bopts.defrandDirtemplateVec.setoutfn(currcommand(1)); 
                    }

                    else if ( currcommand(zeroint()) == "-gP"  )
                    {
                        nopts.isProjection = 2; 
                        gopts.isProjection = 2; 
                        dopts.isProjection = 2; 
                        bopts.isProjection = 2; 

                        int kernnum = 0;

                        Vector<gentype> nkernRealConsts(nopts.defrandDirtemplateFnGP.getKernel().cRealConstants(kernnum));
                        Vector<gentype> gkernRealConsts(gopts.defrandDirtemplateFnGP.getKernel().cRealConstants(kernnum));
                        Vector<gentype> dkernRealConsts(dopts.defrandDirtemplateFnGP.getKernel().cRealConstants(kernnum));
                        Vector<gentype> bkernRealConsts(bopts.defrandDirtemplateFnGP.getKernel().cRealConstants(kernnum));

                        gentype temparg;

                        safeatowhatever(temparg,currcommand(1),argvariables);

                        nkernRealConsts("&",zeroint()) = temparg;
                        gkernRealConsts("&",zeroint()) = temparg;
                        dkernRealConsts("&",zeroint()) = temparg;
                        bkernRealConsts("&",zeroint()) = temparg;

                        nopts.defrandDirtemplateFnGP.getKernel_unsafe().setRealConstants(nkernRealConsts,kernnum);
                        gopts.defrandDirtemplateFnGP.getKernel_unsafe().setRealConstants(gkernRealConsts,kernnum);
                        dopts.defrandDirtemplateFnGP.getKernel_unsafe().setRealConstants(dkernRealConsts,kernnum);
                        bopts.defrandDirtemplateFnGP.getKernel_unsafe().setRealConstants(bkernRealConsts,kernnum);

                        nopts.defrandDirtemplateFnGP.resetKernel(0);
                        gopts.defrandDirtemplateFnGP.resetKernel(0);
                        dopts.defrandDirtemplateFnGP.resetKernel(0);
                        bopts.defrandDirtemplateFnGP.resetKernel(0);
                    }

                    else if ( ( currcommand(zeroint()).substr(0,4) == "-gPk" ) || ( currcommand(zeroint()) == "-gPmtb" ) || ( currcommand(zeroint()) == "-gPbmx" ) )
                    {
                        std::string currcommandis = "-" + ((currcommand(zeroint())).substr(3));

                        ML_Base &kernML = bopts.defrandDirtemplateFnGP;
                        MercerKernel &theKern = kernML.getKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,0,argvariables,gPkernnum,gPfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        nopts.defrandDirtemplateFnGP.getKernel_unsafe() = theKern;
                        gopts.defrandDirtemplateFnGP.getKernel_unsafe() = theKern;
                        dopts.defrandDirtemplateFnGP.getKernel_unsafe() = theKern;

                        gPfirstcall = 0;
                    }

                    else if ( ( currcommand(zeroint()).substr(0,5) == "-gphk" ) || ( currcommand(zeroint()) == "-gphmtb" ) || ( currcommand(zeroint()) == "-gphbmx" ) )
                    {
                        // Process RKHS projection kernel

                        std::string currcommandis = "-" + ((currcommand(zeroint())).substr(4));

                        ML_Base dummyML;

                        processKernel(dummyML,bopts.defrandDirtemplateFnRKHS.kern("&"),currcommandis,currcommand,2,argvariables,gphkernnum,gphfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        nopts.defrandDirtemplateFnRKHS.kern("&") = bopts.defrandDirtemplateFnRKHS.kern();
                        gopts.defrandDirtemplateFnRKHS.kern("&") = bopts.defrandDirtemplateFnRKHS.kern();
                        dopts.defrandDirtemplateFnRKHS.kern("&") = bopts.defrandDirtemplateFnRKHS.kern();

                        gphfirstcall = 0;
                    }

                    else if ( currcommand(zeroint()) == "-g+"  )
                    {
                        Vector<gentype> pterms;

                        safeatoVector(pterms,currcommand(1),argvariables);

                        gentype penterm(0.0);
                        gentype makepos("max([ 0 x ])");

                        if ( pterms.size() )
                        {
                            int ij;

                            for ( ij = 0 ; ij < pterms.size() ; ij++ )
                            {
                                penterm += makepos(pterms(ij));
                            }
                        }

                        errstream() << "Setting p(x) penalty " << penterm << "\n";

                        nopts.penterm = penterm;
                        gopts.penterm = penterm;
                        dopts.penterm = penterm;
                        bopts.penterm = penterm;
                    }







                    else if ( currcommand(zeroint()) == "-ggm" ) { gopts.numZooms = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-ggi" ) { gopts.zoomFact = safeatof(currcommand(1),argvariables); }





                    else if ( currcommand(zeroint()) == "-gdc"  ) { bopts.goptssingleobj.maxits            = ( dopts.maxits            = safeatoi(currcommand(1),argvariables)                                      ); }
                    else if ( currcommand(zeroint()) == "-gdf"  ) { bopts.goptssingleobj.maxevals          = ( dopts.maxevals          = safeatoi(currcommand(1),argvariables)                                      ); }
                    else if ( currcommand(zeroint()) == "-gde"  ) { bopts.goptssingleobj.eps               = ( dopts.eps               = safeatof(currcommand(1),argvariables)                                      ); }
                    else if ( currcommand(zeroint()) == "-gda"  ) { bopts.goptssingleobj.algorithm         = ( dopts.algorithm         = safeatoi(currcommand(1),argvariables) ? DIRECT_ORIGINAL : DIRECT_GABLONSKY ); }
                    else if ( currcommand(zeroint()) == "-gdy"  ) { bopts.goptssingleobj.traintimeoverride = ( dopts.traintimeoverride = safeatof(currcommand(1),argvariables)                                      ); }

                    else if ( currcommand(zeroint()) == "-gBdc" ) { bopts.goptsmultiobj.maxits            = safeatoi(currcommand(1),argvariables);                                      }
                    else if ( currcommand(zeroint()) == "-gBdf" ) { bopts.goptsmultiobj.maxevals          = safeatoi(currcommand(1),argvariables);                                      }
                    else if ( currcommand(zeroint()) == "-gBde" ) { bopts.goptsmultiobj.eps               = safeatof(currcommand(1),argvariables);                                      }
                    else if ( currcommand(zeroint()) == "-gBda" ) { bopts.goptsmultiobj.algorithm         = safeatoi(currcommand(1),argvariables) ? DIRECT_ORIGINAL : DIRECT_GABLONSKY; }
                    else if ( currcommand(zeroint()) == "-gBdy" ) { bopts.goptsmultiobj.traintimeoverride = safeatof(currcommand(1),argvariables);                                      }







                    else if ( currcommand(zeroint()) == "-gNa"  ) { nopts.minf_max = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-gNb"  ) { nopts.ftol_rel = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-gNc"  ) { nopts.ftol_abs = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-gNd"  ) { nopts.xtol_rel = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-gNg"  ) { nopts.xtol_abs = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-gNe"  ) { nopts.maxeval  = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-gNf"  ) { nopts.method   = safeatoi(currcommand(1),argvariables); }







                    else if ( currcommand(zeroint()) == "-gmn"   ) { bopts.sigmuseparate = safeatoi(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmt"   ) { bopts.modeltype     = safeatoi(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmq"   ) { bopts.oracleMode    = safeatoi(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmw"   ) { bopts.fnapprox      = &(getMLref(svmThreadOwner,svmbase,threadInd,( bopts.fnapproxInd    = ( bayesModelNum = safeatoi(currcommand(1),argvariables) ) ),svmContext)); }
                    else if ( currcommand(zeroint()) == "-gmW"   ) { bopts.fnapproxmoo   = &(getMLref(svmThreadOwner,svmbase,threadInd,( bopts.fnapproxmooInd = ( bayesModelNum = safeatoi(currcommand(1),argvariables) ) ),svmContext)); }
                    else if ( currcommand(zeroint()) == "-gmo"   ) { bopts.ismoo         = 1;                                                                                                                  }
                    else if ( currcommand(zeroint()) == "-gms"   ) { bopts.ismoo         = 0;                                                                                                                  }
                    else if ( currcommand(zeroint()) == "-gma"   ) { bopts.default_model_settspaceDim(safeatoi(currcommand(1),argvariables));                                                                  }
                    else if ( currcommand(zeroint()) == "-gmT"   ) { std::stringstream xstr(currcommand(1)); SparseVector<gentype> xt; streamItIn(xstr,xt,0); bopts.xtemplate = xt;                            }
                    else if ( currcommand(zeroint()) == "-gmd"   ) { bopts.default_model_setsigma(safeatof(currcommand(1),argvariables));                                                                      }
                    else if ( currcommand(zeroint()) == "-gmg"   ) { gentype temp = safeatog(currcommand(1),argvariables); bopts.default_model_setkernelg(temp);                                               }
                    else if ( currcommand(zeroint()) == "-gmgg"  ) { Vector<gentype> xxscale; SparseVector<gentype> xscale; xscale = safeatoVector(xxscale,currcommand(1),argvariables); bopts.default_model_setkernelgg(xscale); }
                    else if ( currcommand(zeroint()) == "-gmma"  ) { bopts.tunemu        = safeatoi(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmmb"  ) { bopts.tunesigma     = safeatoi(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmmc"  ) { bopts.tunesrcmod    = safeatoi(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmmd"  ) { bopts.tunediffmod   = safeatoi(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmx"   ) { bopts.tranmeth      = safeatoi(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmxa"  ) { bopts.alpha0        = safeatof(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmxb"  ) { bopts.beta0         = safeatof(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmy"   ) { bopts.kernapprox    = &(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext));                     }
                    else if ( currcommand(zeroint()) == "-gmya"  ) { bopts.kxfnum        = safeatoi(currcommand(1),argvariables);                                                                              }
                    else if ( currcommand(zeroint()) == "-gmyb"  ) { bopts.kxfnorm       = safeatoi(currcommand(1),argvariables);                                                                              }







                    else if ( currcommand(zeroint()) == "-gbH"   ) { bopts.method              = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbs"   ) { bopts.intrinbatch         = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbm"   ) { bopts.intrinbatchmethod   = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbj"   ) { bopts.startpoints         = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gba"   ) { bopts.startseed           = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbb"   ) { bopts.algseed             = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbt"   ) { bopts.totiters            = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbe"   ) { bopts.err                 = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbl"   ) { bopts.stepweight          = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbz"   ) { bopts.ztol                = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbD"   ) { bopts.delta               = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbk"   ) { bopts.nu                  = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbx"   ) { bopts.modD                = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbo"   ) { bopts.a                   = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbB"   ) { bopts.b                   = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbr"   ) { bopts.r                   = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbu"   ) { bopts.p                   = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbv"   ) { bopts.betafn              = currcommand(1);                                                                                          }
                    else if ( currcommand(zeroint()) == "-gbim"  ) { bopts.itcntmethod         = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbpd"  ) { bopts.direcdim            = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbq"   ) { bopts.impmeasu            = &(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext).getIMP()); }
                    else if ( currcommand(zeroint()) == "-gbpp"  ) { bopts.direcpre            = &(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext).getML());  }
                    else if ( currcommand(zeroint()) == "-gbmm"  ) { bopts.direcsubseqpre      = &(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext).getML());  }
                    else if ( currcommand(zeroint()) == "-gbG"   ) { bopts.gridsource          = &(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext));          }
                    else if ( currcommand(zeroint()) == "-gbsp"  ) { bopts.stabpmax            = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbsP"  ) { bopts.stabpmin            = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbsA"  ) { bopts.stabA               = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbsB"  ) { bopts.stabB               = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbsF"  ) { bopts.stabF               = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbsr"  ) { bopts.stabbal             = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbsz"  ) { bopts.stabZeroPt          = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbss"  ) { bopts.stabUseSig          = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbst"  ) { bopts.stabThresh          = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbuu"  ) { bopts.unscentUse          = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbuk"  ) { bopts.unscentK            = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gBbj"  ) { bopts.startpointsmultiobj = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gBbt"  ) { bopts.totitersmultiobj    = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gBbH"  ) { bopts.ehimethodmultiobj   = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(zeroint()) == "-gbpl"  ) { safeatoVector(bopts.direcmin,currcommand(1),argvariables);                                                                           }
                    else if ( currcommand(zeroint()) == "-gbpu"  ) { safeatoVector(bopts.direcmax,currcommand(1),argvariables);                                                                           }
                    else if ( currcommand(zeroint()) == "-gbuS"  ) { safeatoMatrix(bopts.unscentSqrtSigma,currcommand(1),argvariables);                                                                   }

                    else if ( currcommand(zeroint()) == "-gbp"  )
                    {
                        Vector<int> penaltyrefs;

                        safeatoVector(penaltyrefs,currcommand(1),argvariables);

                        (bopts.penalty).resize(penaltyrefs.size());

                        if ( penaltyrefs.size() )
                        {
                            int ij;

                            for ( ij = 0 ; ij < penaltyrefs.size() ; ij++ )
                            {
                                (bopts.penalty)("&",ij) = &(getMLref(svmThreadOwner,svmbase,threadInd,penaltyrefs(ij),svmContext));
                            }
                        }
                    }








                         if ( currcommand(zeroint()) == "-gxs"  ) { std::stringstream xstr(currcommand(1)); streamItIn(xstr,defxres,0); }
                    else if ( currcommand(zeroint()) == "-gtp"  ) { prestring   = currcommand(1);                                       }
                    else if ( currcommand(zeroint()) == "-gtP"  ) { midstring   = currcommand(1);                                       }
                    else if ( currcommand(zeroint()) == "-gbts" ) { interstring = currcommand(1);                                       }
                    else if ( currcommand(zeroint()) == "-gtx"  ) { xfnis       = currcommand(1);                                       }
                    else if ( currcommand(zeroint()) == "-gr"   ) { numOptReps  = safeatoi(currcommand(1),argvariables);                }

                    else if ( ( currcommand(zeroint()) == "-g"  ) ||
                              ( currcommand(zeroint()) == "-gd" ) ||
                              ( currcommand(zeroint()) == "-gN" ) ||
                              ( currcommand(zeroint()) == "-gb" )    )
                    {
                        std::string opttypestring(currcommand(zeroint()));

                        GlobalOptions &xopts = *(   ( currcommand(zeroint()) == "-gd" ) ? &static_cast<GlobalOptions &>(dopts) :
                                                  ( ( currcommand(zeroint()) == "-gb" ) ? &static_cast<GlobalOptions &>(bopts) :
                                                  ( ( currcommand(zeroint()) == "-gN" ) ? &static_cast<NelderOptions &>(nopts) :  &static_cast<GlobalOptions &>(gopts) ) ) );

                        {
                            // 1. Construct vector of arguments
                            // 2. Run through vector and fill in results
                            // 3. Write results to file
                            // 4. Find optimal element of vector
                            // 5. Run set argument

                            int nargst = ((currcommand.size())-3)/5; // total number of arguments

                            if ( nargst )
                            {
                                Vector<std::string> type(nargst);   // type
                                Vector<int> argtype(nargst);        // range (0 lin, 1 log, 2 exp, 3 rand)
                                Vector<int> argvart(nargst);        // variable type (0 int, 1 double)
                                Vector<int> argnums(nargst);        // arg num
                                Vector<int> incrtot(nargst);        // total increments
                                Vector<gentype> startval(nargst);   // start var
                                Vector<gentype> endval(nargst);     // end intvar

                                for ( i = 0 ; i < nargst ; i++ )
                                {
                                    type("&",i)    = currcommand(3+(i*5));
                                    argnums("&",i) = safeatoi(currcommand(4+(i*5)),argvariables);
                                    incrtot("&",i) = safeatoi(currcommand(7+(i*5)),argvariables);

                                    int donesomething = 0; // because fucking visual fucking studio (aka mex) whinges about blocks nested too deeply if I just use standard fucking else fucking if

                                    if ( type(i) == "zb" )
                                    {
                                        // integer linear

                                        argtype("&",i)              = 0;
                                        argvart("&",i)              = 0;
                                        startval("&",i).force_int() = safeatoi(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_int()   = safeatoi(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "zl" )
                                    {
                                        // integer logarithmic

                                        argtype("&",i)              = 1;
                                        argvart("&",i)              = 0;
                                        startval("&",i).force_int() = safeatoi(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_int()   = safeatoi(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "za" )
                                    {
                                        // integer antilogarithmic

                                        argtype("&",i)              = 2;
                                        argvart("&",i)              = 0;
                                        startval("&",i).force_int() = safeatoi(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_int()   = safeatoi(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "zr" )
                                    {
                                        // integer random

                                        argtype("&",i)              = 3;
                                        argvart("&",i)              = 0;
                                        startval("&",i).force_int() = safeatoi(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_int()   = safeatoi(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "zc" )
                                    {
                                        // integer random

                                        argtype("&",i)              = 4;
                                        argvart("&",i)              = 0;
                                        startval("&",i).force_int() = safeatoi(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_int()   = safeatoi(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "fb" )
                                    {
                                        // float linear

                                        argtype("&",i)                 = 0;
                                        argvart("&",i)                 = 1;
                                        startval("&",i).force_double() = safeatof(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_double()   = safeatof(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "fl" )
                                    {
                                        // float logarithmic

                                        argtype("&",i)                 = 1;
                                        argvart("&",i)                 = 1;
                                        startval("&",i).force_double() = safeatof(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_double()   = safeatof(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "fa" )
                                    {
                                        // float antilogarithmic

                                        argtype("&",i)                 = 2;
                                        argvart("&",i)                 = 1;
                                        startval("&",i).force_double() = safeatof(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_double()   = safeatof(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "fr" )
                                    {
                                        // float random

                                        argtype("&",i)                 = 3;
                                        argvart("&",i)                 = 1;
                                        startval("&",i).force_double() = safeatof(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_double()   = safeatof(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "fc" )
                                    {
                                        // float random

                                        argtype("&",i)                 = 4;
                                        argvart("&",i)                 = 1;
                                        startval("&",i).force_double() = safeatof(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_double()   = safeatof(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( !donesomething )
                                    {
                                        STRTHROW("Error: "+type(i)+" is not a known -g type.");
                                    }
                                }

                                Vector<gentype> xres(defxres);
                                Vector<gentype> rawxres(defxres);
                                gentype fres;
                                int ires = 0;
                                int mInd = 0;
                                int muInd = 0;
                                int sigInd = 0;
                                int srcmodInd = 0;
                                int diffmodInd = 0;
                                gentype sres;
                                double hres = 0.0;

                                Vector<Vector<gentype> > allxres;
                                Vector<Vector<gentype> > allrawxres;
                                Vector<gentype> allfres;
                                Vector<gentype> allmres;
                                Vector<int> allires;
                                Vector<gentype> allsres;
                                Vector<double> allhres;
                                Vector<double> sscore;

                                gentype meanfres,varfres;
                                gentype meanires,varires;
                                gentype meantres,vartres;
                                gentype meanTres,varTres;
                                Vector<gentype> meanallfres,varallfres;
                                Vector<gentype> meanallmres,varallmres;

//NB: if you change this you'll also need to change it in globalopt.h!
                                Vector<int> MLnumbers(6); // 0 = ML model (mu), -1 if none
                                                          // 1 = ML model (sigma), -1 if none or same as mu
                                                          // 2 = functional analysis model, -1 if not relevant
                                                          // 3 = random direction model (GPR or distribution)
                                                          // 4 = source model (env-GP,diff-GP)
                                                          // 5 = difference model (diff-GP)

                                MLnumbers("&",zeroint()) = bayesModelNum;
                                MLnumbers("&",1)         = -1;
                                MLnumbers("&",2)         = -1;
                                MLnumbers("&",3)         = -1;
                                MLnumbers("&",4)         = -1;
                                MLnumbers("&",5)         = -1;

                                void *fnarg[18];

                                fnarg[0]  = (void *) &(currcommand("&",1));
                                fnarg[1]  = (void *) &svmContext;
                                fnarg[2]  = (void *) &verblevel;
                                fnarg[3]  = (void *) &finalresult;
                                fnarg[4]  = (void *) &logfile;
                                fnarg[5]  = (void *) &depthin;
                                fnarg[6]  = (void *) &argvariables;
                                fnarg[7]  = (void *) &threadInd;
                                fnarg[8]  = (void *) &globargvariables;
                                fnarg[9]  = (void *) &argnums;
                                fnarg[10] = (void *) getsetExtVar;
                                fnarg[11] = (void *) &svmbase;
                                fnarg[12] = (void *) &svmThreadOwner;
                                fnarg[13] = (void *) &interstring;
                                fnarg[14] = (void *) &xfnis; 
                                fnarg[15] = (void *) &MLnumbers; // IMPORTANT: this number must be fixed!
                                fnarg[16] = (void *) &prestring;
                                fnarg[17] = (void *) &midstring;

                                int dummy = 0;

                                gopts.numPts = incrtot;
                                xopts.MLregfn = gridelmMLreg;

                                xopts.MLdefined = 1;

                                int optres = xopts.optim(nargst,
                                                          xres,
                                                          rawxres,
                                                          fres,
                                                          ires,
                                                          mInd,
                                                          muInd,
                                                          sigInd,
                                                          srcmodInd,
                                                          diffmodInd,
                                                          allxres,
                                                          allrawxres,
                                                          allfres,
                                                          allmres,
                                                          allsres,
                                                          sscore,
                                                          startval,
                                                          endval,
                                                          argtype,
                                                          argvart,
                                                          &gridelmrun,
                                                          (void *) fnarg,
                                                          dummy, 
                                                          numOptReps,
                                                          meanfres,varfres,
                                                          meanires,varires,
                                                          meantres,vartres,
                                                          meanTres,varTres,
                                                          meanallfres,varallfres,
                                                          meanallmres,varallmres);

                                retVector<double> tmpva;
                                retVector<int>    tmpvb;

                                allires = cntintvec(allfres.size(),tmpvb);
                                allhres = zerodoublevec(allfres.size(),tmpva);

                                errstream() << "Optim return code: " << optres << " - ";

                                xfnis.makeNull();

                                if ( opttypestring == "-g" )
                                {
                                    errstream() << " (" << ires << ")\n";
                                }

                                else if ( opttypestring == "-gN" )
                                {
                                    switch ( optres )
                                    {
                                        case 0:    { errstream() << "success\n";                          break; }
                                        case 1:    { errstream() << "success\n";                          break; }
                                        case 2:    { errstream() << "success: stopval reached\n";         break; }
                                        case 3:    { errstream() << "success: ftol reached\n";            break; }
                                        case 4:    { errstream() << "success: xtol reached\n";            break; }
                                        case 5:    { errstream() << "success: max evaluations reached\n"; break; }
                                        case 6:    { errstream() << "success: maxtime exceeded\n";        break; }
                                        case -1:   { errstream() << "generic fail\n";                     break; }
                                        case -2:   { errstream() << "invalid arguments\n";                break; }
                                        case -3:   { errstream() << "out of memory\n";                    break; }
                                        case -4:   { errstream() << "roundoff limited\n";                 break; }
                                        case -5:   { errstream() << "forced stop\n";                      break; }
                                        default:   { errstream() << "Unknown\n";                          break; }
                                    }
                                }

                                else if ( opttypestring == "-gd" )
                                {
                                    switch ( optres )
                                    {
                                        case 0:    { errstream() << "success\n";                         break; }
                                        case 1:    { errstream() << "success: maxfeval exceeded\n";      break; }
                                        case 2:    { errstream() << "success: maxiter exceeded\n";       break; }
                                        case 3:    { errstream() << "success: global found\n";           break; }
                                        case 4:    { errstream() << "success: voltol something\n";       break; }
                                        case 5:    { errstream() << "success: sigmatol something\n";     break; }
                                        case 6:    { errstream() << "success: maxtime exceeded\n";       break; }
                                        case -1:   { errstream() << "invalid bounds\n";                  break; }
                                        case -2:   { errstream() << "maxfeval too big\n";                break; }
                                        case -3:   { errstream() << "init failed\n";                     break; }
                                        case -4:   { errstream() << "sample-points failed\n";            break; }
                                        case -5:   { errstream() << "function sample failed\n";          break; }
                                        case -6:   { errstream() << "hyper-rectangle addition failed\n"; break; }
                                        case -100: { errstream() << "out of memory\n";                   break; }
                                        case -101: { errstream() << "invalid args\n";                    break; }
                                        case -102: { errstream() << "forced stop\n";                     break; }
                                        default:   { errstream() << "Unknown\n";                         break; }
                                    }
                                }

                                else if ( opttypestring == "-gb" )
                                {
                                    errstream() << " (" << ires << ")\n";
                                }

                                // Secondary analysis.

                                Vector<int> optvarind(allxres.size() ? 1 : 0);

                                xopts.analyse(allxres,allmres,allhres,optvarind,1);

                                if ( allfres.size() && ( allfres(zeroint()).size() > 1 ) )
                                {
                                    // Multiobjective optimisation - find the
                                    // Pareto set (xres,fres,ires currently
                                    // ill-defined, so ve must fix zat).
                                    //
                                    // In this case  we need to set xres,
                                    // fres, ires to some representative of
                                    // the complete pareto set.  This is
                                    // arbitrarily chosen to be element
                                    // zero.

                                    xres = allxres(optvarind(zeroint()));
                                    fres = allfres(optvarind(zeroint()));
                                    ires = allires(optvarind(zeroint()));
                                    sres = allsres(optvarind(zeroint()));
                                    hres = allhres(optvarind(zeroint()));
                                }

                                else if ( allfres.size() )
                                {
                                    optvarind.resize(1) = ires;

                                    // Still need to retieve supplementary results!

                                    xres = allxres(optvarind(zeroint())); // Important to do this: xres is non-converted, allxres is
                                    // fres = allfres(optvarind(zeroint()));
                                    // ires = allires(optvarind(zeroint()));
                                    sres = allsres(optvarind(zeroint()));
                                    hres = allhres(optvarind(zeroint()));
                                }

                                else
                                {
                                    optvarind.resize(1) = ires;
                                }

                                if ( optvarind.size() )
                                {
                                    sres = allsres(optvarind(zeroint()));
                                    // Set result (we now know that it is defined)

                                    errstream() << "Setting result " << xres << "\n";

                                    {
                                        //SparseVector<SparseVector<gentype> > gridargvars(argvariables);
                                        SparseVector<SparseVector<gentype> > &gridargvars = argvariables;

                                        // Not necessarily true NiceAssert( xres.size() == argnums.size() );

                                        int i;

                                        if ( xres.size() )
                                        {
                                            for ( i = 0 ; i < xres.size() ; i++ )
                                            {
                                                if ( i < argnums.size() )
                                                {
                                                    gridargvars("&",zeroint())("&",argnums(i)) = xres(i);
                                                }

                                                gridargvars("&",50)("&",i) = xres(i);
                                            }
                                        }

                                        if ( sres.size() )
                                        {
                                            for ( i = 0 ; i < sres.size() ; i++ )
                                            {
                                                gridargvars("&",53)("&",i) = sres(i);
                                            }
                                        }

                                        gridargvars("&",51)("&",zeroint()) = fres;
                                        gridargvars("&",52)("&",zeroint()) = ires;
                                        gridargvars("&",54)("&",zeroint()) = hres;

                                        gridargvars("&",55)("&",zeroint()) = meanfres; gridargvars("&",55)("&",65536) = varfres;
                                        gridargvars("&",57)("&",zeroint()) = meanires; gridargvars("&",57)("&",65536) = varires;
                                        gridargvars("&",58)("&",zeroint()) = meantres; gridargvars("&",58)("&",65536) = vartres;
                                        gridargvars("&",59)("&",zeroint()) = meanTres; gridargvars("&",59)("&",65536) = varTres;

                                        for ( i = 0 ; i < optvarind.size() ; i++ )
                                        {
                                            gridargvars("&",60)("&",i) = allxres(optvarind(i));
                                            gridargvars("&",61)("&",i) = allfres(optvarind(i));
                                            gridargvars("&",62)("&",i) = allires(optvarind(i));
                                            gridargvars("&",63)("&",i) = allsres(optvarind(i));
                                            gridargvars("&",64)("&",i) = allhres(optvarind(i));

                                            gridargvars("&",65)("&",i) = meanallfres(optvarind(i)); gridargvars("&",65)("&",65536+i) = varallfres(optvarind(i));
                                            gridargvars("&",66)("&",i) = meanallmres(optvarind(i)); gridargvars("&",66)("&",65536+i) = varallmres(optvarind(i));
                                        }

                                        for ( i = 0 ; i < allxres.size() ; i++ )
                                        {
                                            gridargvars("&",70)("&",i) = allxres(i);
                                            gridargvars("&",71)("&",i) = allfres(i);
                                            gridargvars("&",72)("&",i) = allires(i);
                                            gridargvars("&",73)("&",i) = allsres(i);
                                            gridargvars("&",74)("&",i) = allhres(i);

                                            gridargvars("&",75)("&",i) = meanallfres(i); gridargvars("&",75)("&",65536+i) = varallfres(i);
                                            gridargvars("&",76)("&",i) = meanallmres(i); gridargvars("&",76)("&",65536+i) = varallmres(i);
                                        }

                                        gridargvars("&",80)("&",zeroint()) = allxres;
                                        gridargvars("&",81)("&",zeroint()) = allfres;
                                        gridargvars("&",82)("&",zeroint()) = allires;
                                        gridargvars("&",83)("&",zeroint()) = allsres;
                                        gridargvars("&",84)("&",zeroint()) = allhres;

                                        gridargvars("&",85)("&",zeroint()) = meanallfres; gridargvars("&",75)("&",65536) = varallfres;
                                        gridargvars("&",86)("&",zeroint()) = meanallmres; gridargvars("&",76)("&",65536) = varallmres;

                                        gridargvars("&",90)("&",zeroint()) = muInd;
                                        gridargvars("&",90)("&",1)         = sigInd;
                                        gridargvars("&",90)("&",2)         = mInd;
                                        //gridargvars("&",90)("&",3)         = randDirtemplateInd; // Not relevant here!
                                        //gridargvars("&",90)("&",4)         = itnum; // Not relevant here!
                                        gridargvars("&",90)("&",5)         = srcmodInd;
                                        gridargvars("&",90)("&",6)         = diffmodInd;

//phantomxyzxyz
                                        int locverblevel = verblevel;

                                        std::stringstream *tmpcommand;
                                        MEMNEW(tmpcommand,std::stringstream(currcommand(2)));
                                        awarestream *gridbox;
                                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                                        Stack<awarestream *> *gridcommstack;
                                        MEMNEW(gridcommstack,Stack<awarestream *>);
                                        gridcommstack->push(gridbox);
                                        std::string loclogfile = logfile+".gridfileopt";

                                        gentype dummyres;

                                        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,dummyres,loclogfile);

                                        MEMDEL(gridcommstack);
                                    }

                                    // Save results

                                    finalresult = fres;

                                    if ( verblevel && ( logfile.length() > 0 ) )
                                    {
                                        retVector<Vector<gentype> > tmpva;
                                        retVector<gentype>          tmpvb;
                                        retVector<gentype>          tmpvc;
                                        retVector<double>           tmpvd;
                                        retVector<int>              tmpve;

                                        errstream() << "Writing gridfile... ";

                                        std::string xgridfilenamefull = logfile+".xgrid"; // x values
                                        std::string fgridfilenamefull = logfile+".fgrid"; // results
                                        std::string mgridfilenamefull = logfile+".mgrid"; // results modified
                                        std::string igridfilenamefull = logfile+".igrid"; // indices
                                        std::string sgridfilenamefull = logfile+".sgrid"; // suplementary information
                                        std::string hgridfilenamefull = logfile+".hgrid"; // hypervolume information
                                        std::string qgridfilenamefull = logfile+".qgrid"; // stability scores

                                        std::string meanfgridfilenamefull = logfile+".fgrid.mean"; // mean results
                                        std::string meanmgridfilenamefull = logfile+".mgrid.mean"; // mean results modified

                                        std::string varfgridfilenamefull = logfile+".fgrid.var"; // var results
                                        std::string varmgridfilenamefull = logfile+".mgrid.var"; // var results modified

                                        std::string xygridfilenamefull = logfile+".xygrid"; // y and x ready to train another model
                                        std::string xytgridfilenamefull = logfile+".xytgrid"; // y and x ready to train another model

                                        writeLog(allxres,xgridfilenamefull,getsetExtVar);
                                        writeLog(allfres,fgridfilenamefull,getsetExtVar);
                                        writeLog(allmres,mgridfilenamefull,getsetExtVar);
                                        writeLog(allires,igridfilenamefull,getsetExtVar);
                                        writeLog(allsres,sgridfilenamefull,getsetExtVar);
                                        writeLog(allhres,hgridfilenamefull,getsetExtVar);
                                        writeLog(sscore ,qgridfilenamefull,getsetExtVar);

                                        writeLog(meanallfres,meanfgridfilenamefull,getsetExtVar);
                                        writeLog(meanallmres,meanmgridfilenamefull,getsetExtVar);

                                        writeLog(varallfres,varfgridfilenamefull,getsetExtVar);
                                        writeLog(varallmres,varmgridfilenamefull,getsetExtVar);

                                        writeLog(allfres,allxres,xygridfilenamefull);
                                        writeLog(allfres,allsres,allxres,xytgridfilenamefull);

                                        errstream() << "Writing paretofile... " << optvarind;

                                        std::string xparetofilenamefull = logfile+".xpareto";
                                        std::string fparetofilenamefull = logfile+".fpareto";
                                        std::string mparetofilenamefull = logfile+".mpareto";
                                        std::string iparetofilenamefull = logfile+".ipareto";
                                        std::string sparetofilenamefull = logfile+".spareto";
                                        std::string hparetofilenamefull = logfile+".hpareto";
                                        std::string qparetofilenamefull = logfile+".qpareto";

                                        std::string meanfparetofilenamefull = logfile+".fpareto.mean";
                                        std::string meanmparetofilenamefull = logfile+".mpareto.mean";

                                        std::string varfparetofilenamefull = logfile+".fpareto.var";
                                        std::string varmparetofilenamefull = logfile+".mpareto.var";

                                        std::string xyparetofilenamefull = logfile+".xypareto";
                                        std::string xytparetofilenamefull = logfile+".xytpareto";

                                        writeLog(allxres(optvarind,tmpva),xparetofilenamefull,getsetExtVar);
                                        writeLog(allfres(optvarind,tmpvc),fparetofilenamefull,getsetExtVar);
                                        writeLog(allmres(optvarind,tmpvc),mparetofilenamefull,getsetExtVar);
                                        writeLog(allires(optvarind,tmpve),iparetofilenamefull,getsetExtVar);
                                        writeLog(allsres(optvarind,tmpvc),sparetofilenamefull,getsetExtVar);
                                        writeLog(allhres(optvarind,tmpvd),hparetofilenamefull,getsetExtVar);
                                        writeLog( sscore(optvarind,tmpvd),qparetofilenamefull,getsetExtVar);

                                        writeLog(meanallfres(optvarind,tmpvc),meanfparetofilenamefull,getsetExtVar);
                                        writeLog(meanallmres(optvarind,tmpvc),meanmparetofilenamefull,getsetExtVar);

                                        writeLog(varallfres(optvarind,tmpvc),varfparetofilenamefull,getsetExtVar);
                                        writeLog(varallmres(optvarind,tmpvc),varmparetofilenamefull,getsetExtVar);

                                        writeLog(allfres(optvarind,tmpvc),allxres(optvarind,tmpva),xyparetofilenamefull);
                                        writeLog(allfres(optvarind,tmpvc),allsres(optvarind,tmpvb),allxres(optvarind,tmpva),xytparetofilenamefull);

                                        errstream() << " done.\n";
                                    }
                                }
                            }
                        }
                    }
                }

                time_used endtime = TIMECALL;
                gridtime = TIMEDIFFSEC(endtime,begintime);

                MEMDEL(xgopts);
                MEMDEL(xdopts);
                MEMDEL(xnopts);
                MEMDEL(xbopts);

                errstream() << " done in " << gridtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Transfer kernel learning steps

            if ( xferopt.size() )
            {
                time_used begintime = TIMECALL;

                int maxiter = 20;
                double maxtime = 0;
                double soltol = 0.01;

                errstream() << "Setting kernel transfer selection parameters... ";

                while ( xferopt.size() )
                {
                    currcommand = xferopt(zeroint());
                    xferopt.remove(zeroint());

                         if ( currcommand(zeroint()) == "-xi" ) { maxiter = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-xt" ) { maxtime = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(zeroint()) == "-xs" ) { soltol  = safeatof(currcommand(1),argvariables); }

                    else if ( currcommand(zeroint()) == "-x" ) 
                    { 
                        int n = safeatoi(currcommand(1),argvariables);
                        Vector<int> mln; safeatoVector(mln,currcommand(2),argvariables);

                        SVM_Scalar &core = dynamic_cast<SVM_Scalar &>(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getSVM());
                        Vector<SVM_Generic *> cases(mln.size());

                        for ( i = 0 ; i < mln.size() ; i++ )
                        {
                            cases("&",i) = &(getMLref(svmThreadOwner,svmbase,threadInd,mln(i),svmContext).getSVM());
                        }

                        xferMLtrain(thread_killswitch,core,cases,n,maxiter,maxtime,soltol);
errstream() << "phantomxyz -42: " << core << "\n";
errstream() << "phantomxyz -43: " << getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext) << "\n";
                    }
                }

                time_used endtime = TIMECALL;
                xfertime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << xfertime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Feature selection steps

            if ( featureopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Setting Feature selection parameters... ";

                int useDescent = 0;

                Vector<SparseVector<gentype> > xtest;
                Vector<gentype> ytest;
                int startdirty = 0;
                int traverse = 0;

                while ( featureopt.size() )
                {
                    currcommand = featureopt(zeroint());
                    featureopt.remove(zeroint());

                    if ( currcommand(zeroint())[2] == 'S' ) { useDescent = 1; }
                    else                            { useDescent = 0; }

                    currcommand("&",0)[2] = 's';

                         if ( currcommand(zeroint()) == "-fsd" ) { startdirty = 1;                                   }
                    else if ( currcommand(zeroint()) == "-fsD" ) { startdirty = 0;                                   }
                    else if ( currcommand(zeroint()) == "-fss" ) { traverse = safeatoi(currcommand(1),argvariables); }

                    else
                    {
                        Vector<int> usedfeats;

                        int numreps = 1;
                        int randcross = 0;
                        int numfolds = 0;
                        double bestres = 1e6;

                        std::string subcom = (currcommand(zeroint())).substr(4,((currcommand(zeroint())).length())-4);  // Contains suffixes only
                        int isANtype = ( (currcommand(zeroint())).substr(0,4) == "-fsF" );                              // Set if i j {k} suffixes present
                        int fileargpos = isANtype ? 3 : ( ( (currcommand(zeroint())).substr(0,3) == "-fsf" ) ? 1 : 0 ); // position of filename/number
                        int setibase = 0;                                                                               // set if ibase (k) present

                        int reverse = 0;              // set 1 if -AAe used.
                        int ignoreStart = 0;          // number to ignore at start
                        int imax = -1;                // max number to add, or -1 if no limit
                        int ibase = -1;               // where to start adding points, or -1 if end.
                        int uselinesvector = 0;       // if 1 then use linesread vector
                        int israw = 0;                // set if output is to be saved in raw format
                        int startpoint = 0;           // set if reoptimisation should start clean-slate
                        int coercetosingle = 0;       // if 1 then class label / target is read but disgarded and
                        int coercefromsingle = 0;     // if 1 then class label / target is given and file is assumed unlabelled
                        gentype fromsingletarget;     // see above
                        std::string trainfile;        // name of training file
                        Vector<int> linesread;        // vector containing lines to be read (if uselinesvector is set)

                        xlateDataSourceSuffixes(isANtype,fileargpos,setibase,currcommand,subcom,argvariables,filevariables,reverse,ignoreStart,imax,ibase,uselinesvector,israw,startpoint,coercetosingle,coercefromsingle,fromsingletarget,trainfile,linesread);

                        if ( uselinesvector )
                        {
                            std::string indexfilename = logfile+".index."+((currcommand(zeroint())).substr(1,((currcommand(zeroint())).length())-1));

                            writeLog(linesread,indexfilename,getsetExtVar);
                        }

                        if ( (currcommand(zeroint())).substr(0,4) == "-fsx" )
                        {
                            bestres = optFeatHillClimb(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),-1,0,0,usedfeats,errstream(),useDescent,xtest,ytest,startpoint,traverse,startdirty);

                            errstream() << "Hill climbing best error: " << bestres << "\n";
                        }

                        else if ( (currcommand(zeroint())).substr(0,4) == "-fsr" )
                        {
                            bestres = optFeatHillClimb(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),-2,0,0,usedfeats,errstream(),useDescent,xtest,ytest,startpoint,traverse,startdirty);

                            errstream() << "Hill climbing best error: " << bestres << "\n";
                        }

                        else if ( ( (currcommand(zeroint())).substr(0,4) == "-fsc" ) || ( (currcommand(zeroint())).substr(0,4) == "-fsC" ) )
                        {
                            numreps    = ( (currcommand(zeroint())).substr(0,4) == "-fsC" ) ? safeatoi(currcommand(1),argvariables) : 1;
                            randcross  = ( (currcommand(zeroint())).substr(0,4) == "-fsC" ) ? 1 : 0;
                            numfolds   = ( (currcommand(zeroint())).substr(0,4) == "-fsC" ) ? safeatoi(currcommand(2),argvariables) : safeatoi(currcommand(1),argvariables);

                            bestres = optFeatHillClimb(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),numfolds,numreps,randcross,usedfeats,errstream(),useDescent,xtest,ytest,startpoint,traverse,startdirty);

                            errstream() << "Hill climbing best error: " << bestres << "\n";
                        }

                        else if ( ( (currcommand(zeroint())).substr(0,3) == "-fsf" ) || ( (currcommand(zeroint())).substr(0,3) == "-fsF" ) )
                        {
                            trainfile = currcommand(1);

                            loadFileForHillClimb(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,trainfile,reverse,ignoreStart,imax,coercetosingle,coercefromsingle,fromsingletarget,binaryRelabel,singleDrop,uselinesvector,linesread,xtest,ytest);

                            bestres = optFeatHillClimb(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),-3,1,0,usedfeats,errstream(),useDescent,xtest,ytest,startpoint,traverse,startdirty);

                            errstream() << "Hill climbing best error: " << bestres << "\n";
                        }

                        {
                            std::string featsfilename = logfile+".feats."+((currcommand(zeroint())).substr(1,((currcommand(zeroint())).length())-1));

                            writeLog(usedfeats,featsfilename,getsetExtVar);
                        }

                        argvariables("&",1)("&",1) = bestres;
                    }
                }

                time_used endtime = TIMECALL;
                featuretime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << featuretime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Fuzzy SVM steps

            if ( fuzzyopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Setting fuzzy SVM parameters... ";

                gentype tfuzzfn("1");
                int tdistkernstarted = 0;
                MercerKernel tdistkern;
                double tfuzzf  = 1;
                double tfuzzm  = 1;
                double tfuzznu = 0.5;
                int tkernnum = 0;
                int tkernfirstcall = 1;
                int dotfuzz = 0;

                gentype sfuzzfn("1");
                int sdistkernstarted = 0;
                MercerKernel sdistkern;
                double sfuzzf  = 1;
                double sfuzzm  = 1;
                double sfuzznu = 0.5;
                int skernnum = 0;
                int skernfirstcall = 1;
                int dosfuzz = 0;

                while ( fuzzyopt.size() )
                {
                    currcommand = fuzzyopt(zeroint());
                    fuzzyopt.remove(zeroint());

                    if ( currcommand(zeroint()) == "-fzt" )
                    {
                        tfuzzfn = currcommand(1);

                        if ( !tdistkernstarted )
                        {
                            tdistkernstarted = 1;
                            tdistkern = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel();
                        }

                        dotfuzz = 1;
                    }

                    else if ( currcommand(zeroint()) == "-fzs" )
                    {
                        sfuzzfn = currcommand(1);

                        if ( !sdistkernstarted )
                        {
                            sdistkernstarted = 1;
                            sdistkern = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel();
                        }

                        dosfuzz = 1;
                    }

                    else if ( currcommand(zeroint()) == "-fztf" )
                    {
                        tfuzzf = safeatof(currcommand(1),argvariables);
                    }

                    else if ( currcommand(zeroint()) == "-fztm" )
                    {
                        tfuzzm = safeatof(currcommand(1),argvariables);
                    }

                    else if ( currcommand(zeroint()) == "-fztNlA" )
                    {
                        tfuzznu = safeatof(currcommand(1),argvariables);
                    }

                    else if ( ( currcommand(zeroint()).substr(0,5) == "-fztk" ) || ( currcommand(zeroint()) == "-fztmtb" ) || ( currcommand(zeroint()) == "-fztbmx" ) )
                    {
                        std::string currcommandis = "-" + ((currcommand(zeroint())).substr(4));

                        if ( !tdistkernstarted )
                        {
                            tdistkernstarted = 1;
                            tdistkern = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel();
                        }

                        ML_Base dummyML;

                        processKernel(dummyML,tdistkern,currcommandis,currcommand,2,argvariables,tkernnum,tkernfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        tkernfirstcall = 0;
                    }

                    else if ( currcommand(zeroint()) == "-fzsf" )
                    {
                        sfuzzf = safeatof(currcommand(1),argvariables);
                    }

                    else if ( currcommand(zeroint()) == "-fzsm" )
                    {
                        sfuzzm = safeatof(currcommand(1),argvariables);
                    }

                    else if ( currcommand(zeroint()) == "-fzsNlA" )
                    {
                        sfuzznu = safeatof(currcommand(1),argvariables);
                    }

                    else if ( ( currcommand(zeroint()).substr(0,5) == "-fzsk" ) || ( currcommand(zeroint()) == "-fzsmtb" ) || ( currcommand(zeroint()) == "-fzsbmx" ) )
                    {
                        std::string currcommandis = "-" + ((currcommand(zeroint())).substr(4));

                        if ( !sdistkernstarted )
                        {
                            sdistkernstarted = 1;
                            sdistkern = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel();
                        }

                        ML_Base dummyML;

                        processKernel(dummyML,sdistkern,currcommandis,currcommand,2,argvariables,skernnum,skernfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        skernfirstcall = 0;
                    }
                }

                if ( dotfuzz )
                {
                    if ( calcFuzzML(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),tfuzzfn,argvariables,tdistkern,tfuzzf,tfuzzm,tfuzznu,1) )
                    {
                        STRTHROW("Unknown error during "+currcommand(zeroint())+" operation.");
                    }
                }

                if ( dosfuzz )
                {
                    if ( calcFuzzML(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),sfuzzfn,argvariables,sdistkern,sfuzzf,sfuzzm,sfuzznu,0) )
                    {
                        STRTHROW("Unknown error during "+currcommand(zeroint())+" operation.");
                    }
                }

                time_used endtime = TIMECALL;
                fuzzytime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << fuzzytime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Bootstrap running steps

            if ( bootopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Running bootstraps... ";

                while ( bootopt.size() )
                {
                    currcommand = bootopt(zeroint());
                    bootopt.remove(zeroint());

                    if ( currcommand(zeroint()) == "-boot" )
                    {
                         bootstrapML(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext));
                    }
                }

                time_used endtime = TIMECALL;
                boottime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << boottime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Macro running steps

            if ( macroopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Running macros... ";

                while ( macroopt.size() )
                {
                    currcommand = macroopt(zeroint());
                    macroopt.remove(zeroint());

                    int nn = 0;

                    if ( currcommand(zeroint()) == "-M0"  ) { nn = 0;  goto MMcase; }
                    if ( currcommand(zeroint()) == "-M1"  ) { nn = 1;  goto MMcase; }
                    if ( currcommand(zeroint()) == "-M2"  ) { nn = 2;  goto MMcase; }
                    if ( currcommand(zeroint()) == "-M3"  ) { nn = 3;  goto MMcase; }
                    if ( currcommand(zeroint()) == "-M4"  ) { nn = 4;  goto MMcase; }
                    if ( currcommand(zeroint()) == "-M5"  ) { nn = 5;  goto MMcase; }
                    if ( currcommand(zeroint()) == "-M6"  ) { nn = 6;  goto MMcase; }
                    if ( currcommand(zeroint()) == "-M7"  ) { nn = 7;  goto MMcase; }
                    if ( currcommand(zeroint()) == "-M8"  ) { nn = 8;  goto MMcase; }
                    if ( currcommand(zeroint()) == "-M9"  ) { nn = 9;  goto MMcase; }
                    if ( currcommand(zeroint()) == "-M10" ) { nn = 10; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M11" ) { nn = 11; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M12" ) { nn = 12; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M13" ) { nn = 13; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M14" ) { nn = 14; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M15" ) { nn = 15; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M16" ) { nn = 16; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M17" ) { nn = 17; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M18" ) { nn = 18; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M19" ) { nn = 19; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M20" ) { nn = 20; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M21" ) { nn = 21; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M22" ) { nn = 22; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M23" ) { nn = 23; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M24" ) { nn = 24; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M25" ) { nn = 25; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M26" ) { nn = 26; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M27" ) { nn = 27; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M28" ) { nn = 28; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M29" ) { nn = 29; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M30" ) { nn = 30; goto MMcase; }
                    if ( currcommand(zeroint()) == "-M31" ) { nn = 31; goto MMcase; }

                    if ( currcommand(zeroint()) == "-MM" )
                    {
                        nn = safeatoi(currcommand(1),argvariables);

                        MMcase:

                        // Setup environment and run command

                        if ( nn <= 31 )
                        {
                            argvariables("&",130)("&",nn) = getmacro(nn);
                        }

                        errstream() << "Running command " << argvariables(130)(nn) << "\n";

                        std::stringstream *tmpcommand;
                        MEMNEW(tmpcommand,std::stringstream((const std::string &) argvariables(130)(nn)));
                        awarestream *gridbox;
                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                        Stack<awarestream *> *gridcommstack;
                        MEMNEW(gridcommstack,Stack<awarestream *>);
                        gridcommstack->push(gridbox);

                        runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,returntag);

                        errstream() << "Finished running command " << argvariables(130)(nn) << "\n";
                    }

                    if ( currcommand(zeroint()) == "-MF" )
                    {
                        nn = safeatoi(currcommand(1),argvariables);

                        SparseVector<SparseVector<gentype> > gridargvars(argvariables);

                        // Setup environment and run command

                        if ( nn <= 23 )
                        {
                            argvariables("&",130)("&",nn) = getmacro(nn);
                        }

                        errstream() << "Running command " << argvariables(130)(nn) << "\n";

                        std::stringstream *tmpcommand;
                        MEMNEW(tmpcommand,std::stringstream((const std::string &) argvariables(130)(nn)));
                        awarestream *gridbox;
                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                        Stack<awarestream *> *gridcommstack;
                        MEMNEW(gridcommstack,Stack<awarestream *>);
                        gridcommstack->push(gridbox);
                        std::string loclogfile = logfile+".gridlog";

                        gentype dummyres;
                        int locverblevel = verblevel;

                        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,dummyres,loclogfile);

                        MEMDEL(gridcommstack);

                        errstream() << "Finished running command " << argvariables(130)(nn) << "\n";
                    }
                }

                time_used endtime = TIMECALL;
                macrotime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << macrotime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Optimisation steps

            {
                errstream() << "Setting optimisation parameters and training SVM... ";

                time_used begintime = TIMECALL;

                if ( optimopt.size() )
                {
                    while ( optimopt.size() )
                    {
                        currcommand = optimopt(zeroint());
                        optimopt.remove(zeroint());

                             if ( currcommand(zeroint()) == "-oo"  ) { doopt = 0;                                                                                                            }
                        else if ( currcommand(zeroint()) == "-oO"  ) { doopt = 1;                                                                                                            }
                        else if ( currcommand(zeroint()) == "-oz"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setzerotol(safeatof(currcommand(1),argvariables));       }
                        else if ( currcommand(zeroint()) == "-ot"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmaxitcnt(safeatoi(currcommand(1),argvariables));      }
                        else if ( currcommand(zeroint()) == "-oy"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmaxtraintime(safeatof(currcommand(1),argvariables));  }
                        else if ( currcommand(zeroint()) == "-olr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlr(safeatof(currcommand(1),argvariables));            }
                        else if ( currcommand(zeroint()) == "-oM"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmemsize(safeatoi(currcommand(1),argvariables));       }
                        else if ( currcommand(zeroint()) == "-oge" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setssvtol(safeatof(currcommand(1),argvariables));        }
                        else if ( currcommand(zeroint()) == "-ogm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setssvmom(safeatof(currcommand(1),argvariables));        }
                        else if ( currcommand(zeroint()) == "-ogr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setssvlr(safeatof(currcommand(1),argvariables));         }
                        else if ( currcommand(zeroint()) == "-ogs" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setssvovsc(safeatof(currcommand(1),argvariables));       }
                        else if ( currcommand(zeroint()) == "-ogt" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setssvmaxitcnt(safeatoi(currcommand(1),argvariables));   }
                        else if ( currcommand(zeroint()) == "-ogT" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setssvmaxtime(safeatof(currcommand(1),argvariables));    }
                        else if ( currcommand(zeroint()) == "-ofa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutermethod(safeatoi(currcommand(1),argvariables));   }
                        else if ( currcommand(zeroint()) == "-ofe" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutertol(safeatof(currcommand(1),argvariables));      }
                        else if ( currcommand(zeroint()) == "-ofm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutermom(safeatof(currcommand(1),argvariables));      }
                        else if ( currcommand(zeroint()) == "-ofr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setouterlr(safeatof(currcommand(1),argvariables));       }
                        else if ( currcommand(zeroint()) == "-ofs" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setouterovsc(safeatof(currcommand(1),argvariables));     }
                        else if ( currcommand(zeroint()) == "-oft" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutermaxitcnt(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommand(zeroint()) == "-ofM" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutermaxcache(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommand(zeroint()) == "-ofy" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).fudgeOn();                                               }
                        else if ( currcommand(zeroint()) == "-ofn" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).fudgeOff();                                              }
                        else if ( currcommand(zeroint()) == "-omr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmlmlr(safeatof(currcommand(1),argvariables));         }
                        else if ( currcommand(zeroint()) == "-ome" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setdiffstop(safeatof(currcommand(1),argvariables));      }
                        else if ( currcommand(zeroint()) == "-oms" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlsparse(safeatof(currcommand(1),argvariables));       }

                        else if ( currcommand(zeroint()) == "-oe" )
                        {
                            double etol = 0;

                            if ( currcommand(1) == "A" )
                            {
                                if ( isSVM(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) )
                                {
                                    etol = ( 0.01*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eps()) > 100*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol()) ) ? 0.01*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eps()) : 100*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol());
                                }


                                else
                                {
                                    etol = 100*getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol();
                                }
                            }

                            else
                            {
                                etol = safeatof(currcommand(1),argvariables);
                            }

                            getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOpttol(etol);
                        }

                        else if ( currcommand(zeroint()) == "-om" )
                        {
                            if      ( currcommand(1) == "a" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOptActive(); }
                            else if ( currcommand(1) == "s" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOptSMO();    }
                            else if ( currcommand(1) == "d" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOptD2C();    }
                            else if ( currcommand(1) == "g" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOptGrad();   }

                            else 
                            {
                                STRTHROW("Error: "+currcommand(1)+" is not a valid -om mode."); 
                            }
                        }

                    }
                }

                // Optimise the SVM

                int res = 0;

                if ( doopt && !bgTrainOn )
                {
                    errstream() << "Training SVM... ";

                    res = 0;
                    getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).train(res,thread_killswitch);
                }

                time_used endtime = TIMECALL;
                optimtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << optimtime << " sec (return " << res << ")\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Just in case there is no logfile name yet set it now

            if ( logfile.length() == 0 )
            {
                logfile = DEFAULTLOGFILE;
                argvariables("&",1)("&",12).makeString(logfile);
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Performance evaluation steps

            if ( performopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Testing SVM... ";

                gentype resfilter("var(1,2)");
                int recordres   = 0;
                int recordxvar  = 0;

                while ( performopt.size() )
                {
                    currcommand = performopt(zeroint());
                    performopt.remove(zeroint());

                    if      ( currcommand(zeroint()) == "-tm"    ) { resfilter   = currcommand(1); }
                    else if ( currcommand(zeroint()) == "-tQ"    ) { recordres   = 1;              }
                    else if ( currcommand(zeroint()) == "-tnQ"   ) { recordres   = 0;              }
                    else if ( currcommand(zeroint()) == "-tvar"  ) { recordxvar  = 1;              }

                    else if ( currcommand(zeroint()) == "-tn" ) 
                    {
                        int errsel = safeatoi(currcommand(1),argvariables);

                             if ( errsel == 0 ) { resfilter = "var(1,2)"; }
                        else if ( errsel == 1 ) { resfilter = "1-var(1,38)"; }
                        else if ( errsel == 2 ) { resfilter = "1-var(1,39)"; }
                        else if ( errsel == 3 ) { resfilter = "1-var(1,40)"; }
                        else if ( errsel == 4 ) { resfilter = "1-var(1,41)"; }
                        else if ( errsel == 5 ) { resfilter = "1-var(1,42)"; }
                    }

                    else if ( currcommand(zeroint()) == "-tM" )
                    {
                        resfilter = currcommand(1);
                        finalresult = resfilter(argvariables);
                        finalresult.finalise();
                    }

                    else if ( currcommand(zeroint()) == "-tMd" )
                    {
                        int dim = safeatoi(currcommand(1),argvariables);

                        gentype f = safeatog(currcommand(2),argvariables); // must be a vector with infsize()
                        gentype g = safeatog(currcommand(3),argvariables);

                        NiceAssert( f.isValVector() );
                        NiceAssert( f.infsize() );

                        finalresult = calcL2distsq(f.cast_vector(),g,dim);
                    }

                    else if ( currcommand(zeroint()) == "-tMD" ) 
                    {
                        int dim = safeatoi(currcommand(1),argvariables);

                        gentype f = safeatog(currcommand(2),argvariables);
                        gentype g = safeatog(currcommand(3),argvariables);

                        finalresult = calcL2distsq(f,g,dim);
                    }

                    else if ( currcommand(zeroint()) == "-tMv" ) 
                    {
                        Set<gentype> setres;

                        setres.add(finalresult);
                        setres.add(safeatog(currcommand(1),argvariables));

                        finalresult = setres;
                    }

                    else if ( currcommand(zeroint()) == "-tV"  )
                    {
                        int firstsum = 1;

                        std::stringstream dstr(currcommand(1));
                        std::stringstream xstr(currcommand(2));

                        Vector<gentype> dz;
                        Vector<SparseVector<gentype> > x;

                        dstr >> dz;
                        streamItIn(xstr,x,0);

                        NiceAssert( dz.size() == x.size() );

                        addtemptox(x,xtemplate);

                        testTest(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),x,dz,firstsum,finalresult,resfilter,argvariables,recordres,recordxvar,getsetExtVar);

                        argvariables("&",1)("&",1) = finalresult;
                    }

                    else if ( currcommand(zeroint()) == "-tW"  )
                    {
                        int firstsum = 1;
                        Vector<gentype> dz;
                        Vector<SparseVector<gentype> > x;
                        gentype temp;

                        loadDataFromMatlab(currcommand(2),currcommand(1),x,dz,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).targType(),getsetExtVar);

                        addtemptox(x,xtemplate);

                        testTest(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),x,dz,firstsum,finalresult,resfilter,argvariables,recordres,recordxvar,getsetExtVar);

                        argvariables("&",1)("&",1) = finalresult;
                    }

                    else if ( currcommand(zeroint()) == "-tb" ) 
                    {
                        int firstsum = 1;

                        int minbad = safeatoi(currcommand(1),argvariables);
                        int maxbad = safeatoi(currcommand(2),argvariables);
                        double nmean = safeatof(currcommand(3),argvariables);
                        double nvar  = safeatof(currcommand(4),argvariables);

                        testSparSens(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,minbad,maxbad,nmean,nvar,0,finalresult,resfilter,argvariables,recordres,recordxvar,getsetExtVar);
                    }

                    else if ( ( currcommand(zeroint()) == "-tg" ) || ( currcommand(zeroint()) == "-tG" ) || ( currcommand(zeroint()) == "-tgc" ) || ( currcommand(zeroint()) == "-tGc" ) )
                    {
                        int firstsum = 1;

                        int N = safeatoi(currcommand(1),argvariables);
                        int d = safeatoi(currcommand(2),argvariables);
                        gentype f(currcommand(3)); // No processing, deliberately
                        double v = safeatof(currcommand(4),argvariables);
                        double nadd = 0;
                        int gorG = ( currcommand(zeroint()) == "-Ag" ) ? 0 : 1;

                        std::string cfn("1");

                        if ( ( currcommand(zeroint()) == "-tgc" ) || ( currcommand(zeroint()) == "-tGc" ) )
                        {
                            cfn = currcommand(5);
                        }

                        gentype cf(cfn);


                        int jj,kk;
                        Vector<SparseVector<gentype> > xdata(N);
                        Vector<gentype> ydata(N);
                        SparseVector<SparseVector<gentype> > z;
                        Vector<double> Qweight(N);

                        Qweight = 1.0;

                        errstream() << "Generated test...\n";

                        // Generate x data

                        for ( jj = 0 ; jj < N ; jj++ )
                        {
                            for ( kk = 0 ; kk < d ; kk++ )
                            {
                                if ( !gorG )
                                {
                                    randnfill(xdata("&",jj)("&",kk)); // Gaussian, zero mean, unit variance.
                                }

                                else
                                {
                                    randfill(xdata("&",jj)("&",kk)); // Uniform 0-1
                                }
                            }

                            z("&",zeroint()) = xdata(jj);

                            randnfill(nadd);

                            ydata("&",jj) = f(z) + (nadd*v);

                            if ( ( (int) cf(z) ) != 1 )
                            {
                                jj--;
                            }
                        }

                        addtemptox(xdata,xtemplate);

                        testTest(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xdata,ydata,firstsum,finalresult,resfilter,argvariables,recordres,recordxvar,getsetExtVar);

                        argvariables("&",1)("&",1) = finalresult;
                    }

                    else if ( currcommand(zeroint()) == "-tMpy"   ) { gentype sf = safeatog(currcommand(2),argvariables); Vector<gentype> fv; pyorexeeval(1,0,1,currcommand(1),sf,fv,finalresult); }
                    else if ( currcommand(zeroint()) == "-tMpyv"  ) { gentype fv = safeatog(currcommand(2),argvariables); gentype sf; NiceAssert( fv.isValVector() ); NiceAssert( fv.infsize() ); pyorexeeval(0,1,1,currcommand(1),sf,fv.cast_vector(),finalresult); }
                    else if ( currcommand(zeroint()) == "-tMpyf"  ) { gentype sf = safeatog(currcommand(2),argvariables); Vector<gentype> fv; pyorexeeval(0,0,1,currcommand(1),sf,fv,finalresult); }

                    else if ( currcommand(zeroint()) == "-tMxpy"  ) { gentype sf = safeatog(currcommand(2),argvariables); Vector<gentype> fv; pyorexeeval(1,0,2,currcommand(1),sf,fv,finalresult); }
                    else if ( currcommand(zeroint()) == "-tMxpyv" ) { gentype fv = safeatog(currcommand(2),argvariables); gentype sf; NiceAssert( fv.isValVector() ); NiceAssert( fv.infsize() ); pyorexeeval(0,1,2,currcommand(1),sf,fv.cast_vector(),finalresult); }
                    else if ( currcommand(zeroint()) == "-tMxpyf" ) { gentype sf = safeatog(currcommand(2),argvariables); Vector<gentype> fv; pyorexeeval(0,0,2,currcommand(1),sf,fv,finalresult); }

                    else if ( currcommand(zeroint()) == "-tMypy"  ) { gentype sf = safeatog(currcommand(2),argvariables); Vector<gentype> fv; pyorexeeval(1,0,3,currcommand(1),sf,fv,finalresult); }
                    else if ( currcommand(zeroint()) == "-tMypyv" ) { gentype fv = safeatog(currcommand(2),argvariables); gentype sf; NiceAssert( fv.isValVector() ); NiceAssert( fv.infsize() ); pyorexeeval(0,1,3,currcommand(1),sf,fv.cast_vector(),finalresult); }
                    else if ( currcommand(zeroint()) == "-tMypyf" ) { gentype sf = safeatog(currcommand(2),argvariables); Vector<gentype> fv; pyorexeeval(0,0,3,currcommand(1),sf,fv,finalresult); }

                    else if ( currcommand(zeroint()) == "-tMexe"  ) { gentype sf = safeatog(currcommand(2),argvariables); Vector<gentype> fv; pyorexeeval(1,0,0,currcommand(1),sf,fv,finalresult); }
                    else if ( currcommand(zeroint()) == "-tMexev" ) { gentype fv = safeatog(currcommand(2),argvariables); gentype sf; NiceAssert( fv.isValVector() ); NiceAssert( fv.infsize() ); pyorexeeval(0,1,0,currcommand(1),sf,fv.cast_vector(),finalresult); }
                    else if ( currcommand(zeroint()) == "-tMexef" ) { gentype sf = safeatog(currcommand(2),argvariables); Vector<gentype> fv; pyorexeeval(0,0,0,currcommand(1),sf,fv,finalresult); }

                    else
                    {
                        int firstsum = 1;
                        int numreps = 1;
                        int randcross = 0;
                        int numfolds = 0;

                        std::string subcom = (currcommand(zeroint())).substr(3,((currcommand(zeroint())).length())-3); // Contains suffixes only
                        int isANtype = ( (currcommand(zeroint())).substr(0,3) == "-tF" );                              // Set if i j {k} suffixes present
                        int fileargpos = isANtype ? 3 : ( ( (currcommand(zeroint())).substr(0,3) == "-tf" ) ? 1 : 0 ); // position of filename/number
                        int setibase = 0;                                                                              // set if ibase (k) present

                        int reverse = 0;              // set 1 if -AAe used.
                        int ignoreStart = 0;          // number to ignore at start
                        int imax = -1;                // max number to add, or -1 if no limit
                        int ibase = -1;               // where to start adding points, or -1 if end.
                        int uselinesvector = 0;       // if 1 then use linesread vector
                        int israw = 0;                // set if output is to be saved in raw format
                        int startpoint = 0;           // set if reoptimisation should start clean-slate
                        int coercetosingle = 0;       // if 1 then class label / target is read but disgarded and
                        int coercefromsingle = 0;     // if 1 then class label / target is given and file is assumed unlabelled
                        gentype fromsingletarget;     // see above
                        std::string trainfile;        // name of training file
                        Vector<int> linesread;        // vector containing lines to be read (if uselinesvector is set)

                        xlateDataSourceSuffixes(isANtype,fileargpos,setibase,currcommand,subcom,argvariables,filevariables,reverse,ignoreStart,imax,ibase,uselinesvector,israw,startpoint,coercetosingle,coercefromsingle,fromsingletarget,trainfile,linesread);

                        if ( uselinesvector )
                        {
                            std::string indexfilename = logfile+".index."+((currcommand(zeroint())).substr(1,((currcommand(zeroint())).length())-1));

                            writeLog(linesread,indexfilename,getsetExtVar);
                        }

                        if ( (currcommand(zeroint())).substr(0,3) == "-tx" )
                        {
                            testLOO(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,startpoint,finalresult,resfilter,argvariables,recordres,recordxvar,getsetExtVar);
                        }

                        else if ( (currcommand(zeroint())).substr(0,3) == "-tl" )
                        {
                            testnegloglike(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,finalresult,resfilter,argvariables,getsetExtVar);
                        }

                        else if ( (currcommand(zeroint())).substr(0,3) == "-tr" )
                        {
                            testRecall(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,finalresult,resfilter,argvariables,recordres,recordxvar,getsetExtVar);
                        }

                        else if ( ( (currcommand(zeroint())).substr(0,3) == "-tc" ) || ( (currcommand(zeroint())).substr(0,3) == "-tC" ) )
                        {
                            numreps    = ( (currcommand(zeroint())).substr(0,3) == "-tC" ) ? safeatoi(currcommand(1),argvariables) : 1;
                            randcross  = ( (currcommand(zeroint())).substr(0,3) == "-tC" ) ? 1 : 0;
                            numfolds   = ( (currcommand(zeroint())).substr(0,3) == "-tC" ) ? safeatoi(currcommand(2),argvariables) : safeatoi(currcommand(1),argvariables);

                            testCross(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,numreps,startpoint,randcross,numfolds,finalresult,resfilter,argvariables,recordres,recordxvar,getsetExtVar);
                        }

                        else if ( ( (currcommand(zeroint())).substr(0,3) == "-tf" ) || ( (currcommand(zeroint())).substr(0,3) == "-tF" ) )
                        {
                            testFileVectors(binaryRelabel,singleDrop,logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),trainfile,reverse,ignoreStart,imax,firstsum,coercetosingle,coercefromsingle,fromsingletarget,finalresult,uselinesvector,linesread,resfilter,argvariables,recordres,recordxvar,getsetExtVar,xtemplate);
                        }
                    }

                    outstream() << "Error: " << finalresult << "\n";

                    argvariables("&",1)("&",1) = finalresult;
                }

                time_used endtime = TIMECALL;
                performtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << performtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Reporting steps

            if ( reportopt.size() )
            {
                errstream() << "Reporting and logging... ";

                time_used begintime = TIMECALL;

                while ( reportopt.size() )
                {
                    currcommand = reportopt(zeroint());
                    reportopt.remove(zeroint());

                    if ( currcommand(zeroint()) == "-echo" )
                    {
                        std::stringstream evalx;
                        gentype echoval = safeatog(currcommand(1),argvariables);
                        evalx << echoval << "\n";
                        stopnow = puttylump(evalx.str(),commstack);
                        outstream() << currcommand(1) << " = " << evalx.str();
                    }

                    if ( currcommand(zeroint()) == "-ECHO" )
                    {
                        std::stringstream evalx;
                        gentype echoval = safeatog(currcommand(1),argvariables);
                        echoval.finalise(2); // First globals (leaving possible distributions in place)
                        echoval.finalise(1); // Then randoms that remain
                        echoval.finalise();  // Then just in case
                        evalx << echoval << "\n";
                        stopnow = puttylump(evalx.str(),commstack);
                        outstream() << currcommand(1) << " = " << evalx.str();
                    }

                    if ( currcommand(zeroint()) == "-echosock" )
                    {
                        std::string sockname(currcommand(1));

                        awarestream *echosock = makeUnixSocket(sockname,1,1,0);

                        NiceAssert( echosock );

                        if ( echosock )
                        {
                            std::ostream echosockout(echosock);

                            std::stringstream evalx;
                            gentype echoval = safeatog(currcommand(2),argvariables);
                            evalx << echoval << "\n";
                            stopnow = puttylump(evalx.str(),commstack);
                            echosockout << evalx.str();

                            delUnixSocket(echosock);
                        }
                    }

                    if ( currcommand(zeroint()) == "-ECHOsock" )
                    {
errstream() << "phantomx 0: " << currcommand(1) << "\n";
                        std::string sockname(currcommand(1));

errstream() << "phantomx 1 -" << sockname << "-\n";
                        awarestream *echosock = makeUnixSocket(sockname,1,1,0);

errstream() << "phantomx 2\n";
                        NiceAssert( echosock );

                        if ( echosock )
                        {
errstream() << "phantomx 3\n";
                            std::ostream echosockout(echosock);

errstream() << "phantomx 4: " << currcommand(2) << "\n";
                            std::stringstream evalx;
                            gentype echoval = safeatog(currcommand(2),argvariables);
errstream() << "phantomx 5\n";
                            echoval.finalise(2); // First globals (leaving possible distributions in place)
                            echoval.finalise(1); // Then randoms that remain
                            echoval.finalise();  // Then just in case
errstream() << "phantomx 6\n";
                            evalx << echoval << "\n";
errstream() << "phantomx 7\n";
                            stopnow = puttylump(evalx.str(),commstack);
errstream() << "phantomx 8: " << evalx.str() << "\n";
                            echosockout << evalx.str();
errstream() << "phantomx 9\n";

                            delUnixSocket(echosock);
errstream() << "phantomx 10\n";
                        }
errstream() << "phantomx 11\n";
                    }

                    else if ( currcommand(zeroint()) == "-hU" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh;
                        gentype resg;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).gh(resh,resg,x);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hP" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        double res;

                        streamItIn(xstr,x,0);

                        int p = safeatoi(currcommand(2),argvariables);
                        double mu = safeatof(currcommand(3),argvariables);
                        double B = safeatof(currcommand(4),argvariables);
                        double pnrm = safeatof(currcommand(5),argvariables);
                        int rot = 0;

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).stabProb(res,x,p,pnrm,rot,mu,B);

                        errstream() << "Pr(x) = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hp" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        double res;

                        streamItIn(xstr,x,0);

                        int p = safeatoi(currcommand(2),argvariables);
                        double mu = safeatof(currcommand(3),argvariables);
                        double B = safeatof(currcommand(4),argvariables);
                        double pnrm = 1;
                        int rot = 1;

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).stabProb(res,x,p,pnrm,rot,mu,B);

                        errstream() << "Pr(x) = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hPi" )
                    {
                        double res;

                        int i = safeatoi(currcommand(1),argvariables);
                        int p = safeatoi(currcommand(2),argvariables);
                        double mu = safeatof(currcommand(3),argvariables);
                        double B = safeatof(currcommand(4),argvariables);
                        double pnrm = safeatof(currcommand(5),argvariables);
                        int rot = 0;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).stabProbTrainingVector(res,i,p,pnrm,rot,mu,B);

                        errstream() << "Pr(x) = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hpi" )
                    {
                        double res;

                        int i = safeatoi(currcommand(1),argvariables);
                        int p = safeatoi(currcommand(2),argvariables);
                        double mu = safeatof(currcommand(3),argvariables);
                        double B = safeatof(currcommand(4),argvariables);
                        double pnrm = 1;
                        int rot = 1;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).stabProbTrainingVector(res,i,p,pnrm,rot,mu,B);

                        errstream() << "Pr(x) = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-K0" )
                    {
                        gentype res;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K0(res);

                        errstream() << "K0() = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-K1" )
                    {
                        std::stringstream xastr(currcommand(1));

                        SparseVector<gentype> xa;

                        gentype res;

                        streamItIn(xastr,xa,0);

                        addtemptox(xa,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K1(res,xa);

                        errstream() << "K1(x) = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-phi2" )
                    {
                        std::stringstream xastr(currcommand(1));

                        SparseVector<gentype> xa;

                        Vector<gentype> res;

                        streamItIn(xastr,xa,0);

                        addtemptox(xa,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).phi2(res,xa);

                        errstream() << "phi2(x) = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-K2" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));

                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;

                        gentype res;

                        streamItIn(xastr,xa,0);
                        streamItIn(xbstr,xb,0);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K2(res,xa,xb);

                        errstream() << "K2(x,y) = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-K3" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));
                        std::stringstream xcstr(currcommand(3));

                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;
                        SparseVector<gentype> xc;

                        gentype res;

                        streamItIn(xastr,xa,0);
                        streamItIn(xbstr,xb,0);
                        streamItIn(xcstr,xc,0);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);
                        addtemptox(xc,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K3(res,xa,xb,xc);

                        errstream() << "K3(x,y,u) = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-K4" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));
                        std::stringstream xcstr(currcommand(3));
                        std::stringstream xdstr(currcommand(4));

                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;
                        SparseVector<gentype> xc;
                        SparseVector<gentype> xd;

                        gentype res;

                        streamItIn(xastr,xa,0);
                        streamItIn(xbstr,xb,0);
                        streamItIn(xcstr,xc,0);
                        streamItIn(xdstr,xd,0);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);
                        addtemptox(xc,xtemplate);
                        addtemptox(xd,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K4(res,xa,xb,xc,xd);

                        errstream() << "K4(x,y,u,v) = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-Km" )
                    { 
                        int ii,m = safeatoi(currcommand(1),argvariables);

                        Vector<SparseVector<gentype> > xx(m);

                        for ( ii = 0 ; ii < m ; ii++ )
                        {
                            std::stringstream xstr(currcommand(ii+2));
                            streamItIn(xstr,xx("&",ii),0);
                        }

                        gentype res;

                        addtemptox(xx,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Km(res,xx);

                        errstream() << "Km(...) = " << res << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hY" )
                    {
                         std::stringstream xstr(currcommand(1)); 
                         SparseVector<gentype> x; 
                         gentype resh; 
                         gentype resg;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        x = safeatog(currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).gh(resh,resg,x);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hZ" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh;
                        gentype resg;
                        int nInd = safeatoi(currcommand(2),argvariables);

                        if ( nInd < 0 )
                        {
                            STRTHROW("Negative SVM index "+currcommand(2)+" in -hZ");
                        }

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        x = safeatog(currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext).gh(resh,resg,x);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hV" )
                    {
                        std::stringstream xstr(currcommand(1));
                        Vector<SparseVector<gentype> > x;
                        Vector<gentype> resh;
                        Vector<gentype> resg;
                        int i;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        if ( x.size() )
                        {
                            for ( i = 0 ; i < x.size() ; i++ )
                            {
                                getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).gh(resh("&",i),resg("&",i),x("&",i));
                            }

                            argvariables("&",1)("&",8) = resh;
                            argvariables("&",1)("&",9) = resg;

                            errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                            errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                        }
                    }

                    else if ( currcommand(zeroint()) == "-hW" )
                    {
                        int i = safeatoi(currcommand(1),argvariables);
                        gentype resh;
                        gentype resg;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).ghTrainingVector(resh,resg,i);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hX" )
                    {
                        int i;
                        Vector<gentype> resh(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N());
                        Vector<gentype> resg(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N());

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        if ( getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() )
                        {
                            for ( i = 0 ; i < getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() ; i++ )
                            {
                                getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).ghTrainingVector(resh("&",i),resg("&",i),i);
                            }
                        }

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hUc" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));
                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;
                        gentype resh,dummy;

                        streamItIn(xastr,xa,0);
                        streamItIn(xbstr,xb,0);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).cov(resh,dummy,xa,xb);

                        errstream() << "cov(x,y) = " << resh << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hYc" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));
                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;
                        gentype resh,dummy;

                        xa = safeatog(currcommand(1),argvariables).cast_vector(1);
                        xb = safeatog(currcommand(2),argvariables).cast_vector(1);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).cov(resh,dummy,xa,xb);

                        errstream() << "cov(x,y) = " << resh << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hZc" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));
                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;
                        gentype resh,dummy;
                        int nInd = safeatoi(currcommand(3),argvariables);

                        if ( nInd < 0 )
                        {
                            STRTHROW("Negative SVM index "+currcommand(2)+" in -hZv");
                        }

                        xa = safeatog(currcommand(1),argvariables).cast_vector(1);
                        xb = safeatog(currcommand(1),argvariables).cast_vector(1);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);

                        getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext).cov(resh,dummy,xa,xb);

                        errstream() << "cov(x,y) = " << resh << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hWc" )
                    {
                        int ia = safeatoi(currcommand(1),argvariables);
                        int ib = safeatoi(currcommand(2),argvariables);
                        gentype resh,dummy;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).covTrainingVector(resh,dummy,ia,ib);

                        errstream() << "cov(x,y) = " << resh << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hUv" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh,dummy;

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).var(resh,dummy,x);

                        errstream() << "var(x) = " << resh << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hYv" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh,dummy;

                        x = safeatog(currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).var(resh,dummy,x);

                        errstream() << "var(x) = " << resh << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hZv" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh,dummy;
                        int nInd = safeatoi(currcommand(2),argvariables);

                        if ( nInd < 0 )
                        {
                            STRTHROW("Negative SVM index "+currcommand(2)+" in -hZv");
                        }

                        x = safeatog(currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext).var(resh,dummy,x);

                        errstream() << "var(x) = " << resh << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hWv" )
                    {
                        int i = safeatoi(currcommand(1),argvariables);
                        gentype resh,dummy;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).varTrainingVector(resh,dummy,i);

                        errstream() << "var(x) = " << resh << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hVv" )
                    {
                        std::stringstream xstr(currcommand(1));
                        Vector<SparseVector<gentype> > x;
                        Vector<gentype> resh;
                        int i;
                        gentype dummy;

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        if ( x.size() )
                        {
                            for ( i = 0 ; i < x.size() ; i++ )
                            {
                                getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).var(resh("&",i),dummy,x("&",i));
                            }

                            errstream() << "var(x) = " << resh(i) << "\n";
                        }
                    }

                    else if ( currcommand(zeroint()) == "-hVV" )
                    {
                        std::stringstream xstr(currcommand(1));
                        Vector<SparseVector<gentype> > x;
                        Matrix<gentype> resh;

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).covar(resh,x);

                        errstream() << "covar(x) = " << resh << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hXv" )
                    {
                        int i;
                        gentype dummy;
                        Vector<gentype> resh(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N());

                        if ( getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() )
                        {
                            for ( i = 0 ; i < getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() ; i++ )
                            {
                                getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).varTrainingVector(resh("&",i),dummy,i);
                            }
                        }

                        errstream() << "var(x) = " << resh << "\n";
                    }

                    else if ( currcommand(zeroint()) == "-hM" )
                    {
                        // Save data to matlab

                        const gentype srcvar = safeatog(currcommand(2),argvariables);
                        gentype resvar;

                        resvar.makeString(currcommand(1));

                        (*getsetExtVar)(resvar,srcvar,-2);
                    }

                    else if ( currcommand(zeroint()) == "-hN" )
                    {
                        // Load data from matlab

                        int nn = safeatoi(currcommand(1),argvariables);

                        const gentype srcvar = argvariables("&",0)("&",nn);
                        gentype &resvar = argvariables("&",0)("&",nn);

                        resvar.makeString(currcommand(2));

                        (*getsetExtVar)(resvar,srcvar,-1);
                    }

                    else if ( currcommand(zeroint()) == "-a" )
                    {
                        NiceAssert( isSVM(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) );

                        argvariables("&",1)("&",18).makeString(currcommand(1));

                        writeLog(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).alpha(),currcommand(1),getsetExtVar);
                    }

                    else if ( currcommand(zeroint()) == "-b" )
                    {
                        NiceAssert( isSVM(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) );

                        argvariables("&",1)("&",19).makeString(currcommand(1));

                        Vector<gentype> biasfill(1);

                        biasfill("&",0) = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).bias();

                        writeLog(biasfill,currcommand(1),getsetExtVar);
                    }

                    else if ( currcommand(zeroint()) == "-s" )
                    {
                        std::string svmfile = currcommand(1);

                        argvariables("&",1)("&",17).makeString(svmfile);

                        std::ofstream sfile;
                        sfile.open(svmfile.c_str(),std::ofstream::out);

                        if ( !sfile.is_open() )
                        {
                            STRTHROW("Unable to open svm file "+currcommand(zeroint())+" "+svmfile);
                        }

                        sfile << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);
                        sfile.close();
                    }
                }

                time_used endtime = TIMECALL;
                reporttime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << reporttime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }


            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Write logfile, SVM file, alpha file etc

            // DISABLED
            if ( 0 )
            {
                errstream() << "Logging and saving...\n";

                currcommand = "**writing logfile "+logfile+"**";

                if ( verblevel && ( logfile.length() > 0 ) )
                {
                    errstream() << "Writing logfile... ";

                    std::string logfilenamefull = logfile+".log";
                    std::ofstream loggerfile;
                    loggerfile.open(logfilenamefull.c_str(),std::ofstream::out);

                    if ( !loggerfile.is_open() )
                    {
                        STRTHROW("Unable to open log file "+currcommand(zeroint())+" "+logfilenamefull);
                    }

                    loggerfile << "Logging setup time (sec):          " << loggingtime     << "\n";
                    loggerfile << "Multiple SVM setup time (sec):     " << multiruntime    << "\n";
                    loggerfile << "Setup time (sec):                  " << svmsetuptime    << "\n";
                    loggerfile << "Preload time (sec):                " << preloadtime     << "\n";
                    loggerfile << "Load time (sec):                   " << loadtime        << "\n";
                    loggerfile << "Postload time (sec):               " << postloadtime    << "\n";
                    loggerfile << "Learning setup time (sec):         " << learningtime    << "\n";
                    loggerfile << "Kernel setup time (sec):           " << kerneltime      << "\n";
                    loggerfile << "Tuning time (sec):                 " << tuningtime      << "\n";
                    loggerfile << "Grid time (sec):                   " << gridtime        << "\n";
                    loggerfile << "Kernel transfer time (sec):        " << xfertime        << "\n";
                    loggerfile << "Feature selection time (sec):      " << featuretime     << "\n";
                    loggerfile << "Fuzzy selection time (sec):        " << fuzzytime     << "\n";
                    loggerfile << "Optimisation time (sec):           " << optimtime       << "\n";
                    loggerfile << "Performance evaluation time (sec): " << performtime     << "\n";
                    loggerfile << "Report time (sec):                 " << reporttime      << "\n\n";

                    loggerfile << "Trained: " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isTrained() << "\n\n";

                    loggerfile << "N:       " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N()     << "\n";
                    loggerfile << "NNC(zeroint()):  " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).NNC(zeroint())  << "\n\n";

                    loggerfile << "Target space dimension: " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).tspaceDim()  << "\n";
                    loggerfile << "Classes:                " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).numClasses() << "\n";
                    loggerfile << "SVM type:               " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).type()       << "\n\n";

                    loggerfile << "Class labels:         " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).ClassLabels() << "\n";

                    loggerfile << "Zero tolerance:      " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol()      << "\n";
                    loggerfile << "Optimal tolerance:   " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Opttol()       << "\n";
                    loggerfile << "Max iterations:      " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).maxitcnt()     << "\n";
                    loggerfile << "Max training time:   " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).maxtraintime() << "\n\n";

                    loggerfile << "Underlying scalar:    " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isUnderlyingScalar() << "\n";
                    loggerfile << "Underlying vectorial: " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isUnderlyingVector() << "\n\n";
                    loggerfile << "Underlying anionic:   " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isUnderlyingAnions() << "\n\n";

                    loggerfile << "Kernel dictionary size: " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().size()      << "\n";
                    loggerfile << "Kernel indexed:         " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().isIndex()   << "\n";
                    loggerfile << "Kernel indices:         " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().cIndexes()  << "\n\n";

                    if ( getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().size() )
                    {
                        for ( i = 0 ; i < getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().size() ; i++ )
                        {
                            loggerfile << "Kernel type       (" << i << "): " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().cType(i)          << "\n";
                            loggerfile << "Kernel weight     (" << i << "): " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().cWeight(i)        << "\n";
                            loggerfile << "Normalised        (" << i << "): " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().isNormalised(i)   << "\n";
                            loggerfile << "Integer constants (" << i << "): " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().cIntConstants(i)  << "\n";
                            loggerfile << "Real constants    (" << i << "): " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().cRealConstants(i) << "\n\n";
                        }

                        loggerfile << "\n";
                    }

                    if ( isSVM(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) )
                    {
                        const SVM_Generic &locsvmref = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getSVMconst();

                        loggerfile << "SVM specifics:\n\n";

                        loggerfile << "NS: " << locsvmref.NS()  << "\n";
                        loggerfile << "NZ: " << locsvmref.NZ()  << "\n\n";

                        loggerfile << "Class representation: " << locsvmref.ClassRep()    << "\n\n";

                        loggerfile << "Linear cost:    " << locsvmref.isLinearCost()    << "\n";
                        loggerfile << "Quadratic cost: " << locsvmref.isQuadraticCost() << "\n";
                        loggerfile << "1-norm cost:    " << locsvmref.is1NormCost()     << "\n\n";

                        loggerfile << "Active set optimisation: " << locsvmref.isOptActive() << "\n";
                        loggerfile << "SMO type optimisation:   " << locsvmref.isOptSMO()    << "\n";
                        loggerfile << "D2C type optimisation:   " << locsvmref.isOptD2C()    << "\n\n";
                        loggerfile << "Grad type optimisation:  " << locsvmref.isOptGrad()   << "\n\n";

                        loggerfile << "Margin norm: " << locsvmref.m() << "\n\n";

                        loggerfile << "C:   " << locsvmref.C()   << "\n";
                        loggerfile << "eps: " << locsvmref.eps() << "\n\n";

                        loggerfile << "Kernel cache size:   " << locsvmref.memsize()      << "\n";
                        loggerfile << "Outer learning rate: " << locsvmref.outerlr()      << "\n";
                        loggerfile << "Outer momentum:      " << locsvmref.outermom()     << "\n";
                        loggerfile << "Outer optimal tol.:  " << locsvmref.outertol()     << "\n\n";

                        loggerfile << "Fuzzy weights on:     " << locsvmref.usefuzzt()     << "\n";
                        loggerfile << "Max fuzzy iterations: " << locsvmref.maxiterfuzzt() << "\n";
                        loggerfile << "Fuzzy learning rate:  " << locsvmref.lrfuzzt()      << "\n";
                        loggerfile << "Fuzzy zero tolerance: " << locsvmref.ztfuzzt()      << "\n";
                        loggerfile << "Fuzzy cost function:  " << locsvmref.costfnfuzzt()  << "\n\n";

                        loggerfile << "Fixed tube:     " << locsvmref.isFixedTube()  << "\n";
                        loggerfile << "Shrinking tube: " << locsvmref.isShrinkTube() << "\n\n";

                        loggerfile << "Epsilon restricted positive: " << locsvmref.isRestrictEpsPos() << "\n";
                        loggerfile << "Epsilon restricted negative: " << locsvmref.isRestrictEpsNeg() << "\n\n";

                        loggerfile << "Linear shrinking factor:    " << locsvmref.nu()     << "\n";
                        loggerfile << "Quadratic shrinking factor: " << locsvmref.nuQuad() << "\n\n";

                        loggerfile << "Classify via SVR: " << locsvmref.isClassifyViaSVR() << "\n";
                        loggerfile << "Classify via SVC: " << locsvmref.isClassifyViaSVM() << "\n\n";

                        loggerfile << "Multiclass via 1vsA:               " << locsvmref.is1vsA()    << "\n";
                        loggerfile << "Multiclass via 1vs1:               " << locsvmref.is1vs1()    << "\n";
                        loggerfile << "Multiclass via DAGSVM:             " << locsvmref.isDAGSVM()  << "\n";
                        loggerfile << "Multiclass via MOC:                " << locsvmref.isMOC()     << "\n";
                        loggerfile << "Multiclass via max wins:           " << locsvmref.ismaxwins() << "\n";
                        loggerfile << "Multiclass via recursive division: " << locsvmref.isrecdiv()  << "\n\n";

                        loggerfile << "SVM at-once:             " << locsvmref.isatonce() << "\n";
                        loggerfile << "SVM reduction-to-binary: " << locsvmref.isredbin() << "\n\n";

                        loggerfile << "Anomaly detection on:    " << locsvmref.isanomalyOn()  << "\n";
                        loggerfile << "Anomaly detection off:   " << locsvmref.isanomalyOff() << "\n";
                        loggerfile << "Anomaly detection nu:    " << locsvmref.anomalyNu()    << "\n";
                        loggerfile << "Anomaly detection class: " << locsvmref.anomalyClass() << "\n\n";

                        loggerfile << "Parameter autoset off:                 " << locsvmref.isautosetOff()          << "\n";
                        loggerfile << "Parameter autoset C/N scaling:         " << locsvmref.isautosetCscaled()      << "\n";
                        loggerfile << "Parameter autoset CK mean scaling:     " << locsvmref.isautosetCKmean()       << "\n";
                        loggerfile << "Parameter autoset CK median scaling:   " << locsvmref.isautosetCKmedian()     << "\n";
                        loggerfile << "Parameter autoset CK/N mean scaling:   " << locsvmref.isautosetCNKmean()      << "\n";
                        loggerfile << "Parameter autoset CK/N median scaling: " << locsvmref.isautosetCNKmedian()    << "\n";
                        loggerfile << "Parameter autoset CS++ scaling:        " << locsvmref.isautosetLinBiasForce() << "\n\n";

                        loggerfile << "Parameter autoset C value:  " << locsvmref.autosetCval()  << "\n";
                        loggerfile << "Parameter autoset nu value: " << locsvmref.autosetnuval() << "\n\n";

                        if ( isSVMMultiC(locsvmref) )
                        {
                            int Ndim = ( locsvmref.isatonce() ? locsvmref.numClasses() : locsvmref.tspaceDim() );
                            int d,q,qd;

                            if ( Ndim )
                            {
                                for ( q = 0 ; q < Ndim ; q++ )
                                {
                                    qd = locsvmref.isatonce() ? (locsvmref.ClassLabels())(q) : q;

                                    loggerfile << "NF  (" << qd << ") = " << locsvmref.NF (qd) << "\n";
                                    loggerfile << "NC  (" << qd << ") = " << locsvmref.NC (qd) << "\n";
                                    loggerfile << "NLB (" << qd << ") = " << locsvmref.NLB(qd) << "\n";
                                    loggerfile << "NLF (" << qd << ") = " << locsvmref.NLF(qd) << "\n";
                                    loggerfile << "NUF (" << qd << ") = " << locsvmref.NUF(qd) << "\n";
                                    loggerfile << "NUB (" << qd << ") = " << locsvmref.NUB(qd) << "\n\n";

                                    loggerfile << "Variable bias (" << qd << "):               " << locsvmref.isVarBias(qd)        << "\n";
                                    loggerfile << "Positive bias (" << qd << "):               " << locsvmref.isPosBias(qd)        << "\n";
                                    loggerfile << "Negative bias (" << qd << "):               " << locsvmref.isNegBias(qd)        << "\n";
                                    loggerfile << "Fixed bias (" << qd << "):                  " << locsvmref.isFixedBias(qd)      << "\n";
                                    loggerfile << "Linear bias forcing term (" << qd << "):    " << locsvmref.LinBiasForce(qd)  << "\n";
                                    loggerfile << "Quadratic bias forcing term (" << qd << "): " << locsvmref.QuadBiasForce(qd) << "\n\n";
                                }
                            }

                            if ( locsvmref.numClasses() )
                            {
                                for ( i = 0 ; i < locsvmref.numClasses() ; i++ )
                                {
                                    d = locsvmref.ClassLabels()(i);

                                    loggerfile << "NNC(" << d << "): " << locsvmref.NNC(d)      << "\n";
                                    loggerfile << "C(" << d << "):   " << locsvmref.Cclass(d)   << "\n";
                                    loggerfile << "eps(" << d << "): " << locsvmref.epsclass(d) << "\n\n";
                                }
                            }
                        }

                        else
                        {
                            loggerfile << "NF:      " << locsvmref.NF()    << "\n";
                            loggerfile << "NC:      " << locsvmref.NC()    << "\n";
                            loggerfile << "NLB:     " << locsvmref.NLB()   << "\n";
                            loggerfile << "NLF:     " << locsvmref.NLF()   << "\n";
                            loggerfile << "NUF:     " << locsvmref.NUF()   << "\n";
                            loggerfile << "NUB:     " << locsvmref.NUB()   << "\n\n";

                            loggerfile << "Bias variable:               " << locsvmref.isVarBias()        << "\n";
                            loggerfile << "Bias positive:               " << locsvmref.isPosBias()        << "\n";
                            loggerfile << "Bias negative:               " << locsvmref.isNegBias()        << "\n";
                            loggerfile << "Bias fixed:                  " << locsvmref.isFixedBias()      << "\n";
                            loggerfile << "Linear bias forcing term:    " << locsvmref.LinBiasForce()  << "\n";
                            loggerfile << "Quadratic bias forcing term: " << locsvmref.QuadBiasForce() << "\n\n";

                            if ( isSVMScalar(locsvmref) )
                            {
                                loggerfile << "C+:      " << locsvmref.Cclass(-1)   << "\n";
                                loggerfile << "C-:      " << locsvmref.Cclass(+1)   << "\n";
                                loggerfile << "C=:      " << locsvmref.Cclass(2)    << "\n";
                                loggerfile << "eps+:    " << locsvmref.epsclass(-1) << "\n";
                                loggerfile << "eps-:    " << locsvmref.epsclass(+1) << "\n";
                                loggerfile << "eps=:    " << locsvmref.epsclass(2)  << "\n";
                                loggerfile << "NNC(-1): " << locsvmref.NNC(-1)      << "\n";
                                loggerfile << "NNC(+1): " << locsvmref.NNC(+1)      << "\n";
                                loggerfile << "NNC(2):  " << locsvmref.NNC(2)          << "\n\n";
                            }

                            else
                            {
                                loggerfile << "C+:      " << locsvmref.Cclass(-1)   << "\n";
                                loggerfile << "C-:      " << locsvmref.Cclass(+1)   << "\n";
                                loggerfile << "eps+:    " << locsvmref.epsclass(-1) << "\n";
                                loggerfile << "eps-:    " << locsvmref.epsclass(+1) << "\n";
                                loggerfile << "NNC(-1): " << locsvmref.NNC(-1)      << "\n";
                                loggerfile << "NNC(+1): " << locsvmref.NNC(+1)      << "\n\n";
                            }
                        }
                    }








                    loggerfile.close();

                    errstream() << " done.\n";
                }
            }

            // Acknowledge block boundary if not finished

            if ( !stopnow )
            {
                errstream() << "=======================================================================\n";
            }
          }

          catch ( const char *error )
          {
            errstream() << "Error thrown during " << currcommand << " operation: " << error << "\n";
            retval  = 301;
            stopnow = 1;
          }

          catch ( const std::string &error )
          {
            errstream() << "Error thrown during " << currcommand << " operation: " << error << "\n";
            retval  = 302;
            stopnow = 1;
          }

          //catch ( ... )
          //{
          //  errstream() << "Unknown error during " << currcommand << " operation.\n";
          //  retval  = 303;
          //  stopnow = 1;
          //}
        }
    }

    emptycommstack(commstack);

    depthin--;

    if ( !depthin )
    {
        controlThreadInd = -1; // This signals that the thread is no longer operating
    }

    return retval;
}











































//
// Callback function used by Bayesian optimiser
//

int gridelmMLreg(int ind, ML_Mutable *MLreg, void *arg)
{
//    std::string &commandstr                                            = *((std::string                           *) ((void **) arg)[0] );
    SparseVector<SVMThreadContext *> &svmContext                       = *((SparseVector<SVMThreadContext *>      *) ((void **) arg)[1] );
//    int &verblevel                                                     = *((int                                   *) ((void **) arg)[2] );
//    gentype &finalresult                                               = *((gentype                               *) ((void **) arg)[3] );
//    std::string &logfile                                               = *((std::string                           *) ((void **) arg)[4] );
//    int &depthin                                                       = *((int                                   *) ((void **) arg)[5] );
//    SparseVector<SparseVector<gentype> > &argvariables                 = *((SparseVector<SparseVector<gentype> >  *) ((void **) arg)[6] );
    int &threadInd                                                     = *((int                                   *) ((void **) arg)[7] );
//    svmvolatile SparseVector<SparseVector<gentype> > &globargvariables = *((SparseVector<SparseVector<gentype> >  *) ((void **) arg)[8] );
//    Vector<int> &argnums                                               = *((Vector<int>                           *) ((void **) arg)[9] );
//    int (*getsetExtVar)(gentype &, const gentype &, int)               = ((int (*)(gentype &, const gentype &, int)) ((void **) arg)[10]);
    SparseVector<ML_Mutable *> &svmbase                                = *((SparseVector<ML_Mutable *>            *) ((void **) arg)[11]);
    SparseVector<int> &svmThreadOwner                                  = *((SparseVector<int>                     *) ((void **) arg)[12]);
//    std::string &interstring                                           = *((std::string                           *) ((void **) arg)[13]);
//    gentype &xfnis                                                     = *((gentype                               *) ((void **) arg)[14]);
//    Vector<int> &MLnumbers                                             = *((Vector<int>                           *) ((void **) arg)[15]);
//    std::string &prestring                                             = *((std::string                           *) ((void **) arg)[16]);
//    std::string &midstring                                             = *((std::string                           *) ((void **) arg)[17]);

    return regsvm(svmThreadOwner,svmbase,threadInd,ind,svmContext,MLreg);
}

void gridelmrun(gentype &res, Vector<gentype> &x, void *arg)
{
    std::string &commandstr                                            = *((std::string                           *) ((void **) arg)[0] );
    SparseVector<SVMThreadContext *> &svmContext                       = *((SparseVector<SVMThreadContext *>      *) ((void **) arg)[1] );
    int &verblevel                                                     = *((int                                   *) ((void **) arg)[2] );
    gentype &finalresult                                               = *((gentype                               *) ((void **) arg)[3] );
    std::string &logfile                                               = *((std::string                           *) ((void **) arg)[4] );
    int &depthin                                                       = *((int                                   *) ((void **) arg)[5] );
    SparseVector<SparseVector<gentype> > &argvariables                 = *((SparseVector<SparseVector<gentype> >  *) ((void **) arg)[6] );
    int &threadInd                                                     = *((int                                   *) ((void **) arg)[7] );
    svmvolatile SparseVector<SparseVector<gentype> > &globargvariables = *((SparseVector<SparseVector<gentype> >  *) ((void **) arg)[8] );
    Vector<int> &argnums                                               = *((Vector<int>                           *) ((void **) arg)[9] );
    int (*getsetExtVar)(gentype &, const gentype &, int)               = ((int (*)(gentype &, const gentype &, int)) ((void **) arg)[10]);
    SparseVector<ML_Mutable *> &svmbase                                = *((SparseVector<ML_Mutable *>            *) ((void **) arg)[11]);
    SparseVector<int> &svmThreadOwner                                  = *((SparseVector<int>                     *) ((void **) arg)[12]);
    std::string &interstring                                           = *((std::string                           *) ((void **) arg)[13]);
    gentype &xfnis                                                     = *((gentype                               *) ((void **) arg)[14]);
    Vector<int> &MLnumbers                                             = *((Vector<int>                           *) ((void **) arg)[15]);
    std::string &prestring                                             = *((std::string                           *) ((void **) arg)[16]);
    std::string &midstring                                             = *((std::string                           *) ((void **) arg)[17]);

    (void) verblevel;
    (void) logfile;
    (void) finalresult;
    (void) depthin;

    int itnum = 0;

    if ( res.isValInteger() )
    {
        itnum = (int) res;
    }

    int itnumalt = (-itnum)/10000; // relevant if itnum < 0
    int itnumid  = (-itnum)%10000; // relevant if itnum < 0

    SparseVector<SparseVector<gentype> > gridargvars(argvariables);

    NiceAssert( ( x.size() >= argnums.size() ) || !x.size() );

    int i;

    gridargvars("&",90)("&",zeroint()) = MLnumbers(zeroint());
    gridargvars("&",90)("&",1)         = MLnumbers(1);
    gridargvars("&",90)("&",2)         = MLnumbers(2);
    gridargvars("&",90)("&",3)         = MLnumbers(3);
    gridargvars("&",90)("&",4)         = ( itnum >= 0 ) ? itnum : itnumalt;
    gridargvars("&",90)("&",5)         = MLnumbers(4);
    gridargvars("&",90)("&",6)         = MLnumbers(5);

    if ( x.size() )
    {
        // standard evaluation

        for ( i = 0 ; i < x.size() ; i++ )
        {
            if ( i < argnums.size() )
            {
                gridargvars("&",zeroint())("&",argnums(i)) = x(i);
            }

            else
            {
                // This is bayes on grid, result in ML is stored in x(dim),
                // so load into gridargvars(0,0)

                gridargvars("&",zeroint())("&",zeroint()) = x(i);
            }
        }

        int locverblevel = verblevel;
        int adim = ( x.size() < argnums.size() ) ? x.size() : argnums.size();

        retVector<gentype> tmpva;

        errstream() << "Running grid " << x(zeroint(),1,adim-1,tmpva) << " ... ";

        if ( argnums.size() < x.size() )
        {
            errstream() << "Grid test value recall = " << gridargvars(zeroint())(zeroint()) << "\n";
        }

        // Bookkeeping stuff.  Not entirely sure what this does - probably
        // should have documented it better when I first wrote it.

        std::stringstream *tmpcommand;
        MEMNEW(tmpcommand,std::stringstream(commandstr));
        awarestream *gridbox;
        MEMNEW(gridbox,awarestream(tmpcommand,1));
        Stack<awarestream *> *gridcommstack;
        MEMNEW(gridcommstack,Stack<awarestream *>);
        gridcommstack->push(gridbox);
        std::string loclogfile = logfile+".gridlog";

        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,res,loclogfile);

        MEMDEL(gridcommstack);
    }

    else if ( ( itnum == 0 ) && interstring.length() )
    {
        int locverblevel = verblevel;

        errstream() << "Running intermediate evaluation ... ";

        // Bookkeeping stuff.  Not entirely sure what this does - probably
        // should have documented it better when I first wrote it.

        std::stringstream *tmpcommand;
        MEMNEW(tmpcommand,std::stringstream(interstring));
        awarestream *gridbox;
        MEMNEW(gridbox,awarestream(tmpcommand,1));
        Stack<awarestream *> *gridcommstack;
        MEMNEW(gridcommstack,Stack<awarestream *>);
        gridcommstack->push(gridbox);
        std::string loclogfile = logfile+".gridlog";

        // Call runsvm to get test result and save result

        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,res,loclogfile);

        MEMDEL(gridcommstack);
    }

    else if ( ( itnum < 0 ) && ( itnumid == 1 ) && prestring.length() )
    {
        int locverblevel = verblevel;

        errstream() << "Running pre-optimisation setup operation (" << prestring << ") ... ";

        // Bookkeeping stuff.  Not entirely sure what this does - probably
        // should have documented it better when I first wrote it.

        std::stringstream *tmpcommand;
        MEMNEW(tmpcommand,std::stringstream(prestring));
        awarestream *gridbox;
        MEMNEW(gridbox,awarestream(tmpcommand,1));
        Stack<awarestream *> *gridcommstack;
        MEMNEW(gridcommstack,Stack<awarestream *>);
        gridcommstack->push(gridbox);
        std::string loclogfile = logfile+".gridlog";

        // Call runsvm to get test result and save result

        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,res,loclogfile);

        MEMDEL(gridcommstack);
    }

    else if ( ( itnum < 0 ) && ( itnumid == 2 ) && midstring.length() )
    {
        int locverblevel = verblevel;

        errstream() << "Running intermediate setup operation ... ";

        // Bookkeeping stuff.  Not entirely sure what this does - probably
        // should have documented it better when I first wrote it.

        std::stringstream *tmpcommand;
        MEMNEW(tmpcommand,std::stringstream(midstring));
        awarestream *gridbox;
        MEMNEW(gridbox,awarestream(tmpcommand,1));
        Stack<awarestream *> *gridcommstack;
        MEMNEW(gridcommstack,Stack<awarestream *>);
        gridcommstack->push(gridbox);
        std::string loclogfile = logfile+".gridlog";

        // Call runsvm to get test result and save result

        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,res,loclogfile);

        MEMDEL(gridcommstack);
    }

    // Update x if relevant

    if ( !xfnis.isValNull() )
    { 
        x = (const Vector<gentype> &) xfnis(gridargvars);
    }

    // Exit - we're done

    errstream() << "Grid raw error: " << res << "\n";

    return;
}

































void emptycommstack(Stack<awarestream *> &commstack)
{
    while ( commstack.size() )
    {
        MEMDEL(commstack.accessTop());

        commstack.pop();
    }

    return;
}

int grabnextarg(Stack<awarestream *> &commstack, std::string &currentarg)
{
    int stopnow = 0;

    while ( !stopnow )
    {
        if ( commstack.size() == 0 )
        {
            // Stack is empty, no more inputs available.

            stopnow = 1;
        }

        else if ( !((commstack.accessTop())->good()) )
        {
            // Top of stack is exhausted.  Delete (which
            // will close file streams) the variable on
            // top of the stack and, pop it off and try
            // again.

            // see also below

            MEMDEL(commstack.accessTop());

            commstack.pop();
        }

        else
        {
            // Stream off next argument.

            currentarg = "";

            if ( (commstack.accessTop())->skim(currentarg) )
            {
                // See above

                MEMDEL(commstack.accessTop());

                commstack.pop();
            }

            else if ( currentarg.length() )
            {
#ifdef MANGLE_UNDERSCORES
                int i;

                for ( i = 0 ; i < (int) currentarg.length() ; i++ )
                {
                    if ( currentarg[i] == '_' )
                    {
                        currentarg[i] = ' ';
                    }
                }
#endif

                break;
            }
        }
    }

    return stopnow;
}

int grabargs(int num, Vector<Vector<std::string> > &destlist, Stack<awarestream *> &commstack, std::string &currentarg, int allowvetor)
{
    int i,j;
    unsigned int k;
    int quotelevel;
    int escchar;

    if ( num > 0 )
    {
        std::string oldarg = currentarg;
        std::string innerarg;

        destlist.add(j = destlist.size());
        // BE CAREFUL!!!  Because of the way vectors work, it is quite
        // possible that the contents of the cell just added will actually
        // be the preserved contents of whatever was in this cell previously
        // (prior to it being deleted), if there were contents previously.
        // This actually can result in bugs.  Specifically, if it had
        // contents (a vector of strings), these were "removed", and then
        // this line re-adds them, then the contents of detlist(j) will be
        // the old arguments, and the new arguments will be concatenated onto
        // these!
        destlist("&",j).resize(0);

        for ( i = 0 ; i < num ; i++ )
        {
            if ( i )
            {
                // If allowvector then the arguments may be a vector.  To
                // allow for this we have a parenthesis stack.

                Stack<char> parstack;
                quotelevel = 0;
                currentarg = "";
                escchar = 0;

                do
                {
                    if ( grabnextarg(commstack,innerarg) )
                    {
                        errstream() << "Syntax error: " << oldarg << " requires " << num << " arguments\n";
                        return 1;
                    }

                    // Parenthesise traversed using stack.  {[( are put on
                    // stack, )]} cancel their pair off the top of the stack,
                    // quotes are modal.

                    // Note that escaped characters are skipped

                    if ( allowvetor && innerarg.length() )
                    {
                        escchar = 0;

                        for ( k = 0 ; k < innerarg.length() ; k++ )
                        {
                            if ( !escchar && ( innerarg[k] == '\\' ) )
                            {
                                escchar = 1;
                                innerarg.erase(k,1);
                                k--;
                            }

                            else if ( escchar )
                            {
                                switch ( innerarg[k] )
                                {
                                    case 'a': { innerarg[k] = '\a'; break; }
                                    case 'b': { innerarg[k] = '\b'; break; }
                                    case 'f': { innerarg[k] = '\f'; break; }
                                    case 'n': { innerarg[k] = '\n'; break; }
                                    case 'r': { innerarg[k] = '\r'; break; }
                                    case 't': { innerarg[k] = '\t'; break; }
                                    case 'v': { innerarg[k] = '\v'; break; }
                                    default: { break; }
                                }

                                // default interpretted as "just keep it"

                                escchar = 0;
                            }

                            else if ( !quotelevel )
                            {
                                if ( ( innerarg[k] == '[' ) ||
                                     ( innerarg[k] == '{' ) ||
                                     ( innerarg[k] == '(' )    )
                                {
                                    parstack.push(innerarg[k]);
                                }

                                else if ( innerarg[k] == ')' )
                                {
                                    if ( parstack.isempty() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.accessTop() != '(' )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.pop() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }
                                }

                                else if ( innerarg[k] == ']' )
                                {
                                    if ( parstack.isempty() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.accessTop() != '[' )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.pop() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }
                                }

                                else if ( innerarg[k] == '}' )
                                {
                                    if ( parstack.isempty() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.accessTop() != '{' )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.pop() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }
                                }

                                else if ( innerarg[k] == '\"' )
                                {
                                    quotelevel = 1;
                                }
                            }

                            else if ( innerarg[k] == '\"' )
                            {
                                quotelevel = 0;
                            }
                        }
                    }

                    currentarg += innerarg;

                    if ( quotelevel || parstack.size() )
                    {
                        currentarg += " ";
                    }
                }
                while ( quotelevel || parstack.size() );
            }

            stripquotes(currentarg);

            destlist("&",j).add(i);
            destlist("&",j)("&",i) = currentarg;
        }
    }

    return 0;
}

int puttylump(const std::string &src, Stack<awarestream *> &commstack)
{
    int stopnow = 0;

    while ( !stopnow )
    {
        if ( commstack.size() == 0 )
        {
            // Stack is empty, no more outputs available.

            stopnow = 1;
        }

        else
        {
            // Push out src

            if ( (commstack.accessTop())->vogon(src) )
            {
                MEMDEL(commstack.accessTop());

                commstack.pop();
            }

            else
            {
                break;
            }
        }
    }

    return stopnow;
}

void stripcurlybrackets(std::string &evalarg)
{
    int repcnt = 0;

    while ( evalarg.length() && ( ( ( evalarg[0] == '{' ) && !repcnt ) || isspace(evalarg[0]) ) )
    {
        evalarg = evalarg.substr(1,evalarg.length()-1);

        if ( !evalarg.length() )
        {
            break;
        }

        repcnt++;
    }

    repcnt = 0;

    while ( evalarg.length() && ( ( ( evalarg[evalarg.length()-1] == '}' ) && !repcnt ) || isspace(evalarg[evalarg.length()-1]) ) )
    {
        evalarg = evalarg.substr(0,evalarg.length()-1);

        if ( !evalarg.length() )
        {
            break;
        }

        repcnt++;
    }

    return;
}

void stripquotes(std::string &evalarg)
{
    int repcnt = 0;

    while ( evalarg.length() && ( ( ( evalarg[0] == '\"' ) && !repcnt ) || isspace(evalarg[0]) ) )
    {
        evalarg = evalarg.substr(1,evalarg.length()-1);

        if ( !evalarg.length() )
        {
            break;
        }

        repcnt++;
    }

    repcnt = 0;

    while ( evalarg.length() && ( ( ( evalarg[evalarg.length()-1] == '\"' ) && !repcnt ) || isspace(evalarg[evalarg.length()-1]) ) )
    {
        evalarg = evalarg.substr(0,evalarg.length()-1);

        if ( !evalarg.length() )
        {
            break;
        }

        repcnt++;
    }

    return;
}

void safeatowhatever(int &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    res = safeatoi(src,argvariables);

    return;
}

void safeatowhatever(double &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    res = safeatof(src,argvariables);

    return;
}
void safeatowhatever(gentype &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    res = safeatog(src,argvariables);

    return;
}

int safeatoi(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    gentype srceqn(src);
    int res;

    srceqn.substitute(argvariables);

    try
    {
        res = (int) srceqn;
    }

    catch ( std::string errstr )
    {
        std::string errstring;
        errstring = "Syntax error: "+src+" does not evaluate as integer ("+errstr+").";
        throw errstring;
    }

    return res;
}

double safeatof(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    gentype srceqn(src);
    double res;

    srceqn.substitute(argvariables);

    try
    {
        res = (double) srceqn;
    }

    catch ( std::string errstr )
    {
        std::string errstring;
        errstring = "Syntax error: "+src+" does not evaluate as double ("+errstr+").";
        throw errstring;
    }

    return res;
}

gentype safeatog(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    gentype res(src);

    res.substitute(argvariables);

    return res;
}

template <class T> Vector<T> &safeatoVector(Vector<T> &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    gentype temp(safeatog(src,argvariables));

    temp.morph_vector();

    int res_size = temp.size();

    res.resize(res_size);

    if ( res_size )
    {
        int i;

        const Vector<gentype> &ghgh = (const Vector<gentype> &) temp;

        for ( i = 0 ; i < res_size ; i++ )
        {
            res("&",i) = (T) ghgh(i);
        }
    }

    return res;
}

template <class T> Matrix<T> &safeatoMatrix(Matrix<T> &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    std::string xsrc("M:");

    xsrc = xsrc + src;

    gentype temp(safeatog(xsrc,argvariables));

    temp.morph_matrix();

    int res_nrows = temp.numRows();
    int res_ncols = temp.numCols();

    res.resize(res_nrows,res_ncols);

    if ( res_nrows && res_ncols )
    {
        int i,j;

        const Matrix<gentype> &ghgh = (const Matrix<gentype> &) temp;

        for ( i = 0 ; i < res_nrows ; i++ )
        {
            for ( j = 0 ; j < res_ncols ; j++ )
            {
                res("&",i,j) = (T) ghgh(i,j);
            }
        }
    }

    return res;
}

void xlateDataSourceSuffixes(int isANtype, int fileargpos, int setibase, const Vector<std::string> &currcommand, const std::string &subcom, SparseVector<SparseVector<gentype> > &argvariables, SparseVector<ofiletype> &filevariables,
     int &reverse, int &ignoreStart, int &imax, int &ibase, int &uselinesvector, int &israw, int &startpoint, int &coercetosingle, int &coercefromsingle, gentype &fromsingletarget, std::string &trainfile,Vector<int> &linesread)
{
    int randorder = 0; // set 1 for random ordering
    int filenum = -1;  // file number in -AA...i... type commands
    int uselinesa = 0;
    int uselinesb = 0;

    trainfile = currcommand(fileargpos);

    reverse          = ( subcom.find("e") != std::string::npos );
    ignoreStart      = isANtype ? safeatoi(currcommand(1),argvariables) : 0;
    imax             = isANtype ? safeatoi(currcommand(2),argvariables) : -1;
    ibase            = ( isANtype && setibase ) ? safeatoi(currcommand(3),argvariables) : -1;
    randorder        = ( subcom.find("r") != std::string::npos ) || ( subcom.find("R") != std::string::npos );
    uselinesa        = ( subcom.find("r") != std::string::npos ) || ( subcom.find("i") != std::string::npos );
    uselinesb        = ( subcom.find("R") != std::string::npos ) || ( subcom.find("I") != std::string::npos );
    uselinesvector   = uselinesa ? 1 : ( uselinesb ? 2 : 0 );
    israw            = ( subcom.find("B") != std::string::npos );
    startpoint       = ( subcom.find("z") != std::string::npos ) ? 1 : 0;
    startpoint       = ( subcom.find("Z") != std::string::npos ) ? 2 : 0;
    coercetosingle   = ( subcom.find("u") != std::string::npos );
    coercefromsingle = ( subcom.find("l") != std::string::npos );
    filenum          = uselinesvector ? safeatoi(trainfile,argvariables) : 0;

    fromsingletarget = 0;

    if ( coercefromsingle )
    {
        fromsingletarget = safeatog(currcommand(fileargpos+1),argvariables);
    }

    if ( uselinesvector )
    {
        preExtractLinesFromFile(filevariables("&",filenum),argvariables("&",0)("&",filenum),trainfile,ignoreStart,imax,linesread,randorder);

        ignoreStart = 0;
        imax = -1;
        ibase = -1;
    }

    return;
}

void preExtractLinesFromFile(ofiletype &filelines, gentype &linesleft, std::string &trainfile, int ignoreStart, int imax, Vector<int> &linesread, int randorder)
{
    if ( filelines.getlinecnt() < ignoreStart )
    {
        std::string errstring;
        errstring = "Error: cannot ignore more lines than are left in a file.";
        throw errstring;
    }

    if ( imax == -1 )
    {
        imax = filelines.getlinecnt();
    }

    imax = ( (filelines.getlinecnt())-ignoreStart <= imax ) ? (filelines.getlinecnt())-ignoreStart : imax;

    trainfile = filelines.getfilename();

    int jjj = 0;
    int kkk;

    // Grab line numbers

    linesread.resize(imax);

    while ( imax )
    {
        kkk = randorder ? (svm_rand()%((filelines.getlinecnt())-ignoreStart))+ignoreStart : ignoreStart;
        linesread("&",jjj) = filelines.pullline(kkk);

        jjj++;
        imax--;
    }

    // Sort vector into order

    int cii,cjj,ckk;

    if ( linesread.size() > 1 )
    {
        for ( cii = 0 ; cii < linesread.size()-1 ; cii++ )
        {
            for ( cjj = cii+1 ; cjj < linesread.size() ; cjj++ )
            {
                if ( linesread(cii) > linesread(cjj) )
                {
                    ckk                 = linesread(cii);
                    linesread("&",cii) = linesread(cjj);
                    linesread("&",cjj) = ckk;
                }
            }
        }
    }

    // Fix line count variable

    linesleft = filelines.getlinecnt();

    return;
}






















































void processKernel(ML_Base &kernML, MercerKernel &theKern, const std::string &currcommandis, const Vector<std::string> &currcommand, int ktype, SparseVector<SparseVector<gentype> > &argvariables, int &kernnum, int firstcall, SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext)
{
    (void) svmInd;

    // Process this first

    if ( ( ktype == 0 ) && ( currcommand(zeroint()) == "-ktk" ) )
    {
        if ( isMLM(kernML) )
        {
            if ( kernML.isMutable() )
            {
                ML_Mutable &kernMLmut = dynamic_cast<ML_Mutable &>(kernML);
                MLM_Generic &kernMLM = kernMLmut.getMLM();

                kernMLM.setknum(safeatoi(currcommand(1),argvariables));
            }

            else
            {
                MLM_Generic &kernMLM = dynamic_cast<MLM_Generic &>(kernML);

                kernMLM.setknum(safeatoi(currcommand(1),argvariables));
            }
        }

        firstcall = 1; // trigger kernnum re-calculation
    }

    // Calculate multi-layer kernel offset

    int keroffset = 0;

    if ( ( ktype == 0 ) && isMLM(kernML) )
    {
        if ( kernML.isMutable() )
        {
            ML_Mutable &kernMLmut = dynamic_cast<ML_Mutable &>(kernML);
            MLM_Generic &kernMLM = kernMLmut.getMLM();

            keroffset = ( kernMLM.tsize() && kernMLM.knum() ) ? 1 : 0;
        }

        else
        {
            MLM_Generic &kernMLM = dynamic_cast<MLM_Generic &>(kernML);

            keroffset = ( kernMLM.tsize() && kernMLM.knum() ) ? 1 : 0;
        }
    }

    // Calculate kernnum on firstcall

    if ( firstcall )
    {
        kernnum = keroffset;
    }

    // Update kernnum as required

    if ( currcommandis == "-ki" )
    {
        kernnum = keroffset + safeatoi(currcommand(1),argvariables);

        if ( kernnum < 0 )
        {
            STRTHROW("Error: kernel number must be non-negative or zero");
        }

        else if ( kernnum >= theKern.size() )
        {
            STRTHROW("Error: kernel number out of dictionary range.");
        }
    }

    // Prepare kernel if required

    if ( ( currcommandis == "-ks"  ) || ( currcommandis == "-kn"  ) || ( currcommandis == "-ku"   ) ||
         ( currcommandis == "-knn" ) || ( currcommandis == "-kuu" ) || 
         ( currcommandis == "-ku"  ) || ( currcommandis == "-ka"  ) ||
         ( currcommandis == "-kb"  ) || ( currcommandis == "-ke"  ) || ( currcommandis == "-kc"   ) ||
         ( currcommandis == "-kuc" ) || ( currcommandis == "-kS"  ) || ( currcommandis == "-kuS"  ) || 
                                        ( currcommandis == "-kMS" ) || ( currcommandis == "-kMuS" ) ||
         ( currcommandis == "-km"  ) || ( currcommandis == "-kum" ) || ( currcommandis == "-kI"   ) ||
         ( currcommandis == "-kw"  ) || ( currcommandis == "-kt"  ) || ( currcommandis == "-kan"  ) ||
         ( currcommandis == "-kg"  ) || ( currcommandis == "-kr"  ) || ( currcommandis == "-kd"   ) ||
         ( currcommandis == "-kG"  ) || ( currcommandis == "-kf"  ) || ( currcommandis == "-kv"   ) ||
         ( currcommandis == "-kV"  ) || ( currcommandis == "-kgg" )                                    )
    {
        if ( ktype == 0 )
        {
            kernML.prepareKernel();
        }
    }

    // Process the command

                        SparseVector<int> newrealover;
                        SparseVector<int> newintover;
                        Vector<int> tempresa;
                        Vector<gentype> tempresb; 

                             if ( currcommandis == "-ks"   ) { theKern.resize(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommandis == "-kn"   ) { theKern.setNormalised(kernnum); }
                        else if ( currcommandis == "-ku"   ) { theKern.setUnNormalised(kernnum); }
                        else if ( currcommandis == "-knn"  ) { theKern.setFullNorm(); }
                        else if ( currcommandis == "-kuu"  ) { theKern.setNoFullNorm(); }
                        else if ( currcommandis == "-koz"  ) { theKern.setRealOverwrite(newrealover,kernnum); }
                        else if ( currcommandis == "-kOz"  ) { theKern.setIntOverwrite(newintover,kernnum); }
                        else if ( currcommandis == "-ku"   ) { theKern.setUnNormalised(kernnum); }
                        else if ( currcommandis == "-ka"   ) { theKern.setnumSamples(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommandis == "-kb"   ) { theKern.setSampleIndices(safeatoVector(tempresa,currcommand(1),argvariables)); }
                        else if ( currcommandis == "-ke"   ) { theKern.setSampleDistribution(safeatoVector(tempresb,currcommand(1),argvariables)); }
                        else if ( currcommandis == "-kc"   ) { theKern.setChained(kernnum); }
                        else if ( currcommandis == "-kuc"  ) { theKern.setUnChained(kernnum); }
                        else if ( currcommandis == "-kS"   ) { theKern.setSplit(kernnum); }
                        else if ( currcommandis == "-kuS"  ) { theKern.setUnSplit(kernnum); }
                        else if ( currcommandis == "-kMS"  ) { theKern.setMulSplit(kernnum); }
                        else if ( currcommandis == "-kMuS" ) { theKern.setUnMulSplit(kernnum); }
                        else if ( currcommandis == "-km"   ) { theKern.setMagTerm(kernnum); }
                        else if ( currcommandis == "-kum"  ) { theKern.setUnMagTerm(kernnum); }
                        else if ( currcommandis == "-kU"   ) { theKern.setUnIndex(); }
                        else if ( currcommandis == "-kw"   ) { theKern.setWeight(safeatog(currcommand(1),argvariables),kernnum); }
                        else if ( currcommandis == "-kt"   ) { theKern.setType(safeatoi(currcommand(1),argvariables),kernnum); }
                        else if ( currcommandis == "-mtb"  ) { theKern.setsuggestXYcache(1); }
                        else if ( currcommandis == "-bmx"  ) { theKern.setsuggestXYcache(0); }
//                        else if ( currcommandis == "-kcy " ) { theKern.setChurnInner(1); }
//                        else if ( currcommandis == "-kcn"  ) { theKern.setChurnInner(0); }
                        else if ( currcommandis == "-kan"  ) { theKern.setAltDiff(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommandis == "-ktx"  ) { theKern.setAltCall((getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext)).MLid(),kernnum); }

                        else if ( currcommandis == "-kI" ) 
                        { 
                            Vector<int> vectInd; 

                            safeatoVector(vectInd,currcommand(1),argvariables); 

                            theKern.setIndexes(vectInd);
                        }

                        else if ( currcommandis == "-ko" )
                        {
                            int destind = safeatoi(currcommand(1),argvariables)+1;
                            int srcind  = safeatoi(currcommand(2),argvariables);

                            if ( destind < 0 )
                            {
                                STRTHROW("Error: destination index must be >= -1 in -ko");
                            }

                            if ( srcind < 0 )
                            {
                                STRTHROW("Error: source index must be >= 0 in -ko");
                            }

                            SparseVector<int> newrealover(theKern.cRealOverwrite());

                            newrealover("&",destind) = srcind;

                            theKern.setRealOverwrite(newrealover,kernnum);
                        }

                        else if ( currcommandis == "-kO" )
                        {
                            int destind = safeatoi(currcommand(1),argvariables);
                            int srcind  = safeatoi(currcommand(2),argvariables);

                            if ( destind < 0 )
                            {
                                STRTHROW("Error: destination index must be >= 0 in -kO");
                            }

                            if ( srcind < 0 )
                            {
                                STRTHROW("Error: source index must be >= 0 in -kO");
                            }

                            SparseVector<int> newintover(theKern.cIntOverwrite());

                            newintover("&",destind) = srcind;

                            theKern.setIntOverwrite(newintover,kernnum);
                        }

                        else if ( currcommandis == "-kg" )
                        {
                            Vector<gentype> kernRealConsts(theKern.cRealConstants(kernnum));
                            gentype temparg;

                            if ( kernRealConsts.size() <= 0 )
                            {
                                STRTHROW("Error: constant r0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernRealConsts("&",zeroint()) = temparg;

                            theKern.setRealConstants(kernRealConsts,kernnum);
                        }

                        else if ( currcommandis == "-kgg" )
                        {
                            Vector<gentype> xxscale;
                            SparseVector<gentype> xscale;

                            xscale = safeatoVector(xxscale,currcommand(1),argvariables);

errstream() << "set xscale: " << xscale << "\n";
                            theKern.setScale(xscale);
                        }

                        else if ( currcommandis == "-kr" )
                        {
                            Vector<gentype> kernRealConsts(theKern.cRealConstants(kernnum));
                            gentype temparg;

                            if ( kernRealConsts.size() <= 1 )
                            {
                                STRTHROW("Error: constant r1 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernRealConsts("&",1) = temparg;

                            theKern.setRealConstants(kernRealConsts,kernnum);
                        }

                        else if ( currcommandis == "-kd" )
                        {
                            Vector<int> kernIntConsts(theKern.cIntConstants(kernnum));
                            int temparg;

                            if ( kernIntConsts.size() <= 0 )
                            {
                                STRTHROW("Error: constant i0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernIntConsts("&",zeroint()) = temparg;

                            theKern.setIntConstants(kernIntConsts,kernnum);
                        }

                        else if ( currcommandis == "-kG" )
                        {
                            Vector<int> kernIntConsts(theKern.cIntConstants(kernnum));
                            int temparg;

                            if ( kernIntConsts.size() <= 0 )
                            {
                                STRTHROW("Error: constant i0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernIntConsts("&",zeroint()) = temparg;

                            theKern.setIntConstants(kernIntConsts,kernnum);
                        }

                        else if ( currcommandis == "-kf" )
                        {
                            Vector<gentype> kernRealConsts(theKern.cRealConstants(kernnum));
                            gentype temparg;

                            if ( kernRealConsts.size() <= 10 )
                            {
                                STRTHROW("Error: constant r10 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernRealConsts("&",10) = temparg;

                            theKern.setRealConstants(kernRealConsts,kernnum);
                        }

                        else if ( currcommandis == "-kv" )
                        {
                            Vector<gentype> kernRealConsts(theKern.cRealConstants(kernnum));
                            gentype temparg;

                            if ( kernRealConsts.size() <= safeatoi(currcommand(1),argvariables) )
                            {
                                STRTHROW("Error: constant r"+currcommand(1)+" not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(2),argvariables);

                            kernRealConsts("&",safeatoi(currcommand(1),argvariables)) = temparg;
    
                            theKern.setRealConstants(kernRealConsts,kernnum);
                        }

                        else if ( currcommandis == "-kV" )
                        {
                            Vector<int> kernIntConsts(theKern.cIntConstants(kernnum));
                            int temparg;

                            if ( kernIntConsts.size() <= safeatoi(currcommand(1),argvariables) )
                            {
                                STRTHROW("Error: constant i"+currcommand(1)+" not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(2),argvariables);

                            kernIntConsts("&",safeatoi(currcommand(1),argvariables)) = temparg;

                            theKern.setIntConstants(kernIntConsts,kernnum);
                        }

    // Reset kernel if required

    if ( ( currcommandis == "-ks"  ) || ( currcommandis == "-kn"  ) || ( currcommandis == "-ku"   ) ||
         ( currcommandis == "-knn" ) || ( currcommandis == "-kuu" ) || 
         ( currcommandis == "-ku"  ) || ( currcommandis == "-ka"  ) ||
         ( currcommandis == "-kb"  ) || ( currcommandis == "-ke"  ) || ( currcommandis == "-kc"   ) ||
         ( currcommandis == "-kuc" ) || ( currcommandis == "-kS"  ) || ( currcommandis == "-kuS"  ) ||
                                        ( currcommandis == "-kMS" ) || ( currcommandis == "-kMuS" ) ||
         ( currcommandis == "-km"  ) || ( currcommandis == "-kum" ) || ( currcommandis == "-kw"   ) ||
         ( currcommandis == "-kt"  ) || ( currcommandis == "-kg"  ) || ( currcommandis == "-kr"   ) ||
         ( currcommandis == "-kd"  ) || ( currcommandis == "-kG"  ) || ( currcommandis == "-kf"   ) ||
         ( currcommandis == "-kv"  ) || ( currcommandis == "-kV"  )                                    )
    {
        if ( ktype == 0 )
        {
            kernML.resetKernel(0,-1,0);
        }

        else if ( ktype == 1 )
        {
            kernML.resetUUOutputKernel(0);
        }
    }

    if ( ( currcommandis == "-ko"  ) || ( currcommandis == "-kO"  ) || ( currcommandis == "-koz" ) ||
         ( currcommandis == "-kOz" ) || ( currcommandis == "-kan" ) || ( currcommandis == "-ktx" ) )
    {
        if ( ktype == 0 )
        {
            kernML.resetKernel(0);
        }

        else if ( ktype == 1 )
        {
             kernML.resetUUOutputKernel(0);
        }
    }

    if ( ( currcommandis == "-kI"  ) || ( currcommandis == "-kU"  ) || ( currcommandis == "-kgg" ) )
    {
        if ( ktype == 0 )
        {
            kernML.resetKernel();
        }

        else if ( ktype == 1 )
        {
            kernML.resetUUOutputKernel();
        }
    }

    return;
}









































void testTest(std::string &logfile, const ML_Mutable &svmbase, const Vector<SparseVector<gentype> > &xtest, const Vector<gentype> &ytest, int &firstsum, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    Vector<int> cnt;
    Matrix<int> cfm;
    double res = 0.0;
    int i;
    Vector<gentype> resh;
    Vector<gentype> resg;
    Vector<gentype> gvarres;

    res = calcTest(svmbase,xtest,ytest,cnt,cfm,resh,resg,gvarres,recordxvar);

    std::string resfilenamefull = logfile+".test.res";
    std::string tarfilenamefull = logfile+".test.tar";
    std::string clafilenamefull = logfile+".test.cla";
    std::string varfilenamefull = logfile+".test.var";

    writeLog(resg,resfilenamefull,getsetExtVar);
    writeLog(ytest,tarfilenamefull,getsetExtVar);
    writeLog(resh,clafilenamefull,getsetExtVar);

    if ( recordxvar )
    {
        writeLog(gvarres,varfilenamefull,getsetExtVar);
    }

    if ( recordres )
    {
        argvariables("&",1)("&",5) = resh;
        argvariables("&",1)("&",6) = resg;
    }

    errstream() << "\n";

    Vector<double> accsum(7);

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        retVector<int> tmpva;

        measureAccuracy(accsum,resg,resh,ytest,oneintvec(resg.size(),tmpva),svmbase);
    }

    else
    {
        accsum = -1.0;

        accsum("&",zeroint()) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;
    }

    errstream() << "Accuracy:  " << accsum(zeroint()) << "\n";
    errstream() << "Precision: " << accsum(1) << "\n";
    errstream() << "Recall:    " << accsum(2) << "\n";
    errstream() << "F1 Score:  " << accsum(3) << "\n";
    errstream() << "AUC:       " << accsum(4) << "\n";
    errstream() << "Sparsity:  " << accsum(5) << "\n";
    errstream() << "Error:     " << accsum(6) << "\n";

    argvariables("&",1)("&",37) = accsum(zeroint());
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);

    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; i++ )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << "Test error: "      << res << "\n";
    errstream() << "Class counts: "    << cnt << "\n";
    errstream() << "Confusion: "       << cfm << "\n";
    errstream() << "Classwise error: " << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+".test.sum";
    std::string cntfilenamefull = logfile+".test.cnt";
    std::string cfmfilenamefull = logfile+".test.cfm";
    std::string clrfilenamefull = logfile+".test.clr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);

    return;
}

void testLOO(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, int startpoint, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    Vector<int> cnt;
    Matrix<int> cfm;
    double res = 0.0;
    int i;
    Vector<gentype> resh;
    Vector<gentype> resg;
    Vector<gentype> gvarres;

    res = calcLOO(svmbase,cnt,cfm,resh,resg,gvarres,startpoint,recordxvar);

    std::string resfilenamefull = logfile+".LOO.res";
    std::string tarfilenamefull = logfile+".LOO.tar";
    std::string clafilenamefull = logfile+".LOO.cla";
    std::string varfilenamefull = logfile+".LOO.var";

    writeLog(resg,resfilenamefull,getsetExtVar);
    writeLog(svmbase.y(),tarfilenamefull,getsetExtVar);
    writeLog(resh,clafilenamefull,getsetExtVar);

    if ( recordxvar )
    {
        writeLog(gvarres,varfilenamefull,getsetExtVar);
    }

    if ( recordres )
    {
        argvariables("&",1)("&",5) = resh;
        argvariables("&",1)("&",6) = resg;
    }

    errstream() << "\n";

    Vector<double> accsum(7);

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        measureAccuracy(accsum,resg,resh,svmbase.y(),svmbase.d(),svmbase);
    }

    else
    {
        accsum = -1.0;

        accsum("&",zeroint()) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;
    }

    errstream() << "Accuracy:  " << accsum(zeroint()) << "\n";
    errstream() << "Precision: " << accsum(1) << "\n";
    errstream() << "Recall:    " << accsum(2) << "\n";
    errstream() << "F1 Score:  " << accsum(3) << "\n";
    errstream() << "AUC:       " << accsum(4) << "\n";
    errstream() << "Sparsity:  " << accsum(5) << "\n";
    errstream() << "Error:     " << accsum(6) << "\n";

    argvariables("&",1)("&",37) = accsum(zeroint());
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);

    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; i++ )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << "LOO error: "       << res << "\n";
    errstream() << "Class counts: "    << cnt << "\n";
    errstream() << "Confusion: "       << cfm << "\n";
    errstream() << "Classwise error: " << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+".LOO.sum";
    std::string cntfilenamefull = logfile+".LOO.cnt";
    std::string cfmfilenamefull = logfile+".LOO.cfm";
    std::string clrfilenamefull = logfile+".LOO.clr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);

    return;
}

void testRecall(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    Vector<int> cnt;
    Matrix<int> cfm;
    double res = 0.0;
    int i;
    Vector<gentype> resh;
    Vector<gentype> resg;
    Vector<gentype> gvarres;

    res = calcRecall(svmbase,cnt,cfm,resh,resg,gvarres,0,recordxvar);

    std::string resfilenamefull = logfile+".recall.res";
    std::string tarfilenamefull = logfile+".recall.tar";
    std::string clafilenamefull = logfile+".recall.cla";
    std::string varfilenamefull = logfile+".recall.var";

    writeLog(resg,resfilenamefull,getsetExtVar);
    writeLog(svmbase.y(),tarfilenamefull,getsetExtVar);
    writeLog(resh,clafilenamefull,getsetExtVar);

    if ( recordxvar )
    {
        writeLog(gvarres,varfilenamefull,getsetExtVar);
    }

    if ( recordres )
    {
        argvariables("&",1)("&",5) = resh;
        argvariables("&",1)("&",6) = resg;
    }

    errstream() << "\n";

    Vector<double> accsum(7);

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        measureAccuracy(accsum,resg,resh,svmbase.y(),svmbase.d(),svmbase);
    }

    else
    {
        accsum = -1.0;

        accsum("&",zeroint()) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;
    }

    errstream() << "Accuracy:  " << accsum(zeroint()) << "\n";
    errstream() << "Precision: " << accsum(1) << "\n";
    errstream() << "Recall:    " << accsum(2) << "\n";
    errstream() << "F1 Score:  " << accsum(3) << "\n";
    errstream() << "AUC:       " << accsum(4) << "\n";
    errstream() << "Sparsity:  " << accsum(5) << "\n";
    errstream() << "Error:     " << accsum(6) << "\n";

    argvariables("&",1)("&",37) = accsum(zeroint());
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);

    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; i++ )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << "Recall error: "    << res << "\n";
    errstream() << "Class counts: "    << cnt << "\n";
    errstream() << "Confusion: "       << cfm << "\n";
    errstream() << "Classwise error: " << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+".recall.sum";
    std::string cntfilenamefull = logfile+".recall.cnt";
    std::string cfmfilenamefull = logfile+".recall.cfm";
    std::string clrfilenamefull = logfile+".recall.clr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);

    return;
}

void testCross(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, int numreps, int startpoint, int randcross, int numfolds, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    Vector<double> repres;
    Vector<double> cnt;
    Matrix<double> cfm;
    double res = 0.0;
    int i,j;
    Vector<Vector<gentype> > resh;
    Vector<Vector<gentype> > resg;
    Vector<Vector<gentype> > gvarres;

    res = calcCross(svmbase,numfolds,randcross,repres,cnt,cfm,resh,resg,gvarres,numreps,startpoint,recordxvar);

    for ( j = 0 ; j < numreps ; j++ )
    {
        std::ostringstream whichrep;

        whichrep << j;

        std::string resfilenamefull = logfile+".nfoldcross.res"+whichrep.str();
        std::string tarfilenamefull = logfile+".nfoldcross.tar"+whichrep.str();
        std::string clafilenamefull = logfile+".nfoldcross.cla"+whichrep.str();
        std::string varfilenamefull = logfile+".nfoldcross.var"+whichrep.str();

        writeLog(resg(j),resfilenamefull,getsetExtVar);
        writeLog(svmbase.y(),tarfilenamefull,getsetExtVar);
        writeLog(resh(j),clafilenamefull,getsetExtVar);

        if ( recordxvar )
        {
            writeLog(gvarres(j),varfilenamefull,getsetExtVar);
        }
    }

    if ( recordres )
    {
        argvariables("&",1)("&",5) = resh;
        argvariables("&",1)("&",6) = resg;
    }

    if ( randcross )
    {
        errstream() << "Random ";
    }

    if ( numreps != 1 )
    {
        errstream() << numreps << "-repeat ";
    }

    errstream() << "\n";

    Vector<double> accsum(7);

    double meanerror = 0.0;
    double meansqerror = 0.0;
    double varerror = 0.0;

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        Vector<double> accsumtmp(7);

        accsum = 0.0;

        for ( i = 0 ; i < resg.size() ; i++ )
        {
            measureAccuracy(accsumtmp,resg(i),resh(i),svmbase.y(),svmbase.d(),svmbase);

            accsum += accsumtmp;

            meanerror += accsumtmp(6);
            meansqerror += (accsumtmp(6)*accsumtmp(6));
        }

        accsum /= (double) resg.size();

        meanerror   /= (double) resg.size();
        meansqerror /= (double) resg.size();

        varerror = meansqerror-(meanerror*meanerror);
    }

    else
    {
        accsum = -1.0;

        accsum("&",zeroint()) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;

        varerror = 0.0;
    }

    errstream() << "Accuracy:   " << accsum(zeroint()) << "\n";
    errstream() << "Precision:  " << accsum(1) << "\n";
    errstream() << "Recall:     " << accsum(2) << "\n";
    errstream() << "F1 Score:   " << accsum(3) << "\n";
    errstream() << "AUC:        " << accsum(4) << "\n";
    errstream() << "Sparsity:   " << accsum(5) << "\n";
    errstream() << "Error:      " << accsum(6) << "\n";
    errstream() << "var(Error): " << varerror  << "\n";

    argvariables("&",1)("&",37) = accsum(zeroint());
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);
    argvariables("&",1)("&",46) = varerror;

    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; i++ )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << numfolds << "-fold error: " << res << "\n";
    errstream() << "Class counts: "    << cnt << "\n";
    errstream() << "Confusion: "       << cfm << "\n";
    errstream() << "Classwise error: " << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+".nfoldcross.sum";
    std::string cntfilenamefull = logfile+".nfoldcross.cnt";
    std::string cfmfilenamefull = logfile+".nfoldcross.cfm";
    std::string clrfilenamefull = logfile+".nfoldcross.clr";
    std::string rprfilenamefull = logfile+".nfoldcross.rpr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);
    writeLog(repres,rprfilenamefull,getsetExtVar);

    return;
}

void testSparSens(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, int minbad, int maxbad, double noisemean, double noisevar, int startpoint, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    Vector<double> repres;
    Vector<double> cnt;
    Matrix<double> cfm;
    double res = 0.0;
    int i,j;
    int numreps = maxbad-minbad+1;
    Vector<Vector<gentype> > resh;
    Vector<Vector<gentype> > resg;
    Vector<Vector<gentype> > gvarres;

    res = calcSparSens(svmbase,repres,cnt,cfm,resh,resg,gvarres,minbad,maxbad,noisemean,noisevar,startpoint,recordxvar);

    for ( j = 0 ; j < numreps ; j++ )
    {
        std::ostringstream whichrep;

        whichrep << j;

        std::string resfilenamefull = logfile+".sparsens.res"+whichrep.str();
        std::string tarfilenamefull = logfile+".sparsens.tar"+whichrep.str();
        std::string clafilenamefull = logfile+".sparsens.cla"+whichrep.str();
        std::string varfilenamefull = logfile+".sparsens.var"+whichrep.str();

        writeLog(resg(j),resfilenamefull,getsetExtVar);
        writeLog(svmbase.y(),tarfilenamefull,getsetExtVar);
        writeLog(resh(j),clafilenamefull,getsetExtVar);

        if ( recordxvar )
        {
            writeLog(gvarres(j),varfilenamefull,getsetExtVar);
        }
    }

    if ( recordres )
    {
        argvariables("&",1)("&",5) = resh;
        argvariables("&",1)("&",6) = resg;
    }

    errstream() << "Noise-Vector Test\n";

    Vector<double> accsum(7);

    double meanerror = 0.0;
    double meansqerror = 0.0;
    double varerror = 0.0;

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        Vector<double> accsumtmp(7);

        accsum = 0.0;

        for ( i = 0 ; i < resg.size() ; i++ )
        {
            measureAccuracy(accsumtmp,resg(i),resh(i),svmbase.y(),svmbase.d(),svmbase);

            accsum += accsumtmp;

            meanerror += accsumtmp(6);
            meansqerror += (accsumtmp(6)*accsumtmp(6));
        }

        accsum /= (double) resg.size();

        meanerror   /= (double) resg.size();
        meansqerror /= (double) resg.size();

        varerror = meansqerror-(meanerror*meanerror);
    }

    else
    {
        accsum = -1.0;

        accsum("&",zeroint()) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;

        varerror = 0.0;
    }

    errstream() << "Error over n: " << repres << "\n";
    errstream() << "Accuracy:   " << accsum(zeroint()) << "\n";
    errstream() << "Precision:  " << accsum(1) << "\n";
    errstream() << "Recall:     " << accsum(2) << "\n";
    errstream() << "F1 Score:   " << accsum(3) << "\n";
    errstream() << "AUC:        " << accsum(4) << "\n";
    errstream() << "Sparsity:   " << accsum(5) << "\n";
    errstream() << "Error:      " << accsum(6) << "\n";
    errstream() << "var(Error): " << varerror  << "\n";

    argvariables("&",1)("&",37) = accsum(zeroint());
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);
    argvariables("&",1)("&",46) = varerror;

//    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",2) = repres;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; i++ )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << "Noise Vector error: "     << repres << "\n";
    errstream() << "Noise Vector error ave: " << res << "\n";
    errstream() << "Class counts: "           << cnt << "\n";
    errstream() << "Confusion: "              << cfm << "\n";
    errstream() << "Classwise error: "        << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+".sparsens.sum";
    std::string cntfilenamefull = logfile+".sparsens.cnt";
    std::string cfmfilenamefull = logfile+".sparsens.cfm";
    std::string clrfilenamefull = logfile+".sparsens.clr";
    std::string rprfilenamefull = logfile+".sparsens.rpr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);
    writeLog(repres,rprfilenamefull,getsetExtVar);

    return;
}

void testFileVectors(int binaryRelabel, int singleDrop, std::string &logfile, const ML_Mutable &svmbase, std::string &tfilename, int reverse, int ignoreStart, int imax, int &firstsum, int coercetosingle, int coercefromsingle, const gentype &fromsingletarget, gentype &finalresult, int uselinesvector, Vector<int> &linesread, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), const SparseVector<gentype> &xtemplate)
{
    int pointsadded;

    Vector<gentype> ytest;
    Vector<gentype> ytestresh;
    Vector<gentype> ytestresg;
    Vector<gentype> yvarres;
    Vector<int> outkernind;

    // File may be very large.  Do not want to keep the whole lot in memory
    // at once.  So rather than load first, then test, we do a load-and-test
    // operation that does not keep x in memory.

    pointsadded = loadFileAndTest(svmbase,xtemplate,tfilename,reverse,ignoreStart,imax,-1,coercetosingle,coercefromsingle,fromsingletarget,binaryRelabel,singleDrop,uselinesvector,linesread,ytest,ytestresh,ytestresg,yvarres,recordxvar,outkernind);

    int i;
    Vector<int> cnt;
    Matrix<int> cfm;

    double res = assessResult(svmbase,cnt,cfm,ytestresh,ytest,outkernind);

    std::string resfilenamefull = logfile+"."+tfilename+".res";
    std::string tarfilenamefull = logfile+"."+tfilename+".tar";
    std::string clafilenamefull = logfile+"."+tfilename+".cla";
    std::string varfilenamefull = logfile+"."+tfilename+".var";

    writeLog(ytestresg,resfilenamefull,getsetExtVar);
    writeLog(ytest,tarfilenamefull,getsetExtVar);
    writeLog(ytestresh,clafilenamefull,getsetExtVar);

    if ( recordxvar )
    {
        writeLog(yvarres,varfilenamefull,getsetExtVar);
    }

    (void) pointsadded;

    if ( recordres )
    {
        argvariables("&",1)("&",5) = ytestresh;
        argvariables("&",1)("&",6) = ytestresg;
    }

    errstream() << "\n";

    Vector<double> accsum(7);

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        retVector<int> tmpva;

        measureAccuracy(accsum,ytestresg,ytestresh,ytest,oneintvec(ytestresg.size(),tmpva),svmbase);
    }

    else
    {
        accsum = -1;

        accsum("&",zeroint()) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;
    }

    errstream() << "Accuracy:  " << accsum(zeroint()) << "\n";
    errstream() << "Precision: " << accsum(1) << "\n";
    errstream() << "Recall:    " << accsum(2) << "\n";
    errstream() << "F1 Score:  " << accsum(3) << "\n";
    errstream() << "AUC:       " << accsum(4) << "\n";
    errstream() << "Sparsity:  " << accsum(5) << "\n";
    errstream() << "Error:     " << accsum(6) << "\n";

    argvariables("&",1)("&",37) = accsum(zeroint());
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);

    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; i++ )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << tfilename << "-test error: "    << res << "\n";
    errstream() << "Class counts: "    << cnt << "\n";
    errstream() << "Confusion: "       << cfm << "\n";
    errstream() << "Classwise error: " << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+"."+tfilename+".sum";
    std::string cntfilenamefull = logfile+"."+tfilename+".cnt";
    std::string cfmfilenamefull = logfile+"."+tfilename+".cfm";
    std::string clrfilenamefull = logfile+"."+tfilename+".clr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);

    return;
}

void testnegloglike(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    (void) getsetExtVar;
    (void) logfile;

    double res = 0.0;

    res = calcnegloglikelihood(svmbase.getMLconst());

    errstream() << "\n";

    argvariables("&",1)("&",2) = res;

    errstream() << "log likelihood: " << res << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    return;
}















// isscalar = nz if scalar
// isvector = nz if vector
// ispy     = 0 if exe
//          = 1 if python
//          = 2 if python, no socket, write 15 samples to lRateList.txt
// evalname = python/exe to be called
// sf       = scalar or function to be evaluated
// v        = vector

int pyorexeeval(int isscalar, int isvector, int ispy, const std::string &evalname, const gentype &sf, const Vector<gentype> &v, gentype &finalresult)
{
    NiceAssert( !isscalar || !isvector );

    if ( ( ispy == 2 ) || ( ispy == 3 ) )
    {
        double lr_min = 0.00001;
        double lr_max = 0.1;

        double lratesam[15];

        int i;

        for ( i = 0 ; i < 15 ; i++ )
        {
            gentype fxval;

            if ( isscalar )
            {
                fxval = sf;
            }

            else if ( isvector && !(v.infsize()) )
            {
                fxval = v(i);
            }

            else if ( isvector )
            {
                fxval = v(((double) i)/15);
            }

            else if ( ispy == 2 )
            {
                gentype ii;

                (ii.force_vector(1))("&",0) = ((double) i)/15;
                fxval = sf(ii);
            }

            else
            {
                gentype ii(((double) i)/15);

                fxval = sf(ii);
            }

            fxval.finalise();

            lratesam[i] = (double) fxval;
        }

        double rmin = lratesam[0];
        double rmax = lratesam[0];

        for ( i = 0 ; i < 15 ; i++ )
        {
            if ( lratesam[i] < rmin )
            {
                rmin = lratesam[i];
            }

            if ( lratesam[i] > rmax )
            {
                rmax = lratesam[i];
            }
        }

        if ( rmax-rmin < 1e-5 )
        {
            rmax = rmin+1e-5;
        }

        for ( i = 0 ; i < 15 ; i++ )
        {
            lratesam[i] = (((lratesam[i]-rmin)/(rmax-rmin))*(lr_max-lr_min))+lr_min;
        }

        remove("lRateList.txt");
        remove("pyres.txt");

        std::ofstream lrfile("lRateList.txt");

        for ( i = 0 ; i < 15 ; i++ )
        {
            lrfile << lratesam[i] << "\n";
            errstream() << "Rate " << i << ": " << lratesam[i] << "\n";
        }

        lrfile.close();

        svm_sleep(5);

        if ( ispy )
        {
            svm_pycall(evalname,0);
        }

        else
        {
            svm_execall(evalname,0);
        }

        svm_sleep(5);

        std::ifstream resfile("pyres.txt");

        resfile >> finalresult;

        resfile.close();

        return 0;
    }

    std::string sockis;
    awarestream *svmsocket = makeUnixSocket(sockis,1);

    if ( svmsocket )
    {
        // Curly bracket to ensure that out-of-scope doesn't mess things up!

        awarestream &svmsock = *svmsocket;

        std::istream svmsockin(&svmsock);
        std::ostream svmsockout(&svmsock);

        std::cout << "socket name: " << sockis << "\n";
        std::string clientstarter(evalname);
        clientstarter += " ";
        clientstarter += sockis;
        std::cout << "Start client: " << clientstarter << "\n";

        if ( ispy )
        {
            svm_pycall(clientstarter,1);
        }

        else
        {
            svm_execall(clientstarter,1);
        }

        gentype ind;

        while ( !ind.isValNull() )
        {
            svmsockin >> ind;

            if ( !ind.isValNull() )
            {
                gentype fxval;

                if ( isscalar )
                {
                    fxval = sf;
                }

                else if ( isvector && !(v.infsize()) )
                {
                    fxval = v((int) ind);
                }

                else if ( isvector )
                {
                    fxval = v(ind);
                }

                else
                {
                    gentype ii;

                    (ii.force_vector(1))("&",0) = ind;
                    fxval = sf(ii);
                }

                fxval.finalise();
                //errstream() << "received " << ind << ", returning " << fxval << ", ";
                svmsockout << fxval << "\n"; // newline flushes the buffer
                //errstream() << "done.\n";
            }
        }

        svmsockin >> finalresult;
        errstream() << "overall result = " << finalresult << "\n";
    }

    else
    {
        return 1;
    }

    delete svmsocket;

    return 0;
}






















































int loadDataFromMatlab(const std::string &xmatname, const std::string &ymatname, Vector<SparseVector<gentype> > &x, Vector<gentype> &dz, char targtype, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    gentype Xmat;
    gentype ymat;

    Xmat.makeString(xmatname);
    ymat.makeString(ymatname);

    gentype matind;

    matind.force_matrix();

    getsetExtVar(Xmat,matind,-1);
    getsetExtVar(ymat,matind,-1);

    NiceAssert( Xmat.numRows() == ymat.numRows() );

    int N = Xmat.numRows();

    int dimx = Xmat.numCols();
    int dimy = ymat.numCols();

    dz.resize(N);
    x.resize(N);

    gentype temp;

    int i,j;

    for ( i = 0 ; i < N ; i++ )
    {
        x("&",i).zero();

        for ( j = 0 ; j < dimx ; j++ )
        {
            x("&",i)("&",j) = (Xmat.dir_matrix())(i,j);
        }

        switch ( targtype )
        {
            case 'B':
            {
                dz("&",i).force_null();

                break;
            }

            case 'Z':
            {
                NiceAssert( dimy == 1 );

                dz("&",i).force_int() = (int) (ymat.force_matrix())(i,0);

                break;
            }

            case 'V':
            {
                if ( dimy > 0 )
                {
                    dz("&",i).force_vector().resize(dimy);

                    for ( j = 0 ; j < dimy ; j++ )
                    {
                        dz("&",i)("&",j) = (ymat.force_matrix())(i,j);
                    }
                }

                break;
            }

            default:
            {
                NiceAssert( dimy == 1 );

                dz("&",i).force_double() = (ymat.force_matrix())(i,0);

                break;
            }
        }
    }

    return N;
}























void printhelp(std::ostream &output, int basic, int advanced)
{
    output << ( ( basic || advanced ) ? "SVMheavy 7.0: an SVM (and more) implementation by Alistair Shilton.           \n" : "" );
    output << ( ( basic || advanced ) ? "============                                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Copyright: all rights reserved.                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Usage:     svmheavyv7 {options}                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Each line  in the  training/testing files  contains a  single sample  with the\n" : "" );
    output << ( ( basic || advanced ) ? "following general format (where  values in {} are  optional; and != and  ! are\n" : "" );
    output << ( ( basic || advanced ) ? "functionally equivalent):                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "{>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} {sVAL} {SVAL} x                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Classification:                                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} must not be included.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y=-1,0,+1,+2,... is classification (0 for test/unknown).             \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - {sVAL} or {SVAL} sets the empirical sigma scale.                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Regression:                                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} defines constraint type (optional, default is =):       \n" : "" );
    output << ( ( basic || advanced ) ? "         o >: g(x) > y                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         o =: g(x) = y                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         o <: g(x) < y                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         o ! or !=: ignore this vector (it may be referenced elsewhere).      \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y gives the target value for this point.                             \n" : "" );
    output << ( ( basic || advanced ) ? "       - if y is anion, vector or gentype then >,< not defined.               \n" : "" );
    output << ( ( basic || advanced ) ? "       - if y is gentype then it must lie in the basis set u_i.               \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the epsilon insensitivity scale.               \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Similarity learning:                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Format as for  regression, except that x takes  the form xa ~ xb (the\n" : "" );
    output << ( ( basic || advanced ) ? "         character ~ acts as a separator), so the constraint is on g({xa,xb}).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Multi-instance learning:                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Format as for regression,  but in this case x has  the general format\n" : "" );
    output << ( ( basic || advanced ) ? "         xa ~ xb ~ ... ~ xn  (the   character  ~  acts  as  a  separator),  so\n" : "" );
    output << ( ( basic || advanced ) ? "         constraint is on g({xa,xb,...,xn}).                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Single class:                                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} must not be included.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must not be included.                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Auto-Encoder:                                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} must not be included.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must not be included (target is same as input x).                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Classification with Scoring:                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} must not be included.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y = integer indicates a standard class membership constraint.        \n" : "" );
    output << ( ( basic || advanced ) ? "       - y = vector indicates a score.  Each element of the vector indicates a\n" : "" );
    output << ( ( basic || advanced ) ? "         score on a different \"axis\",  where each axis is used to derive a set\n" : "" );
    output << ( ( basic || advanced ) ? "         of rank constraints g(x_i) - g(x_j) > 1. Use null as a placeholder if\n" : "" );
    output << ( ( basic || advanced ) ? "         the x is not scored on a given axis.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Regression with Scoring:                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} may be included for scalar constraints.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - {=,!} only for scoring constraints.                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y = scalar indicates a standard constraint g(x) ? y.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - y = vector indicates a score as per classification with scoring.     \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Gentype regression:                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} must not be included.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y can be real, vector, anion, string, equation, set or graph (as long\n" : "" );
    output << ( ( basic || advanced ) ? "         as the concept of (inner) product (possibly numeric) can be defined).\n" : "" );
    output << ( ( basic || advanced ) ? "       - y is projected onto the \"u\" basis (see -Aby etc below).              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Planar regression:                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} defines constraint type:                                \n" : "" );
    output << ( ( basic || advanced ) ? "         o >: v'.g(x) > y                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         o =: v'.g(x) = y                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         o <: v'.g(x) < y                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         o ! or !=: ignore this vector (it may be referenced elsewhere).      \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y is the scalar-valued target.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector of form x ::: 7:v.  v may either be a vector\n" : "" );
    output << ( ( basic || advanced ) ? "         or an integer index to an (output) basis vector u_i. If v is a vector\n" : "" );
    output << ( ( basic || advanced ) ? "         then it must lie in the space spanned by the output basis vectors.   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Multi-expert ranking:                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Like planar  regression but automatically  tunes basis u_i.  The idea\n" : "" );
    output << ( ( basic || advanced ) ? "         is  that  many   \"experts\"  give  their  opinions   (usually  ranking\n" : "" );
    output << ( ( basic || advanced ) ? "         constraints, but not necessarily)  and we want to construct a machine\n" : "" );
    output << ( ( basic || advanced ) ? "         that synthesises these (not  always compatible) sources.  Each expert\n" : "" );
    output << ( ( basic || advanced ) ? "         is assigned to a  particular basis vector u_i, and  the inner product\n" : "" );
    output << ( ( basic || advanced ) ? "         between two such vectors <u_i,u_j> reflects how similar experts i and\n" : "" );
    output << ( ( basic || advanced ) ? "         j are in  their assessments.  The  machine attempts to  automatically\n" : "" );
    output << ( ( basic || advanced ) ? "         tune these basis vectors - that is,  to learn expert similarity - and\n" : "" );
    output << ( ( basic || advanced ) ? "         thereby  combine them  (essentially transfer  learning).   Evaluating\n" : "" );
    output << ( ( basic || advanced ) ? "         g can  either define  which  \"expert\"  x is  aligned with -  that is,\n" : "" );
    output << ( ( basic || advanced ) ? "         g(x ::: 7:i) for expert i - or give all alignments - that is, g(x).  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The format of the x vector may be  either sparse or nonsparse.  Sparse vectors\n" : "" );
    output << ( ( basic || advanced ) ? "vectors have the form  (noting that commas can be  used instead of or combined\n" : "" );
    output << ( ( basic || advanced ) ? "with whitespace):                                                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "<feature1>:<valueF1> <feature2>:<valueF2> ... <featureN>:<valueFN>            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "with all other values being assumed zero.  Sparse vectors have the form:      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "<value1> <value2> ... <valueN>                                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The values may, depending  on context, be real, anionic,  vector, matrix, set,\n" : "" );
    output << ( ( basic || advanced ) ? "strings (encompassed in  quotes \", or a single non-numeric  character which is\n" : "" );
    output << ( ( basic || advanced ) ? "read as a string by default) or equations  (use of symbolic features may incur\n" : "" );
    output << ( ( basic || advanced ) ? "a significant performance hit).  Notes:                                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two vectors is the inner-product of the vectors.      \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two strings is 1 if they are identical, 0 otherwise.  \n" : "" );
    output << ( ( basic || advanced ) ? "       - The sum of two strings is the concatenation of them.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two sets is the number of elements in common.         \n" : "" );
    output << ( ( basic || advanced ) ? "       - The sum of two sets is the union, the difference the intersection.   \n" : "" );
    output << ( ( basic || advanced ) ? "       - Simple equations like sqrt(20) will be evaluated directly.           \n" : "" );
    output << ( ( basic || advanced ) ? "       - Distributions  (eg grand(0,1)) are  processed by the kernel  and then\n" : "" );
    output << ( ( basic || advanced ) ? "         sampled as per Muandet et al, Learning from Distributions via Support\n" : "" );
    output << ( ( basic || advanced ) ? "         Measure Machines (so a support  measure machine (SMM) can be run as a\n" : "" );
    output << ( ( basic || advanced ) ? "         standard SVM with distributions in the x vectors).                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - Functions  are evaluated  like distributions,  where you  provide the\n" : "" );
    output << ( ( basic || advanced ) ? "         distribution of relevant variables to the kernel.                    \n" : "" );
    output << ( ( basic || advanced ) ? "       - Infinite-dimensional  vectors  (functions  treated  as  vectors)  are\n" : "" );
    output << ( ( basic || advanced ) ? "         written [[ : f : d ]],  where f is a function of  x (eg sin(x)) and d\n" : "" );
    output << ( ( basic || advanced ) ? "         d the dimension  of x.  Domain of  x is [0,1]^d.  These  can be  used\n" : "" );
    output << ( ( basic || advanced ) ? "         like vectors: norms,  inner products  etc work  exactly as  you would\n" : "" );
    output << ( ( basic || advanced ) ? "         expect in L2 space.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - RKHS vectors  can't be  entered directly,  but they  do occur  (eg in\n" : "" );
    output << ( ( basic || advanced ) ? "         Bayesian  optimisation).  Inner products  and norms on these occur in\n" : "" );
    output << ( ( basic || advanced ) ? "         RKHS space (not L2),  but mixed inner  products with  inf-dim vectors\n" : "" );
    output << ( ( basic || advanced ) ? "         are calculated in L2.  Note  that you can't  calculate sum/difference\n" : "" );
    output << ( ( basic || advanced ) ? "         between an RKHS vector and an inf-dim vector.                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - Scalar functions are  also evaluated.  These are function of the form\n" : "" );
    output << ( ( basic || advanced ) ? "         @(i,j,n):fn, where  fn is a  function, (i,j) defines  the variable in\n" : "" );
    output << ( ( basic || advanced ) ? "         the function being treated as a scalar.  The result of the product of\n" : "" );
    output << ( ( basic || advanced ) ? "         two scalar functions is the inner  product int_0^1 conj(f(x)).g(x) dx\n" : "" );
    output << ( ( basic || advanced ) ? "         (if i != 0 and/or  j !=0 then x  is replaced  by var(i,j)),  which is\n" : "" );
    output << ( ( basic || advanced ) ? "         numerically approximated over n steps.  The product of a vector and a\n" : "" );
    output << ( ( basic || advanced ) ? "         function  in this  context is  the  same,  assuming the  vector is  a\n" : "" );
    output << ( ( basic || advanced ) ? "         sampled version of the function on [0,1].  Default values for i,j and\n" : "" );
    output << ( ( basic || advanced ) ? "         n are 0,0 and 100, respectively.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "For example the following training sets all define variants of the classic XOR\n" : "" );
    output << ( ( basic || advanced ) ? "problem:                                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Standard (y x format):                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1   -1 -1                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "         1    -1 1                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "         1    1  -1                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "         -1   1  1                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Vector x (y x format):                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    [ -1 1 ] [ -1 1 ]                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         1     [ -1 1 ] [ 1 -1 ]                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         1     [ 1 -1 ] [ -1 1 ]                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    [ 1 -1 ] [ 1 -1 ]                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Symbolic x (y x format):                                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    \"a\" \"a\"                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         1     \"a\" \"b\"                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         1     \"b\" \"a\"                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    \"b\" \"b\"                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Set x with symbolic elements (y x format):                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    { \"cat\" \"horse\" }     { \"car\" }                                \n" : "" );
    output << ( ( basic || advanced ) ? "         1     { \"cat\" \"chicken\" }   { \"lemons\" \"x\"  }                        \n" : "" );
    output << ( ( basic || advanced ) ? "         1     { \"wallaby\" \"mouse\" } { \"walrus\" \"car\" }                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    { \"mouse\" \"wombat\" }  { \"q\" \"lemons\" }                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Distribution x (y x format):                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    grand(-1,0.1) grand(-1,0.1)                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         1     grand(-1,0.1) grand(1,0.1)                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         1     grand(1,0.1)  grand(-1,0.1)                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    grand(1,0.1)  grand(1,0.1)                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Scalar function x (y x format):                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    @():sin(2*pi()*x) @():sin(2*pi()*x)                            \n" : "" );
    output << ( ( basic || advanced ) ? "         1     @():sin(2*pi()*x) @():cos(2*pi()*x)                            \n" : "" );
    output << ( ( basic || advanced ) ? "         1     @():cos(2*pi()*x) @():sin(2*pi()*x)                            \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    @():cos(2*pi()*x) @():cos(2*pi()*x)                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "More generally x has the form (using {...} to denote optional arguments here):\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "x {~ x1 {~ x2 ...}} {: xb {~ xb1 {~ xb2 ...}}} {:: e} {::: a}                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "where xb, e and a  have the same format as x.  These  define rank constraints,\n" : "" );
    output << ( ( basic || advanced ) ? "gradient constraints, tuple format and augmented data as follows:             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Rank constraints: some  models (SVMs, GPs,  LSVs) allow the  inclusion of rank\n" : "" );
    output << ( ( basic || advanced ) ? "         constraints - basically rather then enforcing g(x) ? y, they enforce:\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         g(xa) - g(xb) ? y                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         (where ? depends on model). In the training file these have the form:\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} x : xb                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         where x and xb have the same format as x above. Notes:               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - you cannot  combine gradient  constraints and rank  constraints in  a\n" : "" );
    output << ( ( basic || advanced ) ? "         single vector x (it's ambiguous whether the constraint's on x or xb).\n" : "" );
    output << ( ( basic || advanced ) ? "       - when evaluating the model g(x : xb) = g(x) - g(xb)                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - this enables ordinal regression.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Gradient constraints: some  models (scalar  regression SVM,  scalar regression\n" : "" );
    output << ( ( basic || advanced ) ? "         LS-SVM, scalar regression GP) allow the inclusion of grad constraints\n" : "" );
    output << ( ( basic || advanced ) ? "         of the form:                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         (e'.d/dx) g(x) ? y                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         making this a  constraint on the direction  derivative of the trained\n" : "" );
    output << ( ( basic || advanced ) ? "         machine.  In the training file these have the form:                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} x :: e                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         where e has the same format as x.   Constraints on higher derivatives\n" : "" );
    output << ( ( basic || advanced ) ? "         may also be applied using  augmented data (a_6) as described shortly.\n" : "" );
    output << ( ( basic || advanced ) ? "         Note that:                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - you cannot  combine gradient  constraints and rank  constraints in  a\n" : "" );
    output << ( ( basic || advanced ) ? "         single vector x (it's ambiguous whether the constraint's on x or xb).\n" : "" );
    output << ( ( basic || advanced ) ? "       - when evaluating the model g(x :: e) = (e'.d/dx) g(x).                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Multi-Instance  format:  most  models  can  be  used  for  multi-instance  and\n" : "" );
    output << ( ( basic || advanced ) ? "         similarity (kernel function)  learning.   In this case  vectors x are\n" : "" );
    output << ( ( basic || advanced ) ? "         replaced by sets {x0,x1,...,xm-1},  and g(x) becomes  g({x0,x1,...}).\n" : "" );
    output << ( ( basic || advanced ) ? "         In this case the training file has the form (eg m = 2):              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} x0 ~ x1 ~ x2 ~ ... ~ xn \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         where n is the number of vectors in the set.  Note that:             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - the option -kn does not work with tuple format (but -knn does).      \n" : "" );
    output << ( ( basic || advanced ) ? "       - n must not exceed 4095 (4096 vectors - defined in sparsevector.h).   \n" : "" );
    output << ( ( basic || advanced ) ? "       - you cannot combine gradient  constraints and multi-instance data in a\n" : "" );
    output << ( ( basic || advanced ) ? "         single vector x.                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Multi-Task format: multi-task learning  via ICM kernels is  possible using the\n" : "" );
    output << ( ( basic || advanced ) ? "         same format  as multi-instance  learning.  See -XT  for  information.\n" : "" );
    output << ( ( basic || advanced ) ? "         Multi-task and multi-instance can be  combined, but interpretation is\n" : "" );
    output << ( ( basic || advanced ) ? "         non-trivial: see code for details.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Augmented data: in some cases additional data may be included using the form: \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} x ::: a                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         where elements in a have the following interpretation:               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_0:  replace x with  vector x_{a_0}.  If this is a vector  then x is\n" : "" );
    output << ( ( basic || advanced ) ? "               replaced  by a tuple [ x_{a_0_0}  x_{a_0_1} ... ]. nul for dft.\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_1:  replace xb with vector x_{a_1}.  If this is a vector then xb is\n" : "" );
    output << ( ( basic || advanced ) ? "               replaced  by a tuple [ x_{a_1_0}  x_{a_1_1} ... ]. nul for dft.\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_2:  replace e with  vector x_{a_2}. null for default.              \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_3:  reserved (like a_7, but refers to gentype regr basis. nul dft).\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_4:  diagonal kernel over-ride.  If  present in x1,x2 then kernel is\n" : "" );
    output << ( ( basic || advanced ) ? "               replaced by delta_{x_1(a_4), x_2(a_4)} (0 if only one,nul dft).\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_5:  reserved (null for default).                                     \n" : "" );
//    output << ( ( basic || advanced ) ? "       - a_5:  interpret a_0  vector as  a  metrical  constraint - that  is, a\n" : "" );
//    output << ( ( basic || advanced ) ? "               training constraint on the tuple (x0,x1) taking the form:      \n" : "" );
//    output << ( ( basic || advanced ) ? "               g(x0,x0) + g(x1,x1) - 2g(x0,x1) ? y                            \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_6:  define order  of gradient constraint (1 if  unspecified).  Grad\n" : "" );
    output << ( ( basic || advanced ) ? "               constraint   becomes   (e'.dn/dxn) g(x) ? y,   where   dn/dxn =\n" : "" );
    output << ( ( basic || advanced ) ? "               kronecker_power(d/dx,n).  For  example if xdim = 2 then this is\n" : "" );
    output << ( ( basic || advanced ) ? "               d2/dx2 = [ d2/dx0.dx0 ; d2/dx0.dx1 ; d2/dx1.dx0 ; d2/dx1.dx1 ].\n" : "" );
    output << ( ( basic || advanced ) ? "               You can evaluate  g(x) without  specifying e  in some models to\n" : "" );
    output << ( ( basic || advanced ) ? "               find  the gradient  dn/dxn g(x)  (and likewise  (co)variances).\n" : "" );
    output << ( ( basic || advanced ) ? "               Null for default.                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_7:  vector (or index) part of planar constraint.  Null for default.\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_8:  set 1 to treat distributions as samples from sets, so distances\n" : "" );
    output << ( ( basic || advanced ) ? "               are to the nearest in set, and  inner products to most similar.\n" : "" );
    output << ( ( basic || advanced ) ? "               Null for default.                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_{100q}: for ranking constraints in the vector-target case {>,=,...}\n" : "" );
    output << ( ( basic || advanced ) ? "               applies to  all elements of the target.  To  override for dim q\n" : "" );
    output << ( ( basic || advanced ) ? "               for a given training pair use  this, specifically a_{100q} = -1\n" : "" );
    output << ( ( basic || advanced ) ? "               means <, 0 means !,+1 means > and +2 means =.                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Pipes: - Most output sent to standard error.                                  \n" : "" );
    output << ( (          advanced ) ? "       - Help sent to standard out.                                           \n" : "" );
    output << ( (          advanced ) ? "       - All other output sent direct to files.                               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Matlab/mex: - Standard out and standard error redirected to mexprintf.        \n" : "" );
    output << ( (          advanced ) ? "       - Logfiles written and contents mirrored by matlab variables.          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Arguments: in the descriptions below there are three types of arguments:      \n" : "" );
    output << ( (          advanced ) ? "       - Strings:  indicated by  leading $  (eg $file,  $fn).  These  must not\n" : "" );
    output << ( (          advanced ) ? "         contain whitespace, but note that any occurance of _ will be replaced\n" : "" );
    output << ( (          advanced ) ? "         by a space.                                                          \n" : "" );
    output << ( (          advanced ) ? "       - Sets: held in curly  brackets {} (eg {0,1,2}).  The  argument must be\n" : "" );
    output << ( (          advanced ) ? "         one of the options in the list.                                      \n" : "" );
    output << ( (          advanced ) ? "       - Variables: everything  else.  These  may be integers,  reals, vectors\n" : "" );
    output << ( (          advanced ) ? "         (eg [ 1 2 3.2 ] or [ 0:0.12 1:39 5:2 ]) or anions (eg 1.0i) depending\n" : "" );
    output << ( (          advanced ) ? "         on context.                                                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Variable evaluation: a useful feature here  is that all variable arguments are\n" : "" );
    output << ( (          advanced ) ? "         evaluated.  That is,  you can enter them as  equations - for example,\n" : "" );
    output << ( (          advanced ) ? "         if you enter  2/3 this will  be evaluated  to 0.66666 -  and moreover\n" : "" );
    output << ( (          advanced ) ? "         those equations can have arguments  of the form var(i,j), where i and\n" : "" );
    output << ( (          advanced ) ? "         j are  non-negative integers (and  shortcuts x,y,z,v,w,g =  var(0,0),\n" : "" );
    output << ( (          advanced ) ? "         var(0,1),...,var(0,5) and h= var(42,42) (the current ML)). A complete\n" : "" );
    output << ( (          advanced ) ? "         list is  included (-??v).                                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Global functions: these  may be used to  retrieve data  about ML  models.  Use\n" : "" );
    output << ( (          advanced ) ? "         fnA(h,i) to  retrieve value i  (eg C,  if i = 0) for the  current ML,\n" : "" );
    output << ( (          advanced ) ? "         which is stored in variable h).  Replace h with specific ML number to\n" : "" );
    output << ( (          advanced ) ? "         specify a different ML.  A complete list is included (-??v).         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Variables: a complete  list of vars is  given at the end  of this help.  Three\n" : "" );
    output << ( (          advanced ) ? "         functions  are  particularly  note-worthy  with regard  to variables,\n" : "" );
    output << ( (          advanced ) ? "         namely -fV n $f  and -fW n f,  which let  you set  var(0,n) = f (with\n" : "" );
    output << ( (          advanced ) ? "         -fV being unevaluated  and -fW evaluated), and  -echo x, which simply\n" : "" );
    output << ( (          advanced ) ? "         evaluates x and echoes it to standard out.                           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Comments: comments  can be  included in  argument streams,  which is  handy if\n" : "" );
    output << ( (          advanced ) ? "         you want to put commands in a  file.  Comments are C style - that is,\n" : "" );
    output << ( (          advanced ) ? "         /* comment goes here and gets ignored */.  Note that comments may not\n" : "" );
    output << ( (          advanced ) ? "         interupt commands, so for example  the sequence \"-c 1 /* set c */\" is\n" : "" );
    output << ( (          advanced ) ? "         good, but \"-c /* set c */\" 1 is a syntax error.                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Help options (run when encountered):                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -?              - basic help.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         -??             - advanced help.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         -??k            - list of all available kernel functions.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -??v            - variable assignment table.                         \n" : "" );
    output << ( ( basic || advanced ) ? "         -??g            - variable types, functions etc.                     \n" : "" );
    output << ( ( basic || advanced ) ? "         -???            - print blank lines to standard error.               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "General options (run first):                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -v   {0,1}      - verbosity level (default 1).                       \n" : "" );
    output << ( ( basic || advanced ) ? "                           0 = minimal - cerr feedback only.                  \n" : "" );
    output << ( ( basic || advanced ) ? "                           1 = normal - write details to logfile.log.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -L   $file      - string used to  derive logfile  and ML description.\n" : "" );
    output << ( ( basic || advanced ) ? "                           Files will be file.log, file.svm etc.              \n" : "" );
    output << ( (          advanced ) ? "         -LL  file       - string used to  derive logfile  and ML description.\n" : "" );
    output << ( (          advanced ) ? "                           Files will be file.log, file.svm etc.              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Multi-ML options (after general options):                                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** SVMHeavy can work on  arbitrarily many MLs at **         \n" : "" );
    output << ( (          advanced ) ? "                  ** once.  Each  ML is assigned  an index n >= 1. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** The  working  ML  (default  index 1)  defines **         \n" : "" );
    output << ( (          advanced ) ? "                  ** which of  these is  operated on by  all other **         \n" : "" );
    output << ( (          advanced ) ? "                  ** commands.  Note that copying and swapping may **         \n" : "" );
    output << ( (          advanced ) ? "                  ** be very  slow for large MLs.  The  current ML **         \n" : "" );
    output << ( (          advanced ) ? "                  ** is h.                                         **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -qR  n          - delete ML  n completely.  If  n is working  ML then\n" : "" );
    output << ( (          advanced ) ? "                           return to  clean-slate state (no  data, all default\n" : "" );
    output << ( (          advanced ) ? "                           settings).                                         \n" : "" );
    output << ( (          advanced ) ? "         -qc  n m        - overwrite ML n with copy of ML m.                  \n" : "" );
    output << ( (          advanced ) ? "         -qs  n m        - swap ML n and ML m.                                \n" : "" );
    output << ( (          advanced ) ? "         -qw  n          - set ML n  as current (working) ML.   If n  not used\n" : "" );
    output << ( (          advanced ) ? "                           previously then create clean-slate ML first.       \n" : "" );
    output << ( (          advanced ) ? "         -qpush n        - push current ML index onto stack, run -qw n.       \n" : "" );
    output << ( (          advanced ) ? "         -qpop           - push ML index n off stack, run -qw n.              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Pre-Setup options (after multi-ML options):                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -zl  $file      - preload the ML from file.                          \n" : "" );
    output << ( ( basic || advanced ) ? "         -z   {...}      - ML type:                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Support Vector Machines (SVM):                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           s - SVM: single class.                             \n" : "" );
    output << ( ( basic || advanced ) ? "                           c - SVM: binary classification (default).          \n" : "" );
    output << ( ( basic || advanced ) ? "                           m - SVM: multiclass classification.                \n" : "" );
    output << ( ( basic || advanced ) ? "                           r - SVM: scalar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           v - SVM: vector regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           a - SVM: anionic regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                           u - SVM: cyclic regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           g - SVM: gentype regression (any target).          \n" : "" );
    output << ( ( basic || advanced ) ? "                           e - SVM: auto-encoding SVM.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           p - SVM:*density estimation.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                           t - SVM: pareto frontier SVM.                      \n" : "" );
    output << ( ( basic || advanced ) ? "                           l - SVM: binary scoring (zero bias by default).    \n" : "" );
    output << ( ( basic || advanced ) ? "                           o - SVM: scalar regression with scoring.           \n" : "" );
    output << ( ( basic || advanced ) ? "                           q - SVM: vector regression with scoring.           \n" : "" );
    output << ( ( basic || advanced ) ? "                           i - SVM: planar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           h - SVM: multi-expert ranking.                     \n" : "" );
    output << ( ( basic || advanced ) ? "                           j - SVM: multi-expert binary classification.       \n" : "" );
    output << ( ( basic || advanced ) ? "                           b - SVM: similarity learning.                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               *Uses  1-norm cost,  kernel can be  non-Mercer.\n" : "" );
    output << ( ( basic || advanced ) ? "                                Recommend setting r0 = N, norm with -Sna.     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Least-squares SVMs (LSV):                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsr - LSV: scalar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsv - LSV: vector regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsa - LSV: anionic regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsg - LSV: gentype regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsi - LSV: planar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         lso - LSV: scalar regression with scoring.           \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsq - LSV: vector regression with scoring.           \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsh - LSV: multi-expert ranking.                     \n" : "" );
    output << ( ( basic || advanced ) ? "                         lse - LSV: auto-encoding machine.                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Super-Sparse support vector machines (SSV):    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         sss - SSV: single class.                             \n" : "" );
    output << ( ( basic || advanced ) ? "                         ssc - SSV: binary classification.                    \n" : "" );
    output << ( ( basic || advanced ) ? "                         ssr - SSV: scalar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Gaussian processes (GPR):                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpr - GPR: gaussian process scalar regression.       \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpv - GPR: gaussian process vector regression.       \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpa - GPR: gaussian process anionic regression.      \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpg - GPR: gaussian process gentype regression.      \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpc - GPR: gaussian process binary classification.*  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               *Uses expectation propagation (EP).            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                               Type-II Multi-Layer Kernel Machines (MLM):     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                         mlr - MLM: Type-II MLK machine scalar regression.    \n" : "" );
    output << ( (          advanced ) ? "                         mlc - MLM: Type-II MLK machine scalar regression.    \n" : "" );
    output << ( (          advanced ) ? "                         mlv - MLM: Type-II MLK machine scalar regression.    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                         NB: MLM  is  extremely  experimental.   Binary  might\n" : "" );
    output << ( (          advanced ) ? "                             work (probably not), but none of the others do.  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               K-nearest-neighbour networks (KNN):            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         knc - KNN: binary classification.                    \n" : "" );
    output << ( ( basic || advanced ) ? "                         knm - KNN: multiclass classification.                \n" : "" );
    output << ( ( basic || advanced ) ? "                         knr - KNN: scalar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         knv - KNN: vector regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         kna - KNN: anionic regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                         kng - KNN: gentype regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                         kne - KNN: auto-encoder.                             \n" : "" );
    output << ( ( basic || advanced ) ? "                         knp - KNN: density estimation.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                         kne - KNN: auto-encoding machine.                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               One-layer Neural Networks (ONN):               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         onc - ONN: binary classification.                    \n" : "" );
    output << ( ( basic || advanced ) ? "                         onr - ONN: scalar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         onv - ONN: vector regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         ona - ONN: anionic regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                         one - ONN: auto-encoding machine.                    \n" : "" );
    output << ( ( basic || advanced ) ? "                         ong - ONN: gentype machine.                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Improvement measures (IMPs):                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                          ei - IMP: expected (hypervolume) improvement.       \n" : "" );
    output << ( ( basic || advanced ) ? "                         svm - IMP: 1-norm 1-class modded SVM mono-surrogate. \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Learning blocks and glue (BLK):                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         nop - BLK: NOP machine.                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         mer - BLK: Mercer kernel inheritance block.          \n" : "" );
    output << ( ( basic || advanced ) ? "                         con - BLK: consensus machine.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         fna - BLK: user function machine (elementwise).*     \n" : "" );
    output << ( ( basic || advanced ) ? "                         fnb - BLK: user function machine (vectorwise).*      \n" : "" );
    output << ( ( basic || advanced ) ? "                         mxa - BLK: mex function machine (elementwise).       \n" : "" );
    output << ( ( basic || advanced ) ? "                         mxb - BLK: mex function machine (vectorwise).        \n" : "" );
    output << ( ( basic || advanced ) ? "                          io - BLK: user I/O machine.                         \n" : "" );
    output << ( ( basic || advanced ) ? "                         sys - BLK: system call machine.                      \n" : "" );
    output << ( ( basic || advanced ) ? "                         avr - BLK: scalar averaging machine.                 \n" : "" );
    output << ( ( basic || advanced ) ? "                         avv - BLK: vector averaging machine.                 \n" : "" );
    output << ( ( basic || advanced ) ? "                         ava - BLK: anionic averaging machine.                \n" : "" );
    output << ( ( basic || advanced ) ? "                         fcb - BLK: function callback (do not use).           \n" : "" );
    output << ( ( basic || advanced ) ? "                         ber - BLK: Bernstein basis polynomial.               \n" : "" );
    output << ( ( basic || advanced ) ? "                         bat - BLK: Battery model.**                          \n" : "" );
    output << ( ( basic || advanced ) ? "                         ker - BLK: kernel specialisation.                    \n" : "" );
    output << ( ( basic || advanced ) ? "                         mba - BLK: multi-block sum.                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                    (g(x), kernel transfer ave over multi MLs)\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               *This function can be a  distribution, in which\n" : "" );
    output << ( ( basic || advanced ) ? "                                case g(x) is a sample from  this distribution.\n" : "" );
    output << ( ( basic || advanced ) ? "                                You can \"freeze\"  this (ie. take a  sample and\n" : "" );
    output << ( ( basic || advanced ) ? "                                return it consistently afterwards) by sampling\n" : "" );
    output << ( ( basic || advanced ) ? "                                using -St.                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                              **Battery model as per Cer1.  Evaluation is:    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                g([3 i(x) v s])= time to reach target  voltage\n" : "" );
    output << ( ( basic || advanced ) ? "                                                with charging current i(t). If\n" : "" );
    output << ( ( basic || advanced ) ? "                                                target not reached then result\n" : "" );
    output << ( ( basic || advanced ) ? "                                                is max time + s.Verr.         \n" : "" );
    output << ( ( basic || advanced ) ? "                                g([2 v(x) v s])= time to reach target  voltage\n" : "" );
    output << ( ( basic || advanced ) ? "                                                with charging voltage v(t).   \n" : "" );
    output << ( ( basic || advanced ) ? "                                g([1 p(x) v s])= time to reach target  voltage\n" : "" );
    output << ( ( basic || advanced ) ? "                                                with charging power p(t).     \n" : "" );
    output << ( ( basic || advanced ) ? "                                g([0 i(x) v s])= time to reach target  voltage\n" : "" );
    output << ( ( basic || advanced ) ? "                                                with discharging current i(t).\n" : "" );
    output << ( ( basic || advanced ) ? "                                g([-1 t i v s]) = given  vectors    time  (t),\n" : "" );
    output << ( ( basic || advanced ) ? "                                                current (i),  voltage (v), for\n" : "" );
    output << ( ( basic || advanced ) ? "                                                current  charging, return  how\n" : "" );
    output << ( ( basic || advanced ) ? "                                                close the simulation is to the\n" : "" );
    output << ( ( basic || advanced ) ? "                                                given data (L2 voltage error).\n" : "" );
    output << ( ( basic || advanced ) ? "                                g([ -2 dfile m N s ]) = lie g([ -1 ... ]), but\n" : "" );
    output << ( ( basic || advanced ) ? "                                                data is in a file dfile.  m is\n" : "" );
    output << ( ( basic || advanced ) ? "                                                the startpoint in  the file, N\n" : "" );
    output << ( ( basic || advanced ) ? "                                                is the  max number  of obs  to\n" : "" );
    output << ( ( basic || advanced ) ? "                                                compare (-1 for all), s is the\n" : "" );
    output << ( ( basic || advanced ) ? "                                                scalarisation.  Result is:    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                s*earlystop + ave_error       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                where ave_error is the average\n" : "" );
    output << ( ( basic || advanced ) ? "                                                voltage  error  and  earlystop\n" : "" );
    output << ( ( basic || advanced ) ? "                                                is the number  of observations\n" : "" );
    output << ( ( basic || advanced ) ? "                                                skipped  at  the  end  due  to\n" : "" );
    output << ( ( basic || advanced ) ? "                                                model failure.                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           Using this  function at  any point  will completely\n" : "" );
    output << ( ( basic || advanced ) ? "                           remove any existing ML.                            \n" : "" );
    output << ( (          advanced ) ? "         -zd  {...}      - like  -z,  but keeps  data.   Note that  this is  a\n" : "" );
    output << ( (          advanced ) ? "                           potentially lossy  function, and  may give an error\n" : "" );
    output << ( (          advanced ) ? "                           for  incompatible types  (eg going  from regression\n" : "" );
    output << ( (          advanced ) ? "                           to classification is not possible).  Moreover there\n" : "" );
    output << ( (          advanced ) ? "                           is a degree  of guesswork  involved, so  don't rely\n" : "" );
    output << ( (          advanced ) ? "                           too much on this option.                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -zv  {once,red} - vectorial SVM type.  Modes are:                    \n" : "" );
    output << ( ( basic || advanced ) ? "                           once   - at-once regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                           red    - reduction to binary regression (default). \n" : "" );
    output << ( ( basic || advanced ) ? "         -zc  {1vsA,1vs1,- multiclass classifier type.  Modes are:            \n" : "" );
    output << ( ( basic || advanced ) ? "              DAG,MOC,     1vsA   - 1 versus all (reduction to binary).       \n" : "" );
    output << ( ( basic || advanced ) ? "              maxwin,      1vs1   - 1 versus 1 (reduction to binary).         \n" : "" );
    output << ( ( basic || advanced ) ? "              recdiv}      DAG    - directed acyclic graph (reduct to binary).\n" : "" );
    output << ( ( basic || advanced ) ? "                           MOC    - minimum output coding (reduct to binary). \n" : "" );
    output << ( ( basic || advanced ) ? "                           maxwin - max-wins SVM (at once).                   \n" : "" );
    output << ( ( basic || advanced ) ? "                           recdiv - recursive division SVM (at once, default).\n" : "" );
    output << ( ( basic || advanced ) ? "         -zo  {sch,tax}  - one-class SVM method:                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           sch    - Scholkopf 1999 type (default).            \n" : "" );
    output << ( ( basic || advanced ) ? "                           tax    - Tax  and   Duin,   \"Support  Vector   Data\n" : "" );
    output << ( ( basic || advanced ) ? "                                    Description\" (SVDD), Machine Learning, 54,\n" : "" );
    output << ( ( basic || advanced ) ? "                                    2004.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Setup options (after pre-setup options):                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -N   n          - hint of expected training  set size (this will help\n" : "" );
    output << ( ( basic || advanced ) ? "                           minimise memory usage and duplication overhead).   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fo  n $file    - open file for  future processing  using placeholder\n" : "" );
    output << ( (          advanced ) ? "                           n.  The  file may  then  be  used for  training and\n" : "" );
    output << ( (          advanced ) ? "                           testing.  Variable var(0,n) will contain the number\n" : "" );
    output << ( (          advanced ) ? "                           of vectors remaining in  the file.  If n is already\n" : "" );
    output << ( (          advanced ) ? "                           in use then the  current file will be  closed and a\n" : "" );
    output << ( (          advanced ) ? "                           new file opened.                                   \n" : "" );
    output << ( (          advanced ) ? "         -foe n $file    - target-at-end version of -fo.                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fret n m       - tag variable  var(n,m) to be retained  after return\n" : "" );
    output << ( (          advanced ) ? "                           from for example -MF or -g, -gb etc.               \n" : "" );
    output << ( (          advanced ) ? "         -fV  n $fn      - set argument var(0,n) = $fn (not evaluated).       \n" : "" );
    output << ( (          advanced ) ? "         -fW  n v        - set argument var(0,n) = v (evaluated).             \n" : "" );
    output << ( (          advanced ) ? "         -fWW n v        - set argument var(0,n) = v   (evaluated   but    not\n" : "" );
    output << ( (          advanced ) ? "                           finalised, so  for example  random numbers  are not\n" : "" );
    output << ( (          advanced ) ? "                           drawn from and globals are not evaluated.          \n" : "" );
    output << ( (          advanced ) ? "         -fru n          - set argument var(0,n) = uniform random U(0,1).     \n" : "" );
    output << ( (          advanced ) ? "         -frn n          - set argument var(0,n) = gaussian random N(0,1).    \n" : "" );
    output << ( (          advanced ) ? "         -fri n          - set argument var(0,n) = random integer.            \n" : "" );
    output << ( (          advanced ) ? "         -fM  n args     - set argument var(130,n) =  string  of  args.   args\n" : "" );
    output << ( (          advanced ) ? "                           should be enclosed in {}.  These are used as macros\n" : "" );
    output << ( (          advanced ) ? "                           later.                                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fu  n i x      - single-objective  test function  evaluation.  Given\n" : "" );
    output << ( (          advanced ) ? "                           decision  vector  x evaluates  test  function  i to\n" : "" );
    output << ( (          advanced ) ? "                           produce output that is stored in variable var(0,n).\n" : "" );
    output << ( (          advanced ) ? "                           These functions can also be accessed in expressions\n" : "" );
    output << ( (          advanced ) ? "                           using fnB(-1,i,x) and fnC(-2,i,x,A).  Problem specs\n" : "" );
    output << ( (          advanced ) ? "                           are as follows (d = dim(x), c/f Wikipedia):        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( (          advanced ) ? "|  i: Function name             | d | Range                  | Minimum       |\n" : "" );
    output << ( (          advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( (          advanced ) ? "|  1: Rastrigin function        | d | -5.12   <= x_i <= 5.12 | f(0)     = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  2: Ackley's function         | d | -5      <= x_i <= 5    | f(0)     = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  3: Sphere function           | d | -inf    <= x_i <= inf  | f(0)     = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  4: Rosenbrock function       | d | -inf    <= x_i <= inf  | f(1)     = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  5: Beale's function          | 2 | -4.5    <= x,y <= 4.5  | f(3,0.5) = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  6: Goldstein-Price function  | 2 | -2      <= x,y <= 2    | f(0,-1)  = 3  |\n" : "" );
    output << ( (          advanced ) ? "|  7: Booth's function          | 2 | -10     <= x,y <= 10   | f(1,3)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  8: Bukin function N.6        | 2 | -15,-3  <= x,y <= -5,3 | f(-10,1) = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  9: Matyas function           | 2 | -10     <= x,y <= 10   | f(0,0)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 10: Levi function N.13        | 2 | -10     <= x,y <= 10   | f(1,1)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 11: Himmelblau's function     | 2 | -5      <= x,y <= 5    | f(s,t)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 12: Three-hump camel function | 2 | -5      <= x,y <= 5    | f(0,0)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 13: Easom function            | 2 | -100    <= x,y <= 100  | f(pi,pi) = -1 |\n" : "" );
    output << ( (          advanced ) ? "| 14: Cross-in-tray function    | 2 | -10     <= x,y <= 10   | f(a,a)   = b  |\n" : "" );
    output << ( (          advanced ) ? "| 15: Eggholder function        | 2 | -512    <= x,y <= 512  | f(c,d)   = e  |\n" : "" );
    output << ( (          advanced ) ? "| 16: Holder table function     | 2 | -10     <= x,y <= 10   | f(f,f)   = g  |\n" : "" );
    output << ( (          advanced ) ? "| 17: McCormick function        | 2 | -1.5,-3 <= x,y <= 4,4  | f(h,j)   = k  |\n" : "" );
    output << ( (          advanced ) ? "| 18: Schaffer function N. 2    | 2 | -100    <= x,y <= 100  | f(0,0)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 19: Schaffer function N. 4    | 2 | -100    <= x,y <= 100  | f(0,l)   = m  |\n" : "" );
    output << ( (          advanced ) ? "| 20: Styblinski-Tang function  | d | -5      <= x_i <= 5    | q <= f(p) <= r|\n" : "" );
    output << ( (          advanced ) ? "| 21: Stability test function 1 | 1 | 0       <= x   <= 1    | f(0.2)   = 1.3|\n" : "" );
    output << ( (          advanced ) ? "|     (also has unstable max at |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     f(0.5) = 1.65 (2nd order) |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     and f(1) = 1.5 (1st))     |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "| 22: Stability test function 2 | 1 | 0       <= x   <= 1    | f(1)     = 4  |\n" : "" );
    output << ( (          advanced ) ? "|     Sum  of   two  gaussians, |   |                        | f(0.5)   = 1  |\n" : "" );
    output << ( (          advanced ) ? "|     gamma  =   1/(5.sqrt(2)), |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     centres  at  1  (alpha 4) |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     and 0.5  (alpha  1).  Use |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     A = 0.2, B = 0.05, pmax=2 |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "| 23: Sum   of  RBFs   function | d | 0       <= x   <= 1    | depends       |\n" : "" );
    output << ( (          advanced ) ? "|     sum_i a_{i,0} exp(-(||x-a_{i,2:...}||_2^2)/(2*a_{i,1}*a_{i,1}))        |\n" : "" );
    output << ( (          advanced ) ? "| 10xx: function xx, normalised | d | -1      <= x   <= 1    | depends       |\n" : "" );
    output << ( (          advanced ) ? "|     (nominally) so  -1<=x<=1, |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     0<=f(x)<=1                |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Stability test function:                           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           f(x) = exp(-20*(x-0.2)*(x-0.2))                    \n" : "" );
    output << ( (          advanced ) ? "                                + exp(-20*sqrt(0.00001+((x-0.5)*(x-0.5))))    \n" : "" );
    output << ( (          advanced ) ? "                                + exp(2*(x-0.8))                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Constants: a = +-1.34941                           \n" : "" );
    output << ( (          advanced ) ? "                                      b = -2.06261                            \n" : "" );
    output << ( (          advanced ) ? "                                      c = 512                                 \n" : "" );
    output << ( (          advanced ) ? "                                      d = 404.2319                            \n" : "" );
    output << ( (          advanced ) ? "                                      e = -959.6407                           \n" : "" );
    output << ( (          advanced ) ? "                                      f = +-8.05502                           \n" : "" );
    output << ( (          advanced ) ? "                                      g = -19.2085                            \n" : "" );
    output << ( (          advanced ) ? "                                      h = -0.54719                            \n" : "" );
    output << ( (          advanced ) ? "                                      j = -1.54719                            \n" : "" );
    output << ( (          advanced ) ? "                                      k = -1.9133                             \n" : "" );
    output << ( (          advanced ) ? "                                      l = 1.25313                             \n" : "" );
    output << ( (          advanced ) ? "                                      m = 0.292579                            \n" : "" );
    output << ( (          advanced ) ? "                                      p = -2.903534                           \n" : "" );
    output << ( (          advanced ) ? "                                      q = -39.16617n                          \n" : "" );
    output << ( (          advanced ) ? "                                      r = -39.16616n                          \n" : "" );
    output << ( (          advanced ) ? "                                      (s,t) = (3,2), (-2.805,3.131),          \n" : "" );
    output << ( (          advanced ) ? "                                          (-3.779,-3.283), (3.584,-1.848)     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fuu n i x a    - like -fu with additional user parameter matrix a.  \n" : "" );
    output << ( (          advanced ) ? "         -ft  n i M x    - multi-objective  test  function  evaluation.  Given\n" : "" );
    output << ( (          advanced ) ? "                           decision  vector x  evaluates  test  function i  to\n" : "" );
    output << ( (          advanced ) ? "                           produce  an  M-dimensional  output  vector  that is\n" : "" );
    output << ( (          advanced ) ? "                           stored  in  variable  var(0,n).  These can  also be\n" : "" );
    output << ( (          advanced ) ? "                           accessed via fnC(-3,i,x,M). Available tst functions\n" : "" );
    output << ( (          advanced ) ? "                           are (sources SCH1,DTLZ,FON1 and Wikipedia):        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "+-------------+---+-----+-----------------------------------------+----------+\n" : "" );
    output << ( (          advanced ) ? "|  i: Fn name | n | M   | Function                                | Range    |\n" : "" );
    output << ( (          advanced ) ? "+-------------+---+-----+-----------------------------------------+----------+\n" : "" );
    output << ( (          advanced ) ? "|  1: DTLZ1   | n | <=n | [ x0...xM-2.(1+g(z))/2          ; ]     | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ x0...xM-3.(1-xM-2).(1+g(z))/2 ; ]     |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [   ...                         ; ]     |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ x0...(1-x1).(1+g(z))/2        ; ]     |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1-x0).(1+g(z))/2               ]     |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = 100.( #(z) + sum_i ( (zi-0.5)^2  |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |               - cos(20*pi*(zi-0.5)) ) ) |          |\n" : "" );
    output << ( (          advanced ) ? "|  2: DTLZ2   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | s(x) = sin( x.pi/2 )                    |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | c(x) = cos( x.pi/2 )                    |          |\n" : "" );
    output << ( (          advanced ) ? "|  3: DTLZ3   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = 100.( #(z) + sum_i ( (zi-0.5)^2  |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |               - cos(20*pi*(zi-0.5)) ) ) |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | s(x) = sin( x.pi/2 )                    |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | c(x) = cos( x.pi/2 )                    |          |\n" : "" );
    output << ( (          advanced ) ? "|  4: DTLZ4   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x1<=1 |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] | -5<=xi<=5|\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [   ...                               ] | 2<=i<=n  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | s(x) = sin( (x^alpha).pi/2 )            |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | c(x) = cos( (x^alpha).pi/2 )            |          |\n" : "" );
    output << ( (          advanced ) ? "|  5: DTLZ5   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | s(x) = sin( theta.pi/2 )                |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | c(x) = cos( theta.pi/2 )                |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | theta = (pi/(4.(1+g(z)))).(1+2.g(z).xi) |          |\n" : "" );
    output << ( (          advanced ) ? "|  6: DTLZ6   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = sum_i zi^0.1                     |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | s(x) = sin( theta.pi/2 )                |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | c(x) = cos( theta.pi/2 )                |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | theta = (pi/(4.(1+g(z)))).(1+2.g(z).xi) |          |\n" : "" );
    output << ( (          advanced ) ? "|  7: DTLZ7   | n | <=n | [ x0                              ; ]   | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ x1                              ; ]   |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [   ...                             ]   |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ xM-2                            ; ]   |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (1+g(z)).h(f1,f2,...,fM-2,g(z))   ]   |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = 1 + (9/#(z)) sum_i zi            |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | h = M-sum_i((fi/(1+g)).(1+sin(3pi.fi))) |          |\n" : "" );
    output << ( (          advanced ) ? "|  8: DTLZ8   | n | <n  | [ sum_ib^is xi ], i = 0,1,...,M-1       | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |        ib = floor(i*n/M)-1              |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |        is = floor((i+1)*n/M)-1          |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | **constraints are not implemented yet.  |          |\n" : "" );
    output << ( (          advanced ) ? "|  9: DTLZ9   | n | <n  | [ sum_ib^is xi^0.1 ], i = 0,1,...,M-1   | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |        ib = floor(i*n/M)-1              |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |        is = floor((i+1)*n/M)-1          |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | **constraints are not implemented yet.  |          |\n" : "" );
    output << ( (          advanced ) ? "| 10: FON1    | n | 2   | [ 1-exp(-|| x - 1/sqrt(n) ||^2) ; ]     | -4<=x<=4 |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ 1-exp(-|| x + 1/sqrt(n) ||^2)   ]     |          |\n" : "" );
    output << ( (          advanced ) ? "| 11: SCH1    | 1 | 2   | [ x^2     ; ]                           | free     |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (x-2)^2   ]                           |          |\n" : "" );
    output << ( (          advanced ) ? "| 12: SCH2    | 1 | 2   | [ { -x     if     x <= 1 } ]            | -5<=x<=10|\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ { x-2    if 1 < x <= 3 } ]            |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ { 4-x    if 3 < x <= 4 } ]            |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ { x-4    if 4 < x      } ]            |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [                          ]            |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | [ (x-5)^2                  ]            |          |\n" : "" );
    output << ( (          advanced ) ? "+-------------+---+-----------------------------------------------+----------+\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           SCH1: Schaffer, J. D.:  Some Experiments in Machine\n" : "" );
    output << ( (          advanced ) ? "                                 Learning   using  Vector   Evaluated  Genetic\n" : "" );
    output << ( (          advanced ) ? "                                 Algorithms - PhD Thesis, 1984.               \n" : "" );
    output << ( (          advanced ) ? "                           DTLZ: Deb,   Kalyanmoy  and   Thiele,  Lothar   and\n" : "" );
    output << ( (          advanced ) ? "                                 Laumanns,   Marco    and   Zitzler,   Eckart:\n" : "" );
    output << ( (          advanced ) ? "                                 \"Scalable  Test  Problems   for  Evolutionary\n" : "" );
    output << ( (          advanced ) ? "                                 Multiobjective Optimization\".                \n" : "" );
    output << ( (          advanced ) ? "                           FON1: Fonseca,  C.  M.  and   Fleming,  P.  J.:  An\n" : "" );
    output << ( (          advanced ) ? "                                 Overview of Evolutionary Algorithms in Multi-\n" : "" );
    output << ( (          advanced ) ? "                                 Objective     Optimisation.      Evolutionary\n" : "" );
    output << ( (          advanced ) ? "                                 Multiobjective   Optimisation,    Theoretical\n" : "" );
    output << ( (          advanced ) ? "                                 Advances and Applications, pg. 105-145, 2005.\n" : "" );
    output << ( (          advanced ) ? "                                 (as re-interpretted in DTLZ for n-dim).      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fat alpha      - sets alpha value used by DTLZ4 in -ft evaluation.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** There are two  types of variable:  global and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** local.  Local  variables are  specific to and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** available everywhere  within a single thread. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Global  variables  are  accessible  from  all **         \n" : "" );
    output << ( (          advanced ) ? "                  ** current running threads,  allowing for inter- **         \n" : "" );
    output << ( (          advanced ) ? "                  ** thread  communications.    Use  local  unless **         \n" : "" );
    output << ( (          advanced ) ? "                  ** strictly necessary.                           **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fVg  n $fn     - like -fV  but reads from global variables.         \n" : "" );
    output << ( (          advanced ) ? "         -fWg  n v       - like -fW  but reads from global variables.         \n" : "" );
    output << ( (          advanced ) ? "         -fVG  n $fn     - like -fV  but writes to global variables.          \n" : "" );
    output << ( (          advanced ) ? "         -fWG  n v       - like -fW  but writes to global variables.          \n" : "" );
    output << ( (          advanced ) ? "         -fuuG n i x a   - like -fuu but writes to global variables.          \n" : "" );
    output << ( (          advanced ) ? "         -fuG  n i x     - like -fu  but writes to global variables.          \n" : "" );
    output << ( (          advanced ) ? "         -ftG  n i M x   - like -ft  but writes to global variables.          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -br  d          - binary   class  label:  by   default,   for  binary\n" : "" );
    output << ( (          advanced ) ? "                           classification,  class labels are  -1 and +1.  This\n" : "" );
    output << ( (          advanced ) ? "                           option lets  you automatically  re-label vectors as\n" : "" );
    output << ( (          advanced ) ? "                           they are  loaded from  a file,  so class  d becomes\n" : "" );
    output << ( (          advanced ) ? "                           class +1 and  all other classes  are relabelled -1.\n" : "" );
    output << ( (          advanced ) ? "                           (default 0, no relabelling).                       \n" : "" );
    output << ( (          advanced ) ? "         -bd  d          - class skipping: for classification, if this is non-\n" : "" );
    output << ( (          advanced ) ? "                           zero then points from  this class coming from files\n" : "" );
    output << ( (          advanced ) ? "                           will be silently ignored. This is handy if you want\n" : "" );
    output << ( (          advanced ) ? "                           to ignore a  particular class in a  multiclass case\n" : "" );
    output << ( (          advanced ) ? "                           (default 0, no skipping).                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -XT  [ x ]      - set x template  (sparse).  If there are elements in\n" : "" );
    output << ( (          advanced ) ? "                           this template  that are not  in a  testing/training\n" : "" );
    output << ( (          advanced ) ? "                           vector  then they will  be added.  This  is helpful\n" : "" );
    output << ( (          advanced ) ? "                           for multitask  learning.  Use -XT [ ~ i ],  where i\n" : "" );
    output << ( (          advanced ) ? "                           is the  task number, and  use  -kS -ks 2  -ki 0 ...\n" : "" );
    output << ( (          advanced ) ? "                           -ki 1 -kt 48 -kr r1, where ...  is the usual kernel\n" : "" );
    output << ( (          advanced ) ? "                           setup and r1 is  the similarity (scalar  or matrix)\n" : "" );
    output << ( (          advanced ) ? "                           between tasks.                                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- MEX (Matlab) only options                     --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fWm n $fn v    - set argument var(0,n) = fn(v), where fn is a MATLAB\n" : "" );
    output << ( (          advanced ) ? "                           function.  Use v = null if no args required, set if\n" : "" );
    output << ( (          advanced ) ? "                           more than one arg required.                        \n" : "" );
    output << ( (          advanced ) ? "         -fWM n i v      - set argument var(0,n) = fni(v) (v evaluated), where\n" : "" );
    output << ( (          advanced ) ? "                           fni is a MATLAB function  handle specified when you\n" : "" );
    output << ( (          advanced ) ? "                           called svmmatlab.   If fni if a  variable and not a\n" : "" );
    output << ( (          advanced ) ? "                           handle then result is value of variable.           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ac  {svc,svr}  - classification method:                             \n" : "" );
    output << ( ( basic || advanced ) ? "                           svc: normal SVM classifier (default).              \n" : "" );
    output << ( ( basic || advanced ) ? "                           svr: classify via regression.                      \n" : "" );
    output << ( ( basic || advanced ) ? "         -B   {f,v,p,n}  - bias type:                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                           f - fixed bias (bias value defaults to 0).         \n" : "" );
    output << ( ( basic || advanced ) ? "                           v - variable bias (default).                       \n" : "" );
    output << ( ( basic || advanced ) ? "                           p - positive bias.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           n - negative bias.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -R   {l,q,o,g,G}- empirical risk type.                               \n" : "" );
    output << ( ( basic || advanced ) ? "                           l - linear (default).                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           q - quadratic.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                           o - linear cost, but with  1-norm regularisation on\n" : "" ); 
    output << ( ( basic || advanced ) ? "                               alpha  (not feature  space:  use -m  for that).\n" : "" ); 
    output << ( ( basic || advanced ) ? "                               (to  enforce 1-norm  regularisation  with hard-\n" : "" ); 
    output << ( ( basic || advanced ) ? "                               margin use this in combination with -c 1e20).  \n" : "" ); 
    output << ( ( basic || advanced ) ? "                           g - generalised linear cost (iterative fuzzy).     \n" : "" );
    output << ( ( basic || advanced ) ? "                           G - generalised quadratic cost (iterative fuzzy).  \n" : "" );
    output << ( ( basic || advanced ) ? "                           Note  that  quadratic  quadratic empirical  ignores\n" : "" );
    output << ( ( basic || advanced ) ? "                           values set by -c+, -c-, -c=, -cc and -jc. Note also\n" : "" );
    output << ( ( basic || advanced ) ? "                           that epsilon insensitivity  is used for both linear\n" : "" );
    output << ( ( basic || advanced ) ? "                           and quadratic cases, so  to construct an LS-SVR you\n" : "" );
    output << ( ( basic || advanced ) ? "                           need to set -R q -w 0.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -T   {f,s}      - tube type:                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                           f - fixed tube (default).                          \n" : "" );
    output << ( ( basic || advanced ) ? "                           s - tube shrinking on.                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- LSV specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -bv             - LSV variable bias (default).                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -bz             - LSV zero bias.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- GPR specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -bgv            - GPR variable bias.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -bgz            - GPR zero bias (default).                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- MLM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -mlR n {l,q}    - MLM regularisation type at layer n:                \n" : "" );
    output << ( ( basic || advanced ) ? "                           l - 1-norm.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           q - 2-norm.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         -mls n          - Set number of layers (not including output, dft 0).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SSV specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -sR  {l,q}      - beta  regulation  type.   In SSV  models  the  beta\n" : "" );
    output << ( ( basic || advanced ) ? "                           regularisation term is set by this:                \n" : "" );
    output << ( ( basic || advanced ) ? "                           l - linear: sigma.||beta||_1.                      \n" : "" );
    output << ( ( basic || advanced ) ? "                           q - quadratic: sigma.||beta||_2^2 (default).       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- BLK specific options                          --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -mc  n          - (Mercer kernel inheritance block): sets the size of\n" : "" );
    output << ( (          advanced ) ? "                           the kernel cache (if any) stored inside this block.\n" : "" );
    output << ( (          advanced ) ? "                           Unlike  other  caches  this one  is  simply  an n*n\n" : "" );
    output << ( (          advanced ) ? "                           matrix, and needs to be manually flushed for kernel\n" : "" );
    output << ( (          advanced ) ? "                           changes etc.  Set  -1 for no cache.  This  does not\n" : "" );
    output << ( (          advanced ) ? "                           store 4-kernel  or m-kernel (m>2)  evaluations.  To\n" : "" );
    output << ( (          advanced ) ? "                           clear the cache reuse -mc n flag.                  \n" : "" );
    output << ( (          advanced ) ? "         -mcn {0,1}      - (Mercer  kernel inheritance  block):  normalisation\n" : "" );
    output << ( (          advanced ) ? "                           on (1) or off (0, default).                        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -mba i n        - (Multi-block average): set ML n as element i.      \n" : "" );
    output << ( (          advanced ) ? "         -mbA i          - (Multi-block average): remove ML element i.        \n" : "" );
    output << ( (          advanced ) ? "         -mbw i w        - (Multi-block average): set ML n weight as w.       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -msn n          - set order of Bernstein basis b_{i,n}(x).           \n" : "" );
    output << ( (          advanced ) ? "         -msw i          - set index of Bernstein basis b_{i,n}(x).           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -bat p          - Set battery parameters.  p is a vector:            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                          (defaults)                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                         [ C_0star ] Battery capacity at Istar (261.9 Ah)     \n" : "" );
    output << ( (          advanced ) ? "                         [ K_c     ] Battery parameter (1.18)                 \n" : "" );
    output << ( (          advanced ) ? "                         [ theta_f ] Electrolyte freezing temp (-40 degree C) \n" : "" );
    output << ( (          advanced ) ? "                         [ epsilon ] Battery parameter (1.29)                 \n" : "" );
    output << ( (          advanced ) ? "                         [ delta   ] Battery parameter (1.40)                 \n" : "" );
    output << ( (          advanced ) ? "                         [ Istar   ] Nominal battery current (49 A)           \n" : "" );
    output << ( (          advanced ) ? "                         [         ]                                          \n" : "" );
    output << ( (          advanced ) ? "                         [ E_m0    ] Battery voltage at full charge (2.135 V) \n" : "" );
    output << ( (          advanced ) ? "                         [ K_E     ] Battery parameter (0.58e-3 V/degree C    \n" : "" );
    output << ( (          advanced ) ? "                         [ tau_1   ] Battery parameter (5000 sec)             \n" : "" );
    output << ( (          advanced ) ? "                         [ R_00    ] Battery parameter (2e-3 ohm)             \n" : "" );
    output << ( (          advanced ) ? "                         [ R_10    ] Battery parameter (0.7e-3 ohm)           \n" : "" );
    output << ( (          advanced ) ? "                         [ R_20    ] Battery parameter (15e-3 ohm)            \n" : "" );
    output << ( (          advanced ) ? "                         [ A_0     ] Battery parameter (-0.3)                 \n" : "" );
    output << ( (          advanced ) ? "                         [ A_21    ] Battery parameter (-8.0)                 \n" : "" );
    output << ( (          advanced ) ? "                         [ A_22    ] Battery parameter (-8.45)                \n" : "" );
    output << ( (          advanced ) ? "                         [         ]                                          \n" : "" );
    output << ( (          advanced ) ? "                         [ E_p     ] Battery parameter (1.95 V)               \n" : "" );
    output << ( (          advanced ) ? "                         [ V_p0    ] Battery parameter (0.1 V)                \n" : "" );
    output << ( (          advanced ) ? "                         [ A_p     ] Battery parameter (2.0)                  \n" : "" );
    output << ( (          advanced ) ? "                         [ G_p0    ] Battery parameter (2e-12 sec)            \n" : "" );
    output << ( (          advanced ) ? "                         [         ]                                          \n" : "" );
    output << ( (          advanced ) ? "                         [ C_theta ] Battery parameter (15 Wh/degree C)       \n" : "" );
    output << ( (          advanced ) ? "                         [ R_theta ] Battery parameter (0.2 degree C/W)       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           use null to leave parameter as-is.                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -bam p          - Set time period for battery model (sec, dflt 3600).\n" : "" );
    output << ( (          advanced ) ? "         -bac p          - Set max (dis)charge current (amps, default 30).    \n" : "" );
    output << ( (          advanced ) ? "         -bad d          - Set time-step for battery model (sec, dflt 0.05).  \n" : "" );
    output << ( (          advanced ) ? "         -bav v          - Set start voltage for battery model (volts, 2.135).\n" : "" );
    output << ( (          advanced ) ? "         -baT t          - Set ambient/start temperture for battery (deg, 20).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Preload options (after setup options):                                        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -pR             - reset the ML to initial state (keeps training dat).\n" : "" );
    output << ( (          advanced ) ? "         -pRR            - restart the ML (clean-slate, defaults, no data).   \n" : "" );
    output << ( (          advanced ) ? "         -pr  i          - remove training vector i.                          \n" : "" );
    output << ( (          advanced ) ? "         -pro n          - remove the first n training vectors.               \n" : "" );
    output << ( (          advanced ) ? "         -psz i z        - set target z for training vector i.                \n" : "" );
    output << ( (          advanced ) ? "         -pcw i C        - set C>0 weight for training vector i.              \n" : "" );
    output << ( (          advanced ) ? "         -pcs s          - scale all preloaded C weights (t/TVALs) by s>0.    \n" : "" );
    output << ( (          advanced ) ? "         -pww i eps      - set eps>=0 weight for training vector i.           \n" : "" );
    output << ( (          advanced ) ? "         -pws s          - scale all preloaded eps weight (e/EVALs) by s>=0.  \n" : "" );
    output << ( (          advanced ) ? "         -ps  s          - scale whole  ML by s>=0.  The exact meaning of this\n" : "" );
    output << ( (          advanced ) ? "                           depends on the ML type as follows:                 \n" : "" );
    output << ( (          advanced ) ? "         -pS             - scale to ensure that abs2(alpha) = 1.              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           SVM: alpha and b are scaled by s.                  \n" : "" );
    output << ( (          advanced ) ? "                           LSV/SSV: like SVM.                                 \n" : "" );
    output << ( (          advanced ) ? "                           GPR: y and K (kernel weight) are scaled by s.      \n" : "" );
    output << ( (          advanced ) ? "                           BLK consensus: scale all parts.                    \n" : "" );
    output << ( (          advanced ) ? "                           Others: may or may not be implemented, see code.   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fic            - fill caches. This can be handy if you have a Mercer\n" : "" );
    output << ( (          advanced ) ? "                           kernel  inheritance block  (kernel cache)  that you\n" : "" );
    output << ( (          advanced ) ? "                           want to fill for a specific setting before modding.\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -pk  $file      - load kernel matrix from file (bypass kernel).  File\n" : "" );
    output << ( (          advanced ) ? "                           format is  MATLAB style matrix, []s  must included.\n" : "" );
    output << ( (          advanced ) ? "                           This may be handy for  multi-instance learning with\n" : "" );
    output << ( (          advanced ) ? "                           many (1000s) of instances  in some examples, as for\n" : "" );
    output << ( (          advanced ) ? "                           reasons unknown this can crash during kernel eval. \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -prz            - remove non-support vectors.                        \n" : "" );
    output << ( (          advanced ) ? "         -prm n          - like  -prz, but  continue removing  support vectors\n" : "" );
    output << ( (          advanced ) ? "                           until the number of vectors reaches at max n.      \n" : "" );
    output << ( (          advanced ) ? "         -psd i d        - set class d for training vector i.                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- GP specific options                           --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -pdw i C        - set C>0 weight for training vector i.              \n" : "" );
    output << ( (          advanced ) ? "         -pds s          - scale all preloaded C weights (t/TVALs) by s>0.    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Load options (adding training vectors, after preload options):                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -AA  $file      - add vectors from fname.                            \n" : "" );
    output << ( ( basic || advanced ) ? "         -AN  i j k $file -add vectors from file, ignoring i vectors at start,\n" : "" );
    output << ( ( basic || advanced ) ? "                           adding at  most j vectors  (-1 if all),  and adding\n" : "" );
    output << ( ( basic || advanced ) ? "                           vectors starting from index k (-1 to add to end).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -AU  d [x]      - add training vector x,  target d.  For single-class\n" : "" );
    output << ( (          advanced ) ? "                           ML, target d is ignored  but must still be present.\n" : "" );
    output << ( (          advanced ) ? "                           x must be an (optionally sparse) enclosed vector.  \n" : "" );
    output << ( (          advanced ) ? "                           eg -AU -1 [ 0:1 1:-1 ]                             \n" : "" );
    output << ( (          advanced ) ? "                           or, in non-sparse notation, -AU -1 [ 1 -1 ]        \n" : "" );
    output << ( (          advanced ) ? "         -AY  d x        - like -AU, but x is any vector.                     \n" : "" );
    output << ( (          advanced ) ? "         -AZ  d x m      - like -AU, but x is any vector and ML m.            \n" : "" );
    output << ( (          advanced ) ? "         -AV  [d] [[x]]  - add multiple training vectors, eg                  \n" : "" );
    output << ( (          advanced ) ? "                     -AV [ -1 -1 1 1 ] [ [ -1 -1 ] [ 1 1 ] [ 1 -1 ] [ -1 1 ] ]\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -AGl l          - Set lower bound (scalar or vector) for -AG, -AGc). \n" : "" );
    output << ( (          advanced ) ? "         -AGu u          - Set upper bound (scalar or vector) for -AG, -AGc). \n" : "" );
    output << ( (          advanced ) ? "         -Ag  N d f v    - generate and add training data.  N pairs generated,\n" : "" );
    output << ( (          advanced ) ? "                           vectors have dim d, function is f with noise var v,\n" : "" );
    output << ( (          advanced ) ? "                           features N(0,1).                                   \n" : "" );
    output << ( (          advanced ) ? "         -AG  N d f v    - generate and add training data.  N pairs generated,\n" : "" );
    output << ( (          advanced ) ? "                           vectors have dim d, function is f with noise var v,\n" : "" );
    output << ( (          advanced ) ? "                           features U(l,u).                                   \n" : "" );
    output << ( (          advanced ) ? "         -Agc N d f v c  - like -Ag, but only adds data when c(x) == 1.       \n" : "" );
    output << ( (          advanced ) ? "         -AGc N d f v c  - like -AG, but only adds data when c(x) == 1.       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -AVv d x        - add multiple training vectors with  target vector d\n" : "" );
    output << ( (          advanced ) ? "                           and input  vectors x, eg  -AVv var(81,0)  var(80,0)\n" : "" );
    output << ( (          advanced ) ? "                           after -tx will train a model on LOO error.         \n" : "" );
    output << ( (          advanced ) ? "         -AVV d x s      - add multiple training vectors with target vector d,\n" : "" );
    output << ( (          advanced ) ? "                           input vectors x  and sigma weights s.  For  example\n" : "" );
    output << ( (          advanced ) ? "                           you might use var(83,0) in the above example.      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** For target-at-end  format files, the e suffix **         \n" : "" );
    output << ( (          advanced ) ? "                  ** can be used.  So for example:                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -AA  trainfile   (target-at-start format)     **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -AAe trainfile   (target-at-end format)       **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** To add training vectors  from open files, use **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the i suffix and replace  the filename with n **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (the file number).                            **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** To  add  training  vectors  from  open  files **         \n" : "" );
    output << ( (          advanced ) ? "                  ** but leave them so they can be reused, use the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** I/R suffixes (just like i/r suffixes).        **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** To add training vectors randomly from an open **         \n" : "" );
    output << ( (          advanced ) ? "                  ** file, use the r suffix instead of i.          **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** As labels/targets  may not always  be present **         \n" : "" );
    output << ( (          advanced ) ? "                  ** training files,  the l suffix can  be used to **         \n" : "" );
    output << ( (          advanced ) ? "                  ** apply a label to all vectors in an unlabelled **         \n" : "" );
    output << ( (          advanced ) ? "                  ** training file (ie. a file containing only x). **         \n" : "" );
    output << ( (          advanced ) ? "                  ** General format is:                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** {usual flag}l {usual options} y               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where y is the required class/target.  eg.    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -AA trainfile   becomes   -AAl trainfile y    **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -ANe i j k fle  becomes   -ANel i j k fle y   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** If the training file  has labels that are not **         \n" : "" );
    output << ( (          advanced ) ? "                  ** needed  (for  example,  you want  to  train a **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1-class  ML using  a labelled  training file) **         \n" : "" );
    output << ( (          advanced ) ? "                  ** then the  u suffix  can  be used  to  strip / **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ignore the labels.  The general format is:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** {usual flag}u {usual options}                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** For example:                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -AN i j k fle   becomes   -ANu i j k fle      **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -AAe trainfile  becomes   -AAeu trainfile     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** NB: the l  suffix  is  incompatible  with the **         \n" : "" );
    output << ( (          advanced ) ? "                  **     1-class  ML,  and  the u  suffix is  only **         \n" : "" );
    output << ( (          advanced ) ? "                  **     compatible with the 1-class ML.           **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** The complete list of suffixed options are:    **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -AAe,-AAi,-AAl,-AAu, -AAel,-AAeu,-AAil,-AAiu, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -ANe,-ANi,-ANr, -ANl,-ANu,-ANel, -ANeu,-ANil, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -ANiu, -ANrl, -ANru                           **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Ad  n          - set target space dimension (vector regression).    \n" : "" );
    output << ( (          advanced ) ? "         -AD  n          - set target order (anionic regression: 0,1,2,...).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Rudimentary  transductive learning  supported **         \n" : "" );
    output << ( (          advanced ) ? "                  ** by the following  functions.  Given a dataset **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (labels ignored) points  are classified using **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the ML and added to the  training set if g(x) **         \n" : "" );
    output << ( (          advanced ) ? "                  ** exceeds a given the threshold (set by -ATb).  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** If anomaly detection is  switched on, and the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** trigger level (-ATn) is positive then if more **         \n" : "" );
    output << ( (          advanced ) ? "                  ** than this trigger level of anomalies are det- **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ected  then a  new class  is created  and the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** detected  anomalies  satisfying  the distance **         \n" : "" );
    output << ( (          advanced ) ? "                  ** requirement placed in this class.             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** File format is  the same as for  -AA etc, but **         \n" : "" );
    output << ( (          advanced ) ? "                  ** note  that labels  will always be  ignored if **         \n" : "" );
    output << ( (          advanced ) ? "                  ** present.                                      **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ATA $file      - transductively add vectors from file (see -AA).    \n" : "" );
    output << ( (          advanced ) ? "         -ATN i j k $file -transductively add vectors from file (see -AN).    \n" : "" );
    output << ( (          advanced ) ? "         -ATb d          - distance from classification  boundary required for\n" : "" );
    output << ( (          advanced ) ? "                           point  to be  transductively  added  to that  class\n" : "" );
    output << ( (          advanced ) ? "                           (default: 1).                                      \n" : "" );
    output << ( (          advanced ) ? "         -ATa d          - trigger  distance (distance  point must  lie inside\n" : "" );
    output << ( (          advanced ) ? "                           anonaly class to be counted, default 1).           \n" : "" );
    output << ( (          advanced ) ? "         -ATn N          - set trigger level  for anomalies  (num of anomalies\n" : "" );
    output << ( (          advanced ) ? "                           anomalies required  before they  can be grouped and\n" : "" );
    output << ( (          advanced ) ? "                           incorporated as a new class (default 0, disabled). \n" : "" );
    output << ( (          advanced ) ? "         -ATx d          - class  label  to be  used if  anomaly  class  added\n" : "" );
    output << ( (          advanced ) ? "                           (default 0, disabled).                             \n" : "" );
    output << ( (          advanced ) ? "         -ATy c          - method control for transduction:                   \n" : "" );
    output << ( (          advanced ) ? "                           0: add no training vectors.                        \n" : "" );
    output << ( (          advanced ) ? "                           1: add only non-anomalies satisfying distance -ATb.\n" : "" );
    output << ( (          advanced ) ? "                           2: add only anomalies  satisfying distance -ATa (if\n" : "" );
    output << ( (          advanced ) ? "                              new  class  creation   is  enabled  and  trigger\n" : "" );
    output << ( (          advanced ) ? "                              condition met, otherwise don't add).            \n" : "" );
    output << ( (          advanced ) ? "                           3: combination of methods 1 and 2 (default).       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Aq  n m v      - add n random features N(m,v) to all x.             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- gentype basis definition options (\"u\" above)  --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Aby            - set basis equal to training targets.               \n" : "" );
    output << ( ( basic || advanced ) ? "         -Abu            - set basis equal to user defined values.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -AeA $file      - add basis elements from fname.                     \n" : "" );
    output << ( ( basic || advanced ) ? "         -AeU f          - add basis element f.                               \n" : "" );
    output << ( ( basic || advanced ) ? "         -AeR n d        - set basis of n elements, each a random, 1-norm unit\n" : "" );
    output << ( ( basic || advanced ) ? "                           vector of dimension d.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -Ar  i          - remove basis element i.                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- planar basis definition options (\"v\" above)   --         \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- (also multi-expert ranking)                   --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ABy            - set basis equal to training targets.               \n" : "" );
    output << ( ( basic || advanced ) ? "         -ABu            - set basis equal to user defined values.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -AEA $file      - add basis elements from fname.                     \n" : "" );
    output << ( ( basic || advanced ) ? "         -AEU f          - add basis element f.                               \n" : "" );
    output << ( ( basic || advanced ) ? "         -AER n d        - set basis of n elements, each a random, 1-norm unit\n" : "" );
    output << ( ( basic || advanced ) ? "                           vector of dimension d.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -AR  i          - remove basis element i.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- MEX (Matlab) only options                     --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -AW  $yvar $xvar -get training data from  matlab variables.  xvar and\n" : "" );
    output << ( (          advanced ) ? "                           yvar  have n  rows,  each of  which  is a  training\n" : "" );
    output << ( (          advanced ) ? "                           vector.  yvar is not used  in the single-class case\n" : "" );
    output << ( (          advanced ) ? "                           but must still be present.                         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                   [ y1 ]         [ x1' ]   [ x11 x12 .. x1d ]\n" : "" );
    output << ( (          advanced ) ? "                           yvar =  [ y2 ], xvar = [ x2' ] = [ x2d x2d .. x2d ]\n" : "" );
    output << ( (          advanced ) ? "                                   [ .. ]         [ ... ]   [                ]\n" : "" );
    output << ( (          advanced ) ? "                                   [ yn ]         [ xn' ]   [ xnd xnd .. xnd ]\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Ac  d          - add class to SVM  (multiclass only)  without adding\n" : "" );
    output << ( ( basic || advanced ) ? "                           any vectors from that class.                       \n" : "" );
    output << ( (          advanced ) ? "         -Acz d          - like -Ac d, but sets epsilon = 0 for this component\n" : "" );
    output << ( (          advanced ) ? "                           if recursive division or max wins multiclass used. \n" : "" );
    output << ( (          advanced ) ? "         -Aca d nu       - add anomaly detector to multiclass SVM with label d\n" : "" );
    output << ( (          advanced ) ? "                           and anomaly detection parameter nu.                \n" : "" );
    output << ( (          advanced ) ? "         -Acd            - remove anomaly detector.                           \n" : "" );
    output << ( (          advanced ) ? "         -As  n          - set single-class  SVM non-anomaly  label (+1 or -1,\n" : "" );
    output << ( (          advanced ) ? "                           +1 by defaulg).                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Post-load options (after adding training vectors) options:                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Sa  $file      - load alpha from $file.                             \n" : "" );
    output << ( (          advanced ) ? "         -Sb  $file      - load bias from $file.                              \n" : "" );
    output << ( (          advanced ) ? "                           (NB: must be in raw format for recursive division) \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Snx            - remove any existing data normalisation.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sna            - normalise data features to zero mean, unit var.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -Snb            - normalise data features to zero median, unit var.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -Snc            - normalise data features to range 0,1.              \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNa            - like -Sna, but no shifting applied.                \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNb            - like -Snb, but no shifting applied.                \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNc            - like -Snc, but no shifting applied.                \n" : "" );
    output << ( ( basic || advanced ) ? "         -SnA            - like -Sna, but applies min scale to all elements.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -SnB            - like -Snb, but applies min scale to all elements.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -SnC            - like -Snc, but applies min scale to all elements.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNA            - like -Sna, but no shift and min scale.             \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNB            - like -Snb, but no shift and min scale.             \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNC            - like -Snc, but no shift and min scale.             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sra f          - random alpha/weight initialisation with sparsity f.\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  ** NOTES: - normalisation done by shift+scale in **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          the kernel, so  the same changes are **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          automatically  applied to all future **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          training/testing data.               **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **        - calculated on per-feature basis.     **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **        - does not  apply to  categorical, set **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          or graph valued features.            **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **        - vector-valued  features  are  normed **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          to  zero  mean, unit  covar  matrix. **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          Hence  normalisation can  be used to **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          implement  the  Mahalanobis norm  if **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          data  is given as  a single  vector- **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          valued feature - ie. in the training **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          file use [ x ] rather than x.        **         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -St  l u N      - Takes sample from whatever  distribution the ML is.\n" : "" );
    output << ( ( basic || advanced ) ? "                           N  sample points are  taken from  x ~  U(l,u).  For\n" : "" );
    output << ( ( basic || advanced ) ? "                           example  to  sample  from   the  posterior  of  the\n" : "" );
    output << ( ( basic || advanced ) ? "                           current GP and put in in ML i you might use:       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           -Zx -qc i h -qw i -St [ 0 0 ] [ 1 1 ] 200          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           (Use fnb with grand(0,1) to get a block with random\n" : "" );
    output << ( ( basic || advanced ) ? "                           output).                                           \n" : "" );
    output << ( ( basic || advanced ) ? "         -Snt            - Reverse of -St, if that makes sense in context.    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sx  f          - Target transform: y -> f(y).                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Learning options (after post-load modifications):                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -c   CN         - Set  standard ML  tradeoff parameter  C/N = CN >= 0\n" : "" );
    output << ( ( basic || advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  ** NB: in this code, C/N  is the upper bound for **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     |alpha| (unless  using quadratic cost, of **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     course, but in this  case similar changes **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     apply), whereas  elsewhere (SVMlight, for **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     example)  this upper bound is  denoted C. **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     C may mean different  things in different **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     ML blocks.                                **         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -c+  s          - classification: C scale factor s>0 for class d = +1\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression:  C   scale  factor  for   lower  bounds\n" : "" );
    output << ( ( basic || advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -c-  s          - classification: C scale factor s>0 for class d = -1\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression:  C   scale  factor  for   upper  bounds\n" : "" );
    output << ( ( basic || advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -c=  s          - regression:  C  scale  factor  s>0  for  equalities\n" : "" );
    output << ( (          advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -cd  d s        - classification: C  scale  factor s>0  for  class d.\n" : "" );
    output << ( (          advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -cs  s          - Scale C/N by s>0.                                  \n" : "" );
    output << ( (          advanced ) ? "         -c+s s          - Scale c+ by s>0.                                   \n" : "" );
    output << ( (          advanced ) ? "         -c-s s          - Scale c- by s>0.                                   \n" : "" );
    output << ( (          advanced ) ? "         -c=s s          - Scale c= by s>0.                                   \n" : "" );
    output << ( (          advanced ) ? "         -cds d s        - Scale cd by s>0.                                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -cw  i w        - set C weight w>0 for training vector i.            \n" : "" );
    output << ( (          advanced ) ? "         -ww  i w        - set eps weight w>=0 for training vector i.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -mvb beta       - multi-ranking spread regularisation term (deflt 1).\n" : "" );
    output << ( (          advanced ) ? "         -mvi m          - maximum iterations m>=0 for multi-ranking outerloop\n" : "" );
    output << ( (          advanced ) ? "                           training (0 for unlimited - default).              \n" : "" );
    output << ( (          advanced ) ? "         -mvlr r         - multi-ranking learning rate r>0 (default 0.3).     \n" : "" );
    output << ( (          advanced ) ? "         -mvzt t         - multi-ranking zero tolerance t>=0 (default 0.01).  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -w   eps        - epsilon: sets  width of epsilon  tube/insensitivity\n" : "" );
    output << ( ( basic || advanced ) ? "                           eps>=0 for  regression  or  boundary  distance  for\n" : "" );
    output << ( ( basic || advanced ) ? "                           classification or  single class  (default 0.001 for\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression, 1 for  classification/single class, 0.1\n" : "" );
    output << ( ( basic || advanced ) ? "                           for cyclic regression.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -w+  s          - classification: epsilon scale s>=0 for class d = +1\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression:  epsilon scale factor  for lower bounds\n" : "" );
    output << ( ( basic || advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -w-  s          - classification: epsilon scale s>=0 for class d = -1\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression:  epsilon scale factor  for upper bounds\n" : "" );
    output << ( ( basic || advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -w=  s          - regression:  epsilon  scale   s>=0  for  equalities\n" : "" );
    output << ( (          advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -wd  d s        - classification: epsilon  scale  s>=0  for  class d.\n" : "" );
    output << ( (          advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -ws  s          - Scale eps by s>=0.                                 \n" : "" );
    output << ( (          advanced ) ? "         -w+s s          - Scale eps+ by s>=0.                                \n" : "" );
    output << ( (          advanced ) ? "         -w-s s          - Scale eps- by s>=0.                                \n" : "" );
    output << ( (          advanced ) ? "         -w=s s          - Scale eps= by s>=0.                                \n" : "" );
    output << ( (          advanced ) ? "         -wds d s        - Scale epsd by s>=0.                                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -j   s          - equivalent to -c- 1 -c+ s.                         \n" : "" );
    output << ( (          advanced ) ? "         -jc  s          - equivalent to -c- 1 -c+ s.                         \n" : "" );
    output << ( (          advanced ) ? "         -jw  s          - equivalent to -w- 1 -w+ s.                         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Tl  s          - linear tube shrinking value s>=0 (if used, dflt 0).\n" : "" );
    output << ( (          advanced ) ? "         -Tq  s          - quadratic tube shrinking vl s>=0 (if used, dflt 0).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Bf  b          - fixed bias  used for  fixed bias case  (default 0).\n" : "" );
    output << ( (          advanced ) ? "                           For the vectorial case must be in raw format.      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Mn             - set no monotonicity constraints (default).         \n" : "" );
    output << ( (          advanced ) ? "         -Mi             - set monotonicity constraint increasing.            \n" : "" );
    output << ( (          advanced ) ? "         -Md             - set monotonicity constraint decreasing.            \n" : "" );
    output << ( (          advanced ) ? "                           (these constraints  are sufficient,  not necessary,\n" : "" );
    output << ( (          advanced ) ? "                           and  only  apply for  a  few  kernels  with  finite\n" : "" );
    output << ( (          advanced ) ? "                           dimensional feature maps.  They further assume that\n" : "" );
    output << ( (          advanced ) ? "                           all training vectors are elementwise non-negative).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Nl  f          - linear bias forcing term f, 1-class case (dflt 0). \n" : "" );
    output << ( (          advanced ) ? "         -Nq  f          - quadratic bias forcing f>=0, 1-class case (dft 0). \n" : "" );
    output << ( (          advanced ) ? "         -Nld d f        - linear bias forcing term f, multi case (dflt 0).   \n" : "" );
    output << ( (          advanced ) ? "         -Nqd d f        - quadratic bias forcing f>=0, multi case (deft 0).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -dd  d          - sets d in classify-with-reject  (binary and reduce-\n" : "" );
    output << ( (          advanced ) ? "                           to-binary multiclas only). Recommend combining with\n" : "" );
    output << ( (          advanced ) ? "                           -w 1 (defaults) or this  might not work  as you may\n" : "" );
    output << ( (          advanced ) ? "                           expect. For reduce-to-binary multiclass rejects are\n" : "" );
    output << ( (          advanced ) ? "                           labelled as anomalies if anomaly detection is used,\n" : "" );
    output << ( (          advanced ) ? "                           overriding the standard anomaly detector.  See:    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           o Bartlett et al \"Classification with Reject Option\n" : "" );
    output << ( (          advanced ) ? "                             using Hinge Loss\", JMLR 2008.                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -nm  m          - norm  used   when  calculating  margin   (any  even\n" : "" );
    output << ( (          advanced ) ? "                           positive  integer  is  allowed, but  training  time\n" : "" );
    output << ( (          advanced ) ? "                           increases drastically for m > 4.  In practice m = 4\n" : "" );
    output << ( (          advanced ) ? "                           is feasible for  moderate datasets,  m > 4 only for\n" : "" );
    output << ( (          advanced ) ? "                           small (toy) datasets).                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Notes: - suggest  using -om d  for speed  here (D2C\n" : "" );
    output << ( (          advanced ) ? "                           (m>=4)   won't actually get  used, but it stops the\n" : "" );
    output << ( (          advanced ) ? "                                    optimiser   attempting   to   maintain   a\n" : "" );
    output << ( (          advanced ) ? "                                    Cholesky factorisation of the Hessian).   \n" : "" );
    output << ( (          advanced ) ? "                                  - use of  -knn may  be required to  keep the\n" : "" );
    output << ( (          advanced ) ? "                                    Hessian from exploding.                   \n" : "" );
    output << ( (          advanced ) ? "                                  - large C values should be avoided.         \n" : "" );
    output << ( (          advanced ) ? "                                  - most kernels are fine up to about N = 250,\n" : "" );
    output << ( (          advanced ) ? "                                    but  suggest  using  -kan 2  -mtb  with  a\n" : "" );
    output << ( (          advanced ) ? "                                    distance based kernel form m > 4 to enable\n" : "" );
    output << ( (          advanced ) ? "                                    inner-product cacheing (or K4 pre-cacheing\n" : "" );
    output << ( (          advanced ) ? "                                    may become prohibitive).                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Fi  m          - max iterations m>=0 for iterative fuzzy training (0\n" : "" );
    output << ( (          advanced ) ? "                           for unlimited - default).                          \n" : "" );
    output << ( (          advanced ) ? "         -Flr r          - iterative fuzzy learning rate r>0 (default 0.3).   \n" : "" );
    output << ( (          advanced ) ? "         -Fzt t          - fuzzy zero tolerance t>=0 (default 0.01).          \n" : "" );
    output << ( (          advanced ) ? "         -Fc  $fn        - generalised cost fn of var(0,0) (default tanh(x)). \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -m   {r,m}      - hessian element type:                              \n" : "" );
    output << ( (          advanced ) ? "                           r - real-valued (default).                         \n" : "" );
    output << ( (          advanced ) ? "                           m - matrix/anion  valued  (vector/anion  regression\n" : "" );
    output << ( (          advanced ) ? "                               with  non-real  kernel  and  non-real  training\n" : "" );
    output << ( (          advanced ) ? "                               vectors).                                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -th  theta      - set theta (psd regularisation) for similarity.     \n" : "" );
    output << ( (          advanced ) ? "         -thn {0,1}      - set normalised (1, deft) or unnormal similarity.   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- KNN specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -k   k          - sets the number of neighours k in the KNN.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -K   i          - sets the KNN weight function.  The weight is:      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                             kappa(D(x))/kappabar                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           where D is set by the  kernel controls (this is the\n" : "" );
    output << ( ( basic || advanced ) ? "                           distance  metric),  normalised  to range  0->1, and\n" : "" );
    output << ( ( basic || advanced ) ? "                           kappa is set by this argument.  Options are:       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           0: kappa(d) = 1/2                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                           1: kappa(d) = 1-d                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                           2: kappa(d) = 3/4 (1-d^2)                          \n" : "" );
    output << ( ( basic || advanced ) ? "                           3: kappa(d) = 15/16 (1-d^2)^2                      \n" : "" );
    output << ( ( basic || advanced ) ? "                           4: kappa(d) = 35/32 (1-d^2)^3                      \n" : "" );
    output << ( ( basic || advanced ) ? "                           5: kappa(d) = pi/4 cos(pi/2 d)                     \n" : "" );
    output << ( ( basic || advanced ) ? "                           6: kappa(d) = 1/sqrt(2.pi) exp(d^2/2)              \n" : "" );
    output << ( ( basic || advanced ) ? "                           7: kappa(d) = 1/d                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- GP specific options                           --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -d   sigma      - sets the measurement noise for the GP.             \n" : "" );
    output << ( (          advanced ) ? "         -dw  i w        - set sigma weight w>0 for training vector i.        \n" : "" );
    output << ( (          advanced ) ? "         -ds  s          - Scale sigma/N by s>0.                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- IMP specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -iz  zref       - sets zref factor used by EHI.                      \n" : "" );
    output << ( ( basic || advanced ) ? "         -ie  i          - sets EHI calculation method.  Options are:         \n" : "" );
    output << ( ( basic || advanced ) ? "                           0: fully optimised recursive method.               \n" : "" );
    output << ( ( basic || advanced ) ? "                           1: partially optimised recursive method.           \n" : "" );
    output << ( ( basic || advanced ) ? "                           2: un-optimised recursive method.                  \n" : "" );
    output << ( ( basic || advanced ) ? "                           3: Hupkens IRS method.                             \n" : "" );
    output << ( ( basic || advanced ) ? "                           4: Couckuyt method with binary cells.              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- BLK specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -blx $fn        - set operating function for block (applied to output\n" : "" );
    output << ( ( basic || advanced ) ? "                           except  for  user  function  machine,  where  it is\n" : "" );
    output << ( ( basic || advanced ) ? "                           applied to all inputs).                            \n" : "" );
    output << ( ( basic || advanced ) ? "         -bly $fn        - set MEX callback  function for block  (used to calc\n" : "" );
    output << ( ( basic || advanced ) ? "                           g(x) for types mxa and mxb.                        \n" : "" );
    output << ( ( basic || advanced ) ? "         -blz i          - set integer argument for MEX callback function (for\n" : "" );
    output << ( ( basic || advanced ) ? "                           MEX  this is  >=0 for  commandline argument,  -1 to\n" : "" );
    output << ( ( basic || advanced ) ? "                           load  named external  variable,  -3  to run  matlab\n" : "" );
    output << ( ( basic || advanced ) ? "                           function.   External   variables  can  be  function\n" : "" );
    output << ( ( basic || advanced ) ? "                           handles.  Default is -3).                          \n" : "" );
    output << ( ( basic || advanced ) ? "         -bls fn         - set SYSTEM  call used  to calculate g(xx)  for sys.\n" : "" );
    output << ( ( basic || advanced ) ? "                           On call, x,y,z,... are  sustituted with values from\n" : "" );
    output << ( ( basic || advanced ) ? "                           from xx.  For example:                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           -bls \\[ \\\".\\/echoit.exe\\\" x \\\"\\>\\ temp.txt\\\" \\]    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           (note escape  characters - these may  or may not be\n" : "" );
    output << ( ( basic || advanced ) ? "                           required  depending on  the operating  environment)\n" : "" );
    output << ( ( basic || advanced ) ? "                           will  cause calls  to g(xx)  to be  evaluated using\n" : "" );
    output << ( ( basic || advanced ) ? "                           the system call:                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           ./echoit.exe x > temp.txt                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           where x is replaced with xx(0).  In this example we\n" : "" );
    output << ( ( basic || advanced ) ? "                           use the shortcut where a vector is converted to the\n" : "" );
    output << ( ( basic || advanced ) ? "                           elements concatenated with spaces between them.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -bfx $fn        - save x data to file $fn on sys g(x) if defined.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -bfy $fn        - save y data to file $fn on sys g(x) if defined.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -bfxy $fn       - save x,y data to file $fn on sys g(x) if defined.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -bfyx $fn       - save y,x data to file $fn on sys g(x) if defined.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -bfr $fn        - get result from file $fn on sys g(x) if defined.   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SSV specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -nzs i          - sets number of support vectors for SSV.            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -vlb [x]        - sets lower bound for support vectors for SSV.      \n" : "" );
    output << ( ( basic || advanced ) ? "         -vub [x]        - sets upper bound for support vectors for SSV.      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -sNl f          - (linear) bias forcing term f (default 0).          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- MLM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -mlc i c        - Set C (regularisation) value for layer i (deflt 1).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Kernel selection options (after learning options):                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** By default a  single kernel is  chosen.  More **         \n" : "" );
    output << ( (          advanced ) ? "                  ** complex kernel  dictionaries are  possible by **         \n" : "" );
    output << ( (          advanced ) ? "                  ** setting kernel dictionary  size > 1.  In this **         \n" : "" );
    output << ( (          advanced ) ? "                  ** case K(x,y) = w0.K0(x,y)  + w1.K1(x,y) + ..., **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where w0,w1,... are weights and K0,K1,... are **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the kernel fucntions.                         **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ki  i          - define which kernel elm i  is being set (default 0,\n" : "" );
    output << ( (          advanced ) ? "                           or 1 if this is an MLM non-input layer).           \n" : "" );
    output << ( (          advanced ) ? "         -ks  n          - set kernel dictionary size n (default 1).          \n" : "" );
    output << ( ( basic || advanced ) ? "         -kn             - set kernel normalised (this element).              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ku             - set kernel unnormalised (this element, default).   \n" : "" );
    output << ( ( basic || advanced ) ? "         -knn            - set kernel normalised (overall).                   \n" : "" );
    output << ( ( basic || advanced ) ? "         -kuu            - set kernel unnormalised (overall, default).        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -kg  g          - set x scale, non-ARD style ( x:= x/g ).            \n" : "" );
    output << ( ( basic || advanced ) ? "         -kgg g          - set x scale, ARD style ( x_i := x_i/g_i for all i).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -km             - modify so K(x,y) -> K(x,x).K(y,y).                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -kum            - undo -km (default).                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -kt  t          - type of kernel function:                           \n" : "" );
    output << ( ( basic || advanced ) ? "                           Kernels 0-99 are intended for ML use (deft 2).     \n" : "" );
    output << ( ( basic || advanced ) ? "                           Kernels 100-299 are intended for NN use (deft 201).\n" : "" );
    output << ( ( basic || advanced ) ? "                           Kernels 300-399 are intended for kNN use (dft 300).\n" : "" );
    output << ( ( basic || advanced ) ? "                           Defaults: 2 for most MLs.                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                     200 for density estimation.              \n" : "" );
    output << ( ( basic || advanced ) ? "                                     300 for K-nearest neighbours.            \n" : "" );
    output << ( ( basic || advanced ) ? "                                     200 for neural-networks.                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           For full list see -??k.                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    0   = Constant kernel: K(x,y) = r1                                        \n" : "" );
    output << ( ( basic || advanced ) ? "    1   = Linear kernel: K(x,y) = <x,y>/(r0.r0)                               \n" : "" );
    output << ( ( basic || advanced ) ? "    2   = Polynomial kernel: K(x,y) = ( r1 + <x,y>/(r0.r0) )^i0               \n" : "" );
    output << ( ( basic || advanced ) ? "    3   = Gaussian kernel: K(x,y) = exp(-||x-y||^2/(2*r0*r0)-r1)              \n" : "" );
    output << ( ( basic || advanced ) ? "    4   = Laplacian kernel: K(x,y) = exp(-||x-y||/r0-r1)                      \n" : "" );
    output << ( ( basic || advanced ) ? "    5   = Polynoise kernel: K(x,y) = exp(-||x-y||^r1/(r1*r0^r1)-r2)           \n" : "" );
    output << ( ( basic || advanced ) ? "    7   = Sigmoid kernel (CPD): K(x,y) = tanh( <x,y>/(r0.r0) + r1 )           \n" : "" );
    output << ( ( basic || advanced ) ? "    8   = Rational quadratic kernel: K(x,y) =( 1+||x-y||^2/(2*r0*r0*r1))^(-r1)\n" : "" );
    output << ( ( basic || advanced ) ? "    9   = Multiquadric kernel (NM): K(x,y) = sqrt(||x-y||^2/(r0.r0)+r1^2)     \n" : "" );
    output << ( ( basic || advanced ) ? "    10  = Inverse multiquadric kernel: K(x,y) = 1/sqrt(||x-y||^2/(r0*r0)+r1^2)\n" : "" );
    output << ( ( basic || advanced ) ? "    11  = Circular kernel (MR2): K(x,y) = 2/pi * ( arccos(-||x-y||/r0)        \n" : "" );
    output << ( ( basic || advanced ) ? "                                         - ||x-y||*sqrt(1-||x-y||^2/r0^2)/r0 )\n" : "" );
    output << ( ( basic || advanced ) ? "    12  = Spherical kernel (MR3): K(x,y) = 1 - 1.5*||x-y||/r0                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                         + 0.5*||x-y||^3/r0^3                 \n" : "" );
    output << ( ( basic || advanced ) ? "    13  = Wave kernel: K(x,y) = (r0/||x-y||).sin(||x-y||/r0)                  \n" : "" );
    output << ( ( basic || advanced ) ? "    14  = Power kernel: K(x,y) = -(||x-y||/r0)^r1                             \n" : "" );
    output << ( ( basic || advanced ) ? "    15  = Log kernel (CPD): K(x,y) = -log((||x-y||/r0)^r1 + 1)                \n" : "" );
    output << ( ( basic || advanced ) ? "    19  = Cauchy kernel: K(x,y) = 1/(1+((||x-y||^2/(r0.r0))))                 \n" : "" );
    output << ( ( basic || advanced ) ? "    23  = Generalised T-Student kernel: K(x,y) = 1/(1+||x-y||^r0)             \n" : "" );
    output << ( ( basic || advanced ) ? "    24  = Vovk's real polynomial: K(x,y)= (1-((<x,y>/r0^2)^i0))/(1-<x,y>/r0^2)\n" : "" );
    output << ( ( basic || advanced ) ? "    25  = Weak fourier kernel: K(x,y) = pi.cosh(pi-(||x-y||/r0))              \n" : "" );
    output << ( ( basic || advanced ) ? "    26  = Thin spline (1): K(x,y) = ((||x-y||/r0)^(r1+0.5))                   \n" : "" );
    output << ( ( basic || advanced ) ? "    27  = Thin spline (2): K(x,y) = ((||x-y||/r0)^r1)*ln(sqrt(||x-y||/r0))    \n" : "" );
    output << ( ( basic || advanced ) ? "    32  = Diagonal offset kernel: r1 if diagonal Hessian, 0 otherwise         \n" : "" );
    output << ( ( basic || advanced ) ? "    33  = Uniform kernel: K(x,y) = 1/2r0 if |||x-y||| < r0, 0 otherwise       \n" : "" );
    output << ( ( basic || advanced ) ? "    34  = Triang kernel: K(x,y) = 1/r0 (1-|||x-y|||/r0) if |||x-y||| < r0, 0  \n" : "" );
    output << ( ( basic || advanced ) ? "    35  = d-Matern kernel:                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "    37  = d+1/2-Matern kernel:                                                \n" : "" );
    output << ( ( basic || advanced ) ? "    38  = 1/2-Matern kernel: K(x,y) = exp(-||x-y||/r0)                        \n" : "" );
    output << ( ( basic || advanced ) ? "    39  = 3/2-Matern kernel: K(x,y) = (1 + sqrt(3)*||x-y||/r0).               \n" : "" );
    output << ( ( basic || advanced ) ? "                                      exp(-sqrt(3)*||x-y||/r0)                \n" : "" );
    output << ( ( basic || advanced ) ? "    40  = 5/2-Matern kernel: K = (1 + sqrt(5)*||x-y||/r0 + 5*||x-y||^2/r0^2). \n" : "" );
    output << ( ( basic || advanced ) ? "                                      exp(-sqrt(5)*||x-y||/r0)                \n" : "" );
    output << ( ( basic || advanced ) ? "    41  = RBF rescale kernel: K(x,y) = z^(1/(2*r0*r0)) = exp(log(z)/(2*r0*r0))\n" : "" );
    output << ( ( basic || advanced ) ? "    42  = Inverse gudermannian kernel: K(x,y) = igd(<x,y>/(r0.r0))            \n" : "" );
    output << ( ( basic || advanced ) ? "    43  = Log ratio kernel: K(x,y) = log((1+<x,y>/(r0.r0))/(1-<x,y>/(r0.r0))) \n" : "" );
    output << ( ( basic || advanced ) ? "    44  = Exponential kernel: K(x,y) = exp(<x,y>/(r0.r0)-r1)                  \n" : "" );
    output << ( ( basic || advanced ) ? "    45  = Hyperbolic sine kernel: K(x,y) = sinh(<x,y>/(r0.r0))                \n" : "" );
    output << ( ( basic || advanced ) ? "    46  = Hyperbolic cosine kernel: K(x,y) = cosh(<x,y>/(r0.r0))              \n" : "" );
    output << ( ( basic || advanced ) ? "    47  = Sinc kernel: K(x,y) = sinc(||x-y||/r0).cos(2*pi*||x-y||/(r0.r1))    \n" : "" );
    output << ( ( basic || advanced ) ? "    48  = LUT kernel: K(x,y) = r1((int) x, (int) y) if r1 is a matrix         \n" : "" );
    output << ( ( basic || advanced ) ? "                             = r1 if (int) x != (int) y and r1 not a matrix   \n" : "" );
    output << ( ( basic || advanced ) ? "                             = 1  if (int) x == (int) y and r1 not a matrix   \n" : "" );
    output << ( ( basic || advanced ) ? "               (this can be used eg to implement multitask learning with ICM).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    100 = Linear 0/1 neuron:    K(z) = z/(r0.r0)                              \n" : "" );
    output << ( ( basic || advanced ) ? "    101 = Logistic 0/1 neuron:  K(z) = 1/(1+exp(-r0.z))                       \n" : "" );
    output << ( ( basic || advanced ) ? "    102 = Gen. logistic 0/1:    K(z) = 1/(1+r1.exp(-r2.(z-r3)/(r0.r0)))^(1/r2)\n" : "" );
    output << ( ( basic || advanced ) ? "    103 = Heavyside 0/1 neuron: K(z) = 1 if real(z) > 0, 0 otherwise          \n" : "" );
    output << ( ( basic || advanced ) ? "    104 = ReLU 0/1 neuron:      K(z) = z/(r0.r0) if real(z) > 0, 0 otherwise  \n" : "" );
    output << ( ( basic || advanced ) ? "    105 = Softplus 0/1 neuron:  K(z) = ln(r1+exp(z/(r0.r0)))                  \n" : "" );
    output << ( ( basic || advanced ) ? "    106 = Leaky ReLU 0/1 neuron:K(z) = z/(r0.r0) if real(z) > 0               \n" : "" );
    output << ( ( basic || advanced ) ? "                                     = (r1*z)/(r0.r0) if real(z) <= 0         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    200 = Linear -1/+1 neuron:    K(z) = z/(r0.r0)-1                          \n" : "" );
    output << ( ( basic || advanced ) ? "    201 = Logistic -1/+1 neuron:  K(z) = 2/(1+exp(-z/(r0.r0))) -1             \n" : "" );
    output << ( ( basic || advanced ) ? "    202 = Gen. logistic -1/+1: K(z) = 2/(1+r1.exp(-r2.(z-r3)/(r0^2)))^(1/r2)-1\n" : "" );
    output << ( ( basic || advanced ) ? "    203 = Heavyside -1/+1 neuron: K(z) = 1   if real(z) > 0, -1 otherwise     \n" : "" );
    output << ( ( basic || advanced ) ? "    204 = ReLU -1/+1 neuron:      K(z) = z/(r0.r0)-1 if real(z) > 0, -1 other \n" : "" );
    output << ( ( basic || advanced ) ? "    205 = Softplus -1/+1 neuron:  K(z) = 2.ln(r1+exp(z/(r0.r0))) -1           \n" : "" );
    output << ( ( basic || advanced ) ? "    204 = Leaky ReLU -1/+1 neuron:K(z) = z/(r0.r0)-1 if real(z) > 0           \n" : "" );
    output << ( ( basic || advanced ) ? "                                       = (r1*z)/(r0.r0)-1 if real(z) <= 0     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    300 = Euclidean distance: K(x,y) = -1/2 ||x-y||_2^2/(r0.r0)               \n" : "" );
    output << ( ( basic || advanced ) ? "    301 = 1-norm distance:    K(x,y) = -1/2 ||x-y||_1^2/(r0.r0)               \n" : "" );
    output << ( ( basic || advanced ) ? "    302 = inf-norm distance:  K(x,y) = -1/2 ||x-y||_inf^2/(r0.r0)             \n" : "" );
    output << ( ( basic || advanced ) ? "    303 = 0-norm distance:    K(x,y) = -1/2 ||x-y||_0^2/(r0.r0)               \n" : "" );
    output << ( ( basic || advanced ) ? "    304 = p-norm distance:    K(x,y) = -1/2 ||x-y||_r1^2/(r0.r0)              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    8xx = kernel transfer (see below)                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    900 = evaluate kernel by sending x,y data to unix socket kerni0.sock.  See\n" : "" );
    output << ( ( basic || advanced ) ? "          kernel9xx function in mercer.cc for details.  Assumes K symmetric.  \n" : "" );
    output << ( ( basic || advanced ) ? "    901 = like 900, but assumes K anti-symmetric.                             \n" : "" );
    output << ( ( basic || advanced ) ? "    902 = like 900, but assumes K non-symmetric.                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        Notes: - ' indicates conjugate transpose                              \n" : "" );
    output << ( ( basic || advanced ) ? "               - ||x||^2 = conj(x)'x        (not the norm if (hyper-)complex).\n" : "" );
    output << ( ( basic || advanced ) ? "               - ||x-y||^2 = (x-y').(x-y')  (not the norm if (hyper-)complex).\n" : "" );
    output << ( ( basic || advanced ) ? "                           = ||x||^2 + ||y||^2 - 2<x,y>                       \n" : "" );
    output << ( ( basic || advanced ) ? "               - <x,y> = ( x'y + conj(x)'conj(y) )/2.                         \n" : "" );
    output << ( ( basic || advanced ) ? "               - for neural kernels, z=<x,y>.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -kg  x          - kernel parameter r0 = x (default 1).               \n" : "" );
    output << ( ( basic || advanced ) ? "         -kr  x          - kernel parameter r1 = x (default 0 or 1).          \n" : "" );
    output << ( ( basic || advanced ) ? "         -kf  $fn        - kernel param r10 = $fn (dft (var(0,1)+var(0,2))/2).\n" : "" );
    output << ( ( basic || advanced ) ? "         -kv  i x        - kernel parameter ri = x (default 0).               \n" : "" );
    output << ( ( basic || advanced ) ? "         -kd  x          - kernel parameter i0 = x (default 2).               \n" : "" );
    output << ( ( basic || advanced ) ? "         -kG  x          - kernel parameter i0 = x (default 1).               \n" : "" );
    output << ( ( basic || advanced ) ? "         -kV  i x        - kernel parameter ii = x (default 0).               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kI  v          - set kernels  indexing using  given index  vector v,\n" : "" );
    output << ( (          advanced ) ? "                           where  the  argument  is a  vector of  non-negative\n" : "" );
    output << ( (          advanced ) ? "                           integers   (eg [ 0 4 5 ])   in   increasing   order\n" : "" );
    output << ( (          advanced ) ? "                           corresponding to the indexes used.                 \n" : "" );
    output << ( (          advanced ) ? "         -kU             - set kernel unindexed.                              \n" : "" );
    output << ( (          advanced ) ? "         -kw  w          - set weight w>=0 of kernel function (default 1). The\n" : "" );
    output << ( (          advanced ) ? "                           weight can be anything (not just double), which may\n" : "" );
    output << ( (          advanced ) ? "                           be useful eg for matrix-valued kernels.            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ka  n          - number of samples used  when computing distribution\n" : "" );
    output << ( (          advanced ) ? "                           similarity (Muandet et al SMM).                    \n" : "" );
    output << ( (          advanced ) ? "         -kb  [ i j .. ] - indices of var(0,..) variables sampled.            \n" : "" );
    output << ( (          advanced ) ? "         -ke  [ fi fj ..]- distribution useds for var(0,..) ].                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Kernel  chaining  allows the  construction of **         \n" : "" );
    output << ( (          advanced ) ? "                  ** rudimentary deep kernels.  If a kernel elm is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** chained then rather than adding the output of **         \n" : "" );
    output << ( (          advanced ) ? "                  ** this element to the result it is instead used **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to calculate  the next kernel  element in the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** chain.   So if the  dictionary size  is 3, k0 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** is chained but k1 and k2 are not then:        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x,y) = k1(m0(x),m0(y)) + k2(x,y)            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where m0 is  the feature map  associated with **         \n" : "" );
    output << ( (          advanced ) ? "                  ** k0.  This only works  for kernels that can be **         \n" : "" );
    output << ( (          advanced ) ? "                  ** written as:                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** kn(x,y) = kn(||x||^2,||y||^2,<x,y>)           **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** as for example in the above example:          **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x,y) = k1(k0(x,x),k0(y,y),k0(x,y))+k2(x,y)  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (it's  a little  more  involved for  division **         \n" : "" );
    output << ( (          advanced ) ? "                  ** algebraic kernels, but that's essentially it) **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** NB: - chaining   is  only   implemented   for **         \n" : "" );
    output << ( (          advanced ) ? "                  **       standard 2-norm kernels.                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Example: -ks 3 -ki 0 -kc -kt 2 -kd 2 -ki 1    **         \n" : "" );
    output << ( (          advanced ) ? "                  **          -kt 7 -ki 2 -kt 1                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **   gives: k(x,y) = tanh(1+(1+<x,y>)^2) + <x,y> **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kc             - set kernel chained.                                \n" : "" );
    output << ( (          advanced ) ? "         -kuc            - set kernel unchained.                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Kernel splitting allows  different kernels to **         \n" : "" );
    output << ( (          advanced ) ? "                  ** applied to  different parts  of the  vectors. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** For example if splitting  is set of element 1 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** of the kernel with dictionary size 2 then:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x0 ~ x1,x2 ~ x3) = k1(x0,x2).k2(x1,x3)      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Note  that  this  is instead  of  the  multi- **         \n" : "" );
    output << ( (          advanced ) ? "                  ** instance  interpretation  -  you  cannot  use **         \n" : "" );
    output << ( (          advanced ) ? "                  ** both at the same time.  m-kernel splitting is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** also supported but is somewhat complicated.   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kS             - set kernel split.                                  \n" : "" );
    output << ( (          advanced ) ? "         -kuS            - set kernel unsplit (default).                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Kernel  multiply points allow  for support of **         \n" : "" );
    output << ( (          advanced ) ? "                  ** product kernels of the form:                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x,y) = k0(x,y).k0(x,y)....                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where k0 is specified  by elements 0 to first **         \n" : "" );
    output << ( (          advanced ) ? "                  ** element  with -kMS  set, k1  is specified  by **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the  element  immediately  after this  to the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** next with -kMS set,  and so on (final kn goes **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to end of elements).                          **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** NB: on  evaluation, kernel is  first split at **         \n" : "" );
    output << ( (          advanced ) ? "                  **     at  multiply points,  then the  fragments **         \n" : "" );
    output << ( (          advanced ) ? "                  **     are split at split  points, then chaining **         \n" : "" );
    output << ( (          advanced ) ? "                  **     occurs.                                   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kMS            - set kernel multiply point.                         \n" : "" );
    output << ( (          advanced ) ? "         -kMuS           - set kernel non-multiply point.                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** These options  allow kernel  parameters to be **         \n" : "" );
    output << ( (          advanced ) ? "                  ** taken from elements  of the training vectors. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** That is, for example:                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **    ri = conj(xj).yj                           **         \n" : "" );
    output << ( (          advanced ) ? "                  **   (ri = yj for neural networks)               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** This allows for example the RBF width to be a **         \n" : "" );
    output << ( (          advanced ) ? "                  ** function of position  in input space tuned on **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the density of points.   The kernel weight is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** parameter r-1 (-ko -1 ...).                   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ko  i j        - replace parameter  ri with input  product xj.yj (or\n" : "" );
    output << ( (          advanced ) ? "                           just yj for neural networks).                      \n" : "" );
    output << ( (          advanced ) ? "         -kO  i j        - replace  parameter ii  with input  (int) xj.yj  (or\n" : "" );
    output << ( (          advanced ) ? "                           just (int) yj for neural networks).                \n" : "" );
    output << ( (          advanced ) ? "         -koz            - delete all defined ri parameter replacements.      \n" : "" );
    output << ( (          advanced ) ? "         -kOz            - delete all defined ii parameter replacements.      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** When computing m-kernels we replace ||x-y||^2 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** with one of:                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 0: ||x||_m^m + ||y||_m^m +..- m.<<x,y,...>>_m **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1: ||x||_p^2 + ||y||_p^2 +..- p.<<x,y,...>>_m **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2: ||x||_p^2 + ||y||_p^2 +..-                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                    (1/m).(sum_{ij} <xi,xj>))  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Alternatively  you  can   use  a  moment-like **         \n" : "" );
    output << ( (          advanced ) ? "                  ** kernel:                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x0,x1,...) = D sum_{s} K(||x{s}||_p^2)      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where: x{s} = sum_i s_i x_i                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **        s = [ +-1 +-1 ... ] has dim m          **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** The following variants are available:         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 103: D = 1/2^{m-1}                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **      s : |i:si=+1| + |i:si=-1| in 4Z_+        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 104: D = 1/m!                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **      s : |i:si=+1| = |i:si=-1|                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 203: like 103,  but expansion  only occurs on **         \n" : "" );
    output << ( (          advanced ) ? "                  **      first kernel in chain.                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 204: like 104,  but expansion  only occurs on **         \n" : "" );
    output << ( (          advanced ) ? "                  **      first kernel in chain.                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 300: true moment-kernel expansion.            **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kan i          - Set difference definition (default 1).             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Assumptions: these can speed up optimisation, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** but make  sure they're  valid and  disable if **         \n" : "" );
    output << ( (          advanced ) ? "                  ** they are not.                                 **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -mtb            - always use  inner-product cache for speed.  This is\n" : "" );
    output << ( (          advanced ) ? "                           faster  if you're  doing kernel  tuning,  but  uses\n" : "" );
    output << ( (          advanced ) ? "                           almost double  the memory and offers  no speedup if\n" : "" );
    output << ( (          advanced ) ? "                           kernel is not tuned.                               \n" : "" );
    output << ( (          advanced ) ? "         -bmx            - save memory by not using inner-prd cache (default).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
//    output << ( (          advanced ) ? "         -kcy            - enable re-calc  of <x,y>,  ||x-y|| (where possible)\n" : "" );
//    output << ( (          advanced ) ? "                           when adjusting  kernel parameters.  Note  that this\n" : "" );
//    output << ( (          advanced ) ? "                           does not  guarantee  re-calc  will  happen  for all\n" : "" );
//    output << ( (          advanced ) ? "                           kernels (eg <x,y>+b  or  ||x-y||  cannot  always be\n" : "" );
//    output << ( (          advanced ) ? "                           calculated from K(x,y), this won't help when moving\n" : "" );
//    output << ( (          advanced ) ? "                           from an  inner product to  distance-based kernel or\n" : "" );
//    output << ( (          advanced ) ? "                           vice-versa, and  is incompatible  with shift/scale)\n" : "" );
//    output << ( (          advanced ) ? "                           and indexing).  Also  the speed-up  gained is often\n" : "" );
//    output << ( (          advanced ) ? "                           small  unless dealing  with kernel  transfer with a\n" : "" );
//    output << ( (          advanced ) ? "                           non-trivial base kernel inheritance.  Also be aware\n" : "" );
//    output << ( (          advanced ) ? "                           that this will double the memory used by the kernel\n" : "" );
//    output << ( (          advanced ) ? "                           cache when the kernel is changed.                  \n" : "" );
//    output << ( (          advanced ) ? "         -kcn            - disable <x,y> and ||x-y|| re-calc (default).       \n" : "" );
//    output << ( (          advanced ) ? "                                                                              \n" : "" );
//    output << ( (          advanced ) ? "                  ** NOTE: rather than use -kcy,  consider using a **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** mer block (-z  mer) with an  appropriate size **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** cache.  For  example,  assuming ML 0  is type **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** mer with a  linear kernel (-kt 1)  with cache **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** size n (-mc n)  then the following  will make **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** use of the  cacheing of  <x,y> inside  an RBF **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** kernel:                                       **         \n" : "" );
//    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** -ks 2 -ki 0 -kc -kt 800 -ktx 0 -ki 1 -kt 3 -kg 1 **      \n" : "" );
//    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** So the kernel is k1(k0(x,y)),  where k1 is an **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** RBF (set by -ki 1  -kt 3 -kg 1) and k0 refers **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** to ML  0 (set by  -ki 0  -kt 800 -ktx 0,  -kc **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** meaning chained), where k0 caches results.    **         \n" : "" );
//    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- Kernel transfer                               --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Kernel  transfer  is  a   method  for  taking **         \n" : "" );
    output << ( (          advanced ) ? "                  ** features learnt in  training an ML and coding **         \n" : "" );
    output << ( (          advanced ) ? "                  ** it into a kernel that  can be used elsewhere. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** For  example for  an SVM  with  kernel  K the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** transferred kernel Kx is defined to be:       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Kx(y,z) = sum_ij alpha_i alpha_j K(y,z,xi,xj) **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where K here is  interpretted as an m-kernel. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** You  can  do this  for  multiple  levels  and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** treat  them like  any other  kernel.  To  use **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the  kernel  so constructed  from an  ML  set **         \n" : "" );
    output << ( (          advanced ) ? "                  ** kernel 8xx (-kt 8xx)  and the ML number using **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -ktx  i (i  is the  ML number  providing Kx). **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Available kernels are (Kx is kernel of ML i): **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 800: Trivial: K(x,y) = Kx(x,y)                **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 801: m-norm:                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = sum_ij ai aj Kx(x,y,xi,xj)      **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where for SVM ai = alpha_i for xi        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 802: Moment:                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = sum_ij ai aj Kx(x,xi) Kx(y,xj)  **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where for SVM a_i = alpha_i for x_i      **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 803: reserved.                                **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 804: K-learn:                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = sum_i ai Kx(xi,(x,y))           **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where for SVM a_i = alpha_i for x_i      **         \n" : "" );
    output << ( (          advanced ) ? "                  **      Typically xi = (xai,xbi)                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 805: K2-learn:                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = (sum_i ai Kx(xi,(x,y)))^2       **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where for SVM a_i = alpha_i for x_i      **         \n" : "" );
    output << ( (          advanced ) ? "                  **      Typically xi = (xai,xbi)                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 806: Traditional multilayer network:          **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = Kx(f(x),f(y))                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 81x: like 80x, but assumes  a common dataset, **         \n" : "" );
    output << ( (          advanced ) ? "                  **      so indices are passed through and caches **         \n" : "" );
    output << ( (          advanced ) ? "                  **      used.                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Example: suppose ML 0  is a trained  SVM with **         \n" : "" );
    output << ( (          advanced ) ? "                  **      kernel 3 (RBF), and we are setting up ML **         \n" : "" );
    output << ( (          advanced ) ? "                  **      1s kernel using the command:             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      -ks 2 -ki 0 -kc -kt 801 -ktx 0 -ki 1     **         \n" : "" );
    output << ( (          advanced ) ? "                  **      -kt 2 -kd 2                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      gives you the kernel function:           **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = (K0(x,y)+1)^2                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where:                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K0(x,y) = sum_ij ai aj Kr(x,y,ui,uj)     **         \n" : "" );
    output << ( (          advanced ) ? "                  **      Kr(x,y,u,v) = exp(4<x,y,u,v>-x4-y4-uU-vV)**         \n" : "" );
    output << ( (          advanced ) ? "                  **      ai = alpha_i for ML 0                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **      ui = x_i for ML 0                        **         \n" : "" );
    output << ( (          advanced ) ? "                  **      uU = ||u||_4^4                           **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      That is,  m(x) (the  feature  map of the **         \n" : "" );
    output << ( (          advanced ) ? "                  **      the rbf  kernel Kr(x,y))  is elementwise **         \n" : "" );
    output << ( (          advanced ) ? "                  **      weighted  to  give   w.*m(x)  using  the **         \n" : "" );
    output << ( (          advanced ) ? "                  **      weights  found by  the  SVM  ML 0,  then **         \n" : "" );
    output << ( (          advanced ) ? "                  **      mapped using  n (the feature  map of the **         \n" : "" );
    output << ( (          advanced ) ? "                  **      second order  polynomial kernel) to give **         \n" : "" );
    output << ( (          advanced ) ? "                  **      the  composite  map n(w.m(x)),  which is **         \n" : "" );
    output << ( (          advanced ) ? "                  **      the feature map embodied by K(x,y).      **         \n" : "" );
//    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** Note: in the above example (or similar) it is **         \n" : "" );
//    output << ( (          advanced ) ? "                  **      useful  to apply  -kcy  when  optimising **         \n" : "" );
//    output << ( (          advanced ) ? "                  **      kernel hyper-parameters to prevent slow- **         \n" : "" );
//    output << ( (          advanced ) ? "                  **      down due to thrashing when recalculating **         \n" : "" );
//    output << ( (          advanced ) ? "                  **      the inner kernel K0 (-kcy caches this).  **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ktx i          - obtain (transfer) this kernel from ML i.           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- MLM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ktk i          - which layer kernel is set above (-1 output, deflt).\n" : "" );
    output << ( ( basic || advanced ) ? "                           Use -ktk -1 to adjust the inheritance type (must be\n" : "" );
    output << ( ( basic || advanced ) ? "                           a type 8xx kernel, default is 802).                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- gentype specific options                      --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** When dealing  with generic targets  MLs there **         \n" : "" );
    output << ( (          advanced ) ? "                  ** may also be kernel defined on target space to **         \n" : "" );
    output << ( (          advanced ) ? "                  ** measure  similarity.   To modify  this kernel **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the above commands but with the prefix -e.    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** For example:                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **  -ekt sets output kernel type                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **  -eks seta parameter r0 for kernel            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** etc.                                          **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -e...   sets kernel parameters for output kernels.                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Automatic parameter tuning options (after kernel selection):                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -bal            - for all classes i, set Ci = N/Ni.                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** These  options set  parameters in  such a way **         \n" : "" );
    output << ( (          advanced ) ? "                  ** that  they  will  change  automatically  when **         \n" : "" );
    output << ( (          advanced ) ? "                  ** relevant parameters (N,kern) are changed.  So **         \n" : "" );
    output << ( (          advanced ) ? "                  ** for example -NlA will modify learning options **         \n" : "" );
    output << ( (          advanced ) ? "                  ** appropriately when additionl training vectors **         \n" : "" );
    output << ( (          advanced ) ? "                  ** are  added or  removed.  More  importantly it **         \n" : "" );
    output << ( (          advanced ) ? "                  ** will  automatically   modify  the  parameters **         \n" : "" );
    output << ( (          advanced ) ? "                  ** during n-fold-error analysis, for example.    **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -cA             - C/N = 1/(N*mean(kern(i,i))) (updated auto).        \n" : "" );
    output << ( (          advanced ) ? "         -cB             - C/N = 1/(N*median(kern(i,i))) (updated auto).      \n" : "" );
    output << ( (          advanced ) ? "         -cAN            - C/N = 1/mean(kern(i,i)) (updated auto).            \n" : "" );
    output << ( (          advanced ) ? "         -cBN            - C/N = 1/median(kern(i,i)) (updated auto).          \n" : "" );
    output << ( (          advanced ) ? "         -cX  x          - C/N = x/N (updated auto).                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -NlA nu C       - automatically  set  linear  bias  forcing  for  the\n" : "" );
    output << ( (          advanced ) ? "                           1-class and  CS++-SVM given  nu and C as  for those\n" : "" );
    output << ( (          advanced ) ? "                           formulations,  with  N and  n as  for  the  current\n" : "" );
    output << ( (          advanced ) ? "                           trained SVM.  For  the CS++-SVM this  is equivalent\n" : "" );
    output << ( (          advanced ) ? "                           to:                                                \n" : "" );
    output << ( (          advanced ) ? "                           -c C(n-1)/(N.nu) -w sqrt((n-1)/(2.(n-2))) -Nld d C \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           where d is  as set  by the  -Acz d  call.  For  the\n" : "" );
    output << ( (          advanced ) ? "                           1-class SVM this is equivalent to:                 \n" : "" );
    output << ( (          advanced ) ? "                           -c C/(N.nu) -w 0 -Nl -C                            \n" : "" );
    output << ( (          advanced ) ? "                           (update auto).                                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -cua            - turns off auto update if currently in use.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Grid search parameter selection (after automatic parameter tuning):           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -g   ...        - select  given parameters  using a  grid  search  to\n" : "" );
    output << ( (          advanced ) ? "                           minimise some error measure.  Arguments are:       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           -g nargs $evalstring $setstring ...                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           where evalstring is the string used when evaluating\n" : "" );
    output << ( (          advanced ) ? "                           a  particular  choice and  setstring  is used  when\n" : "" );
    output << ( (          advanced ) ? "                           setting the final optimal  parameter choice.  nargs\n" : "" );
    output << ( (          advanced ) ? "                           sets the number  of parameters being  tuned, and in\n" : "" );
    output << ( (          advanced ) ? "                           the strings  themselves  the  variable  var(0,n) is\n" : "" );
    output << ( (          advanced ) ? "                           used to represent  these parameters,  where n is an\n" : "" );
    output << ( (          advanced ) ? "                           argnum except 0.  So for example:                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -g 2 \"-c y -kd z -tx\" \"-c y -kd z\" ...                           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           tells svmheavy to  do a grid search  over C and the\n" : "" );
    output << ( (          advanced ) ? "                           kernel parameter d (r2), as represented by var(0,1)\n" : "" );
    output << ( (          advanced ) ? "                           (y) and  var(0,2) (z)  respectively,and  then set C\n" : "" );
    output << ( (          advanced ) ? "                           and d to the  optimal value resulting  from this as\n" : "" );
    output << ( (          advanced ) ? "                           measured using leave-one-out error (-tx).          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           The  ranges are  set in  quadruples,  one for  each\n" : "" );
    output << ( (          advanced ) ? "                           var(0,n), after the first three arguments, namely: \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           ... = t n m M I                                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           where t sets the argument type, n is the arg num, m\n" : "" );
    output << ( (          advanced ) ? "                           is the minimum  value, M the maximum  value, with I\n" : "" );
    output << ( (          advanced ) ? "                           steps.  Valid options for the type t are:          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           zb - parameter is integer, linear increments.      \n" : "" );
    output << ( (          advanced ) ? "                           zl - parameter is integer, logarithmic increments. \n" : "" );
    output << ( (          advanced ) ? "                           za - parameter is integer, exponential increments. \n" : "" );
    output << ( (          advanced ) ? "                           zc - parameter is integer, inverse logistic incr.  \n" : "" );
    output << ( (          advanced ) ? "                           zr - parameter is integer, random increms (-g only)\n" : "" );
    output << ( (          advanced ) ? "                           fb - parameter is real, linear increments.         \n" : "" );
    output << ( (          advanced ) ? "                           fl - parameter is real, logarithmic increments.    \n" : "" );
    output << ( (          advanced ) ? "                           fa - parameter is real, exponential increments.    \n" : "" );
    output << ( (          advanced ) ? "                           fc - parameter is real, inverse logistic incr.     \n" : "" );
    output << ( (          advanced ) ? "                           fr - parameter is real, random increments (-g only)\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           so the grid search  is done on t in  the range 0,1,\n" : "" );
    output << ( (          advanced ) ? "                           and t = 0.01 => x = m,  t = 0.99 => x = M, t = 0 =>\n" : "" );
    output << ( (          advanced ) ? "                           x < -1e12, t = 1 => x > 1e12.                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           In the above example we might say:                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -g 2 \"-c y -kd z -tx\" \"-c y -kd z\" fl 1 1e-2 1e2 10 zb 2 1 5 5   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           which tells svmheavy to do a grid search over C and\n" : "" );
    output << ( (          advanced ) ? "                           kernel parameter  d, with C ranging  over 10 values\n" : "" );
    output << ( (          advanced ) ? "                           on a log scale from 0.01 to 100 and d Selected_from\n" : "" );
    output << ( (          advanced ) ? "                           1,2,3,4,5, find  the optimal selection  to minimise\n" : "" );
    output << ( (          advanced ) ? "                           leave-one-out error, then set C and d to the values\n" : "" );
    output << ( (          advanced ) ? "                           so selected.                                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           In the  case of a  non-unique minimum,  the minimum\n" : "" );
    output << ( (          advanced ) ? "                           closest  to the  centre  of the  grid is  selected.\n" : "" );
    output << ( (          advanced ) ? "                           This is based on  the assumption that  the grid has\n" : "" );
    output << ( (          advanced ) ? "                           been chosen  such that the central  values are more\n" : "" );
    output << ( (          advanced ) ? "                           practical (or  likely) than the edge  values.  This\n" : "" );
    output << ( (          advanced ) ? "                           distance is stored as var(1,0).                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           These  functions can  do more  than just  parameter\n" : "" );
    output << ( (          advanced ) ? "                           selection - they  can also  do global  optimisation\n" : "" );
    output << ( (          advanced ) ? "                           more generally.  For example the command:          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -gd 2 \"-tM y^2+(z-1)^2\" \"-echo y -echo z\" fb 1 -2 3 1 fb 2 -2 4 1\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           will minimise the function y^2+(z-1)^2 using DIRect\n" : "" );
    output << ( (          advanced ) ? "                           and echo the result (note that y = var(0,1) and z =\n" : "" );
    output << ( (          advanced ) ? "                           var(0,2); and that  x = var(0,0) cannot  be used as\n" : "" );
    output << ( (          advanced ) ? "                           it is reserved).  See -tM for details.  Another eg:\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -gb 1 \"-fu 2 22 y -Zx -tM -z\" \"-echo y\" fb 1 0 1 1               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           will  maximise  test  function  22  using  Bayesian\n" : "" );
    output << ( (          advanced ) ? "                           optimisation.  Finally, in mex,  if f is a function\n" : "" );
    output << ( (          advanced ) ? "                           handle - eg:                                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             f = @(x) x(1)^2+(x(2)-1)^2                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           (note that it takes a vector argument) then you can\n" : "" );
    output << ( (          advanced ) ? "                           minimise in matlab using for example:              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                            svmmatlab('-gd 2 \"-fWM 4 0 [ y z ] -tM var(0,4)\"  \n" : "" );
    output << ( (          advanced ) ? "                             \"-echo y -echo z\" fb 1 -2 3 1 fb 2 -2 4 1',1,f)  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Alternatively if you have a function bayestest.m:  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             function x = bayesTest(y,z)                      \n" : "" );
    output << ( (          advanced ) ? "                             x = y^2+(z-1)^2;                                 \n" : "" );
    output << ( (          advanced ) ? "                             end                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Then you could use:                                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             f = @(x) bayestest(x(1),x(2))                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Apart from grid optimisation the following exist:  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gd  ...        - like  above, but uses DIRect global optimiser.     \n" : "" );
    output << ( (          advanced ) ? "         -gN  ...        - like  above, but uses Nelder-Mead local optimiser. \n" : "" );
    output << ( (          advanced ) ? "         -gb  ...        - like  above, but uses Bayesian optimisation.       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Overall algorithm  (steps in brackets  for Bayesian\n" : "" );
    output << ( (          advanced ) ? "                           or model-based only):                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                          (0. Set up models for Bayesian optimisation.)       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           1. Set up  projections.  The core  optimiser always\n" : "" );
    output << ( (          advanced ) ? "                              sees  a finite  dimensional  problem  on [0,1]^d\n" : "" );
    output << ( (          advanced ) ? "                              with linear scaling, specifically:              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - min_{x in [0,1]^d} f(q(p(x)))                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              where:                                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - p : [0,1]^d  ->  R^d is  a finite  dimensional\n" : "" );
    output << ( (          advanced ) ? "                                projection operator  defined by fb,fl,... etc.\n" : "" );
    output << ( (          advanced ) ? "                                z... variants just round to nearest integer.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - q : R^d -> F is present  for random projection\n" : "" );
    output << ( (          advanced ) ? "                                and functional optimisation. See -gp -gP, ....\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           2. Run $pstring defined by -gtp.   This can be used\n" : "" );
    output << ( (          advanced ) ? "                              to set  non-standard  kernels  etc for  Bayesian\n" : "" );
    output << ( (          advanced ) ? "                              optimisation and  functional optimisation.   The\n" : "" );
    output << ( (          advanced ) ? "                              following indices are set (-1 where/if n/a):    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,0): model for Bayesian optimisation.   \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,1): noise model for Bayesian optim..   \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,2): weighting projection template.     \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,5): source data model (env-GP,diff-GP).\n" : "" );
    output << ( (          advanced ) ? "                              - var(90,6): difference data model (diff-GP).   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              var(90,2)  and  var(90,3) are  used  for  random\n" : "" );
    output << ( (          advanced ) ? "                              projection and functional  analysis, where q has\n" : "" );
    output << ( (          advanced ) ? "                              the form:                                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - q(x)(t) = [ x_0.q_0(t) + x_1.q_1(t) + ... ]   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              where:                                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - x is the vector p(x) in R^d.                  \n" : "" );
    output << ( (          advanced ) ? "                              - q_i is a draw from ML var(90,3).              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              Typically ML  var(90,3) will be  a random vector\n" : "" );
    output << ( (          advanced ) ? "                              distribution, a GP  (function distribution) or a\n" : "" );
    output << ( (          advanced ) ? "                              set of basis functions (eg Bernstein polys).    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           3. Random  projection:  draw  q_0,  q_1,  ...  from\n" : "" );
    output << ( (          advanced ) ? "                              defined distribution (template ML var(90,3)).   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           4. Run $mstring defined by  -gtP.  This can be used\n" : "" );
    output << ( (          advanced ) ? "                              used  for  things  like  tweaking  the  template\n" : "" );
    output << ( (          advanced ) ? "                              distributions between inner-loop opts, so for eg\n" : "" );
    output << ( (          advanced ) ? "                              you could start with a very smooth SE kernel and\n" : "" );
    output << ( (          advanced ) ? "                              gradually make  it sharper.  The  following vars\n" : "" );
    output << ( (          advanced ) ? "                              are defined here (-1 if n/a):                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,0): model for Bayesian optimisation.   \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,1): noise model for Bayesian optim.    \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,2): weighting projection template.     \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,3): current q(t) function.             \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,4): iteration  count  (0 first  time, 1\n" : "" );
    output << ( (          advanced ) ? "                                           second time etc... up to -gpr).    \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,5): source data model (env,diff-GP).   \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,6): difference data model (diff-GP).   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              Note  that   changes  here  affect   the  *next*\n" : "" );
    output << ( (          advanced ) ? "                              projection step 3, not the current one.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           5. Inner  optimiser to  solve  using relevant  alg,\n" : "" );
    output << ( (          advanced ) ? "                              where evaluation of f calls $evalstring:        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - min_{x in [0,1]^d} f(q(p(x)))                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              Available variables are as-per step 4.          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           6. Repeat from step 3 as defined by -gpr, ie:      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - -gpr 0: run steps 3-5 once.                   \n" : "" );
    output << ( (          advanced ) ? "                              - -gpr 1: run steps 3-5 twice.                  \n" : "" );
    output << ( (          advanced ) ? "                              - -gpr 2: run steps 3-5 thrice.                 \n" : "" );
    output << ( (          advanced ) ? "                                   ...                                        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Notes:                                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - be wary  of using  -fo, -foe, -AAi...  and -tI...\n" : "" );
    output << ( (          advanced ) ? "                             here as they will  not work as  expected (vectors\n" : "" );
    output << ( (          advanced ) ? "                             taken out of  files in one iteration will  not be\n" : "" );
    output << ( (          advanced ) ? "                             put back for the next).                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - $evalstring etc work  as function calls.  Changes\n" : "" );
    output << ( (          advanced ) ? "                             to (non-global)  variables made  during each call\n" : "" );
    output << ( (          advanced ) ? "                             will not  be saved.  If you  want to  return vars\n" : "" );
    output << ( (          advanced ) ? "                             from these calls use global variables - eg:      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -gb 2 \"-tM y^2+(z-1)^2\" \"-fWG 10 y -fWG 11 z\" fb 1 -2 3 1        \n" : "" );
    output << ( (          advanced ) ? "                                                                   fb 2 -2 4 1\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             will store  y and z  in global  vars that  can be\n" : "" );
    output << ( (          advanced ) ? "                             retrieved, for example using:                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    -fWg 1 var(0,10) -fWg 2 var(0,11)         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - multi-objective  optimisation  is possible  using\n" : "" );
    output << ( (          advanced ) ? "                             the  Bayesian optimiser  and  an appropriate  IMP\n" : "" );
    output << ( (          advanced ) ? "                             (see -gbq below) and the -tm ... option, e.g.    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    -tm [ var(1,37) var(1,42) ] -tc 5         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             evaluates   performance    with   5-fold   cross-\n" : "" );
    output << ( (          advanced ) ? "                             validation and  seeks to  minimise both  negative\n" : "" );
    output << ( (          advanced ) ? "                             accuracy   (var(1,37))  and   negative   sparsity\n" : "" );
    output << ( (          advanced ) ? "                             (var(1,42))  (which is  equivalent to  maximising\n" : "" );
    output << ( (          advanced ) ? "                             accuracy and sparsity.  The Pareto set is written\n" : "" );
    output << ( (          advanced ) ? "                             to logfilename.pareto).                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - complete  results  are   stored  in  grid  files.\n" : "" );
    output << ( (          advanced ) ? "                             ....xgrid  is  the  x   values  tested  (inputs),\n" : "" );
    output << ( (          advanced ) ? "                             ....fgrid  is the  f(x)  values found  (outputs),\n" : "" );
    output << ( (          advanced ) ? "                             ....mgrid  is f(x)  modified for  feasibility and\n" : "" );
    output << ( (          advanced ) ? "                             unscented   optimisation,    and   ...sgrid   are\n" : "" );
    output << ( (          advanced ) ? "                             suplementary result, the  nature of which depends\n" : "" );
    output << ( (          advanced ) ? "                             on  the specific optimiser used. In each file the\n" : "" );
    output << ( (          advanced ) ? "                             optimum  value  (or  values  for  multi-objective\n" : "" );
    output << ( (          advanced ) ? "                             optimisation)   are  marked   with  an   asterix.\n" : "" );
    output << ( (          advanced ) ? "                             ....xpareto,    ....fpareto,   ....mpareto    and\n" : "" );
    output << ( (          advanced ) ? "                             ....spareto are also written  containing just the\n" : "" );
    output << ( (          advanced ) ? "                             pareto set and associated.                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             Supplementary results are (bayesian):            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    [ tstart = start time of iteration (sec) ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ tend   = end time of iteration (sec)   ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ nrec   = rec. number in this batch     ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ beta   = beta value for this batch     ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ mu     = mu(x) for this recomm.        ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ sigma  = sigma(x) for this recomm.     ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ UCB    = UCB(x) for this recomm.       ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ LCB    = LCB(x) for this recomm.       ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ DVAR   = DVAR(x) for this recomm.      ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ UDIST  = UDIST(x) for this recomm.     ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ r      = r(x) for this recomm.         ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ dtime  = DIRect run time (sec).        ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ mutime = mu GP training time (sec).    ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ sitime = sigma GP training time (sec). ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ ftime  = function evaluation time (sec)]\n" : "" );
    output << ( (          advanced ) ? "                                    [ gridi  = grid index of point (if grid) ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ gridy  = grid value of point (if grid) ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ fvar   = variance of f (if available)  ]\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             noting that mu is modelled on -f(x); and:        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    UCB(x)   = mu(x) + sqrt(beta).sigma(x)    \n" : "" );
    output << ( (          advanced ) ? "                                    LCB(x)   = mu(x) - sqrt(beta).sigma(x)    \n" : "" );
    output << ( (          advanced ) ? "                                    DVAR(x)  = 2.sqrt(beta).sigma(x)          \n" : "" );
    output << ( (          advanced ) ? "                                    UDIST(x) = -fmin - -f(x)                  \n" : "" );
    output << ( (          advanced ) ? "                                    r(x)     = min(UDIST(X),DVAR(X))          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - also saved in ....xygrid,  which is in y x format\n" : "" );
    output << ( (          advanced ) ? "                             ready to train another  model, and xytgrid, which\n" : "" );
    output << ( (          advanced ) ? "                             includes variance for GP training.               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - you can  recurse these  (grid search  within grid\n" : "" );
    output << ( (          advanced ) ? "                             search) to arbitrary depth.                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - when evaluating the setstring and  after the vars\n" : "" );
    output << ( (          advanced ) ? "                             that have been optimised are stored:             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    var(50,0): optimal x vector.              \n" : "" );
    output << ( (          advanced ) ? "                                    var(51,0): optimal f(x).                  \n" : "" );
    output << ( (          advanced ) ? "                                    var(52,0): optimal index.                 \n" : "" );
    output << ( (          advanced ) ? "                                    var(53,0): optimal supplement result.     \n" : "" );
    output << ( (          advanced ) ? "                                    var(54,0): optimal hypervolume.           \n" : "" );
    output << ( (          advanced ) ? "                                    var(55,0): mean f(x) (see -gr).           \n" : "" );
    output << ( (          advanced ) ? "                                    var(56,0): reserved for mean hypervolume. \n" : "" );
    output << ( (          advanced ) ? "                                    var(57,0): mean index (time to min f(x)). \n" : "" );
    output << ( (          advanced ) ? "                                    var(58,0): mean iterations to softmin.    \n" : "" );
    output << ( (          advanced ) ? "                                    var(59,0): mean iterations to hardmin.    \n" : "" );
    output << ( (          advanced ) ? "                                               (variances in var(..,65536)).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    var(6x,...): as  for var(5x,...),  but all\n" : "" );
    output << ( (          advanced ) ? "                                                 pareto-optimal results.      \n" : "" );
    output << ( (          advanced ) ? "                                    var(7x,...): as  for var(6x,...),  but all\n" : "" );
    output << ( (          advanced ) ? "                                                 results are included.        \n" : "" );
    output << ( (          advanced ) ? "                                    var(8x,0): as for var(7x,...) in one.     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    var(90,0): SMBO model index (or -1).      \n" : "" );
    output << ( (          advanced ) ? "                                    var(90,1): SMBO sigma model index (or -1).\n" : "" );
    output << ( (          advanced ) ? "                                    var(90,2): optimal function index (or -1).\n" : "" );
    output << ( (          advanced ) ? "                                    var(90,3): functional model.              \n" : "" );
    output << ( (          advanced ) ? "                                    var(90,4): iteration count.               \n" : "" );
    output << ( (          advanced ) ? "                                    var(90,5): source data model (env,dif-GP).\n" : "" );
    output << ( (          advanced ) ? "                                    var(90,6): difference data model (dif-GP).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             Note that var(50,0) and var(53,0) are expanded.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Non-trivial optimisation examples:            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Minimise   y^2 + (z-1)^2  using   Bayesian **         \n" : "" );
    output << ( (          advanced ) ? "                  **    optimisation:                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gmd 0.01 -gbH 3 -gb 2 \"-tM y^2+(z-1)^2\"      **         \n" : "" );
    output << ( (          advanced ) ? "                  **  \"-echo y -echo z\" fb 1 -2 3 1 fb             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2 -2 4 1                                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **  - -gmd 0.01 sets the  (nominal) noise in the **         \n" : "" );
    output << ( (          advanced ) ? "                  **    target; and while the target is noiseless, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    it is best to select non-zero to avoid bad **         \n" : "" );
    output << ( (          advanced ) ? "                  **    conditioning on the kernel matrix.         **         \n" : "" );
    output << ( (          advanced ) ? "                  **  - -gbH 3 selects GP-UCB optimisation.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **  - -tM ... specifies the target function.     **         \n" : "" );
    output << ( (          advanced ) ? "                  **  - fb 1 ...  fb 2 ... specifies the variables **         \n" : "" );
    output << ( (          advanced ) ? "                  **    (y and z) and their ranges.                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. As  for example  1,  but  this time  using **         \n" : "" );
    output << ( (          advanced ) ? "                  **    random   projections  (REMBO   style)  for **         \n" : "" );
    output << ( (          advanced ) ? "                  **    demonstrative purposes:                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gmd 0.01 -gp urand([ -2 -2 ],[ 2 2 ]) -gbH 3 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gb 2 \"-tM derefv(y,0)^2+(derefv(y,1)-1)^2\"   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** \"-echo y\" fb 1 -2 3 1 fb 2 -2 4 1             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    In  this variant  the  optimiser  searches **         \n" : "" );
    output << ( (          advanced ) ? "                  **    over y,z, but the actual target becomes:   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **          f( y.r0 + z.r1 )                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where r0 and r1 are  both vectors from the **         \n" : "" );
    output << ( (          advanced ) ? "                  **    (uniform) distribution U([-2,-2],[2,2]) as **         \n" : "" );
    output << ( (          advanced ) ? "                  **    specified by -gp ...  The number of random **         \n" : "" );
    output << ( (          advanced ) ? "                  **    vectors ri  in this sum  and the  range of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    their weights  are controlled by  the args **         \n" : "" );
    output << ( (          advanced ) ? "                  **    in -gb ...                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Note  that the  function minimised  is now **         \n" : "" );
    output << ( (          advanced ) ? "                  **    written in terms of  the components of y - **         \n" : "" );
    output << ( (          advanced ) ? "                  **    ie derefv(y,0) and  derefv(y,1) - which is **         \n" : "" );
    output << ( (          advanced ) ? "                  **    a  vector  (y   here  because   the  first **         \n" : "" );
    output << ( (          advanced ) ? "                  **    in the -gb ... expression is y = var(0,1). **         \n" : "" );
    output << ( (          advanced ) ? "                  **    In general it will be var(0,i), where i is **         \n" : "" );
    output << ( (          advanced ) ? "                  **    provided by the first var defined in -gb   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 3. As for  example 1,  but using  Kirschner's **         \n" : "" );
    output << ( (          advanced ) ? "                  **    method  of  repeated  1-d  subspaces  (see **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Adaptive and Safe Bayesian Optimization in **         \n" : "" );
    output << ( (          advanced ) ? "                  **    High   Dimensions    via   One-Dimensional **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Subspaces):                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gpr 3 -gmd 0.01 -gp urand([ -2 -2 ],[ 2 2 ]) **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 3 -gb 1                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  ** \"-tM derefv(y,0)^2+(derefv(y,1)-1)^2\"         **         \n" : "" );
    output << ( (          advanced ) ? "                  ** \"-echo y\" fb 1 -2 3 1                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    The main distinction between this and eg 2 **         \n" : "" );
    output << ( (          advanced ) ? "                  **    is that -gpr 3 tells  the optimiser to run **         \n" : "" );
    output << ( (          advanced ) ? "                  **    an additional 3 times  in sequence (K-1 in **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Kirschner)  with  a new  random  direction **         \n" : "" );
    output << ( (          advanced ) ? "                  **    from the previous  best.  We use -g 1 here **         \n" : "" );
    output << ( (          advanced ) ? "                  **    to specify  line-search,  but you  can use **         \n" : "" );
    output << ( (          advanced ) ? "                  **    more to specify narg-dim subspace search.  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 4. Functional  optimisation using the  method **         \n" : "" );
    output << ( (          advanced ) ? "                  **    of   random  subspaces    (target  is   to **         \n" : "" );
    output << ( (          advanced ) ? "                  **    replicate the sin(x) on range [0,1]):      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gf 1 -gpr 5 -gmd 0.01 -gP 0.5 -gbH 3 -gb 1   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** \"-tM norm2(sin(2*pi()*x)-y)\" \"-echo y\"        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** fb 1 -10 10 1 -Zx -qw var(90,2) -Zx Zinteract **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    In this case,  rather than -gp  to specify **         \n" : "" );
    output << ( (          advanced ) ? "                  **    a distribution for a  random-vector basis, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    we use  -gP 0.5 to  specify  a GP  with SE **         \n" : "" );
    output << ( (          advanced ) ? "                  **    kernel  with  lengthscale  0.5 from  which **         \n" : "" );
    output << ( (          advanced ) ? "                  **    random  functions  are to  be drawn.  Note **         \n" : "" );
    output << ( (          advanced ) ? "                  **    also that  -gf 1 specifies that  functions **         \n" : "" );
    output << ( (          advanced ) ? "                  **    are to be treated as scalar functions, not **         \n" : "" );
    output << ( (          advanced ) ? "                  **    zero-variance  distributions, allowing  us **         \n" : "" );
    output << ( (          advanced ) ? "                  **    evaluate  norm2(sin(2*pi()*x)-y) to  real. **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Finally,  -Zinteract  allows you  to  test **         \n" : "" );
    output << ( (          advanced ) ? "                  **    the resulting function.                    **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Generic parameter search options.             **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gy  t          - max training  time for  search alg (in  seconds - 0\n" : "" );
    output << ( (          advanced ) ? "                           for no limit, default).                            \n" : "" );
    output << ( (          advanced ) ? "         -gxs [...]      - initial value of x vector (default []).            \n" : "" );
    output << ( (          advanced ) ? "         -gfm l          - min value for function, stop if f<=l (deflt -inf). \n" : "" );
    output << ( (          advanced ) ? "         -gfu m          - max value of function, stop if f>=l (deflt +inf).  \n" : "" );
    output << ( (          advanced ) ? "         -gfM l          - soft minimum value of  function (don't stop, but is\n" : "" );
    output << ( (          advanced ) ? "                           used by some variants of for example Bayesian opt).\n" : "" );
    output << ( (          advanced ) ? "         -gfU l          - soft max value  for function.  If  exceeded, result\n" : "" );
    output << ( (          advanced ) ? "                           clipped (that is, res = max(f(x),l) (deflt +inf).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gr  n          - number of repeats (default 1).  If > 1 then results\n" : "" );
    output << ( (          advanced ) ? "                           for the  final repeat  are  returned,  except fgrid\n" : "" );
    output << ( (          advanced ) ? "                           values are replaced by [ fmean, fvar ], and fres is\n" : "" );
    output << ( (          advanced ) ? "                           meaningless.                                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gnp            - set no projection (default).                       \n" : "" );
    output << ( (          advanced ) ? "         -gp  $fn        - set  projection.  If  set,  what  you are  actually\n" : "" );
    output << ( (          advanced ) ? "                           optimising   is  f(p(x)),  where  p   is  a  random\n" : "" );
    output << ( (          advanced ) ? "                           projection of type:                                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              p(x) = [ x_0.p_0 + x_1.p_1 + ... ]              \n" : "" );
    output << ( (          advanced ) ? "                                     [ x_1                     ]              \n" : "" );
    output << ( (          advanced ) ? "                                     [ x_2                     ]              \n" : "" );
    output << ( (          advanced ) ? "                                     [ ...                     ]              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           where p_i  is taken from  the distribution $fn  (eg\n" : "" );
    output << ( (          advanced ) ? "                           you could used grand([0 0],M:[1 0 ; 0 1]) for norml\n" : "" );
    output << ( (          advanced ) ? "                           random  vectors, or urand([ 0 0 0 ],[ 1 1 1 ])  for\n" : "" );
    output << ( (          advanced ) ? "                           uniform. Note that in this scheme the first element\n" : "" );
    output << ( (          advanced ) ? "                           in -g may be anything valued  (but is vector valued\n" : "" );
    output << ( (          advanced ) ? "                           in the example).                                   \n" : "" );
    output << ( (          advanced ) ? "         -gP  g          - like -gp but in this case p_i are functions sampled\n" : "" );
    output << ( (          advanced ) ? "                           from a GP with RBF kernel of lengthscale g.        \n" : "" );
    output << ( (          advanced ) ? "         -gPk...         - set kernel parameters on GP for -gP.               \n" : "" );
    output << ( (          advanced ) ? "         -gpb            - like -gp, but p_i are Bernstein basis polynomials. \n" : "" );
    output << ( (          advanced ) ? "         -gpB n          - like -gpb, but with  schedule.  Degree of Bernstein\n" : "" );
    output << ( (          advanced ) ? "                           starts at n, then increases  with every repeat (set\n" : "" );
    output << ( (          advanced ) ? "                           by -gpr).  Note that in this case random repeats do\n" : "" );
    output << ( (          advanced ) ? "                           not give an alternate re-projection. The max degree\n" : "" );
    output << ( (          advanced ) ? "                           is just the number of variables.                   \n" : "" );
    output << ( (          advanced ) ? "         -gph            - set RKHS projection.  Like -gp, except in this case\n" : "" );
    output << ( (          advanced ) ? "                           the function is in a random RKHS, that is:         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              p(x) = [ sum_i x_i K(xx_i,x) ]                  \n" : "" );
    output << ( (          advanced ) ? "                                     [ x_1                 ]                  \n" : "" );
    output << ( (          advanced ) ? "                                     [ x_2                 ]                  \n" : "" );
    output << ( (          advanced ) ? "                                     [ ...                 ]                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           where xx_i  are selected uniform  randomly [0,1] to\n" : "" );
    output << ( (          advanced ) ? "                           required dimension.                                \n" : "" );
    output << ( (          advanced ) ? "         -gpbk...        - set kernel parameters on RKHS for -gph.            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gpd d          - for functional optimisation  (-gP, -gpb) by default\n" : "" );
    output << ( (          advanced ) ? "                           the function is of one variable.  This lets you set\n" : "" );
    output << ( (          advanced ) ? "                           d variables instead  (default 1).  Variables are x,\n" : "" );
    output << ( (          advanced ) ? "                           y,... (var(0,0), var(0,1), ...) and range [0,1].   \n" : "" );
    output << ( (          advanced ) ? "         -gpr n          - number of sequential random projections. If NZ then\n" : "" );
    output << ( (          advanced ) ? "                           n random subspaces will  be found in sequence, with\n" : "" );
    output << ( (          advanced ) ? "                           each having as \"point zero\" the previous best.  For\n" : "" );
    output << ( (          advanced ) ? "                           RKHS this acts  like n restarts (but  make sure you\n" : "" );
    output << ( (          advanced ) ? "                           clear the model between restarts (-gmt 1 or 2).    \n" : "" );
    output << ( (          advanced ) ? "         -gf  n          - selects how functions are treated by the kernel:   \n" : "" );
    output << ( (          advanced ) ? "                           0 - fns are treated as zero-mean distributions.    \n" : "" );
    output << ( (          advanced ) ? "                           1 - fns are treated as scalar functions @():f(x).  \n" : "" );
    output << ( (          advanced ) ? "         -gc  n          - include bias step:                                 \n" : "" );
    output << ( (          advanced ) ? "                           0 - fns as described previously.                   \n" : "" );
    output << ( (          advanced ) ? "                           1 - for first iteration, final p_... replaced by C.\n" : "" );
    output << ( (          advanced ) ? "         -gC  C          - constant used in -gc 1.                            \n" : "" );
    output << ( (          advanced ) ? "         -gns n          - number of pts in approx integration for scalar fns.\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gtp  $pstring  - string  to be  evaluated  at start  of  optim.  Put\n" : "" );
    output << ( (          advanced ) ? "                           fancy kernel setups etc here.  Evaluated once, just\n" : "" );
    output << ( (          advanced ) ? "                           before random projection (c/f -gpr).               \n" : "" );
    output << ( (          advanced ) ? "         -gtP  $mstring  - to be evaluated after  each random projection.  Put\n" : "" );
    output << ( (          advanced ) ? "                           kernel tweaking steps here.   Evaluated after inner\n" : "" );
    output << ( (          advanced ) ? "                           optimisation and random projections (c/f -gpr).    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gtx $xfn       - if set, the x  stored and logged is  not the x that\n" : "" );
    output << ( (          advanced ) ? "                           was  evaluated but  rather xfn(x).   null (default)\n" : "" );
    output << ( (          advanced ) ? "                           disables this  feature.  This is  evaluated *after*\n" : "" );
    output << ( (          advanced ) ? "                           $evalstring, which is handy for nested bayesian.   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -g+ [ $fn1 .. ] - Add penalty sum_i max(0,$fni(x))  to the objective.\n" : "" );
    output << ( (          advanced ) ? "                           For example -g+ [ norminf(x)-B ] will add a penalty\n" : "" );
    output << ( (          advanced ) ? "                           term if ||p(x)||_inf > B.  This  can be helpful for\n" : "" );
    output << ( (          advanced ) ? "                           constraining projected searches.                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Grid-search specific options.                 **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ggm n          - number of zooms.  After  the optimum for a grid has\n" : "" );
    output << ( (          advanced ) ? "                           been  found a  zoom  involves  doing an  additional\n" : "" );
    output << ( (          advanced ) ? "                           grid-search with a  finer grid over a smaller range\n" : "" );
    output << ( (          advanced ) ? "                           around  the  previous  solution.  This  is  done  n\n" : "" );
    output << ( (          advanced ) ? "                           times (default 0).                                 \n" : "" );
    output << ( (          advanced ) ? "         -ggi f          - width  of  zoomed grid  is width  of previous  grid\n" : "" );
    output << ( (          advanced ) ? "                           multiplied  by f (real,  < 1).  Grid is  trimmed to\n" : "" );
    output << ( (          advanced ) ? "                           lie inside previous range.  Default 0.3333.        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** DIRect global optimiser options.              **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gdc m          - max number of cube divisions (default 200).        \n" : "" );
    output << ( (          advanced ) ? "         -gdf m          - max number of function evaluations (default 1000). \n" : "" );
    output << ( (          advanced ) ? "         -gde e          - epsilon factor (default 1e-4).                     \n" : "" );
    output << ( (          advanced ) ? "         -gda t          - algorithm.  0 is original (default), 1 Gablowsky.  \n" : "" );
    output << ( (          advanced ) ? "         -gdy t          - max training time over-ride for DIRect.  Applies to\n" : "" );
    output << ( (          advanced ) ? "                           the direct algorithm only, so when DIRect is called\n" : "" );
    output << ( (          advanced ) ? "                           by another  algorithm (eg by a  Bayesian optimiser)\n" : "" );
    output << ( (          advanced ) ? "                           then  this controls  the time  spent in  the DIRect\n" : "" );
    output << ( (          advanced ) ? "                           calls  and -gy  controls  the  total overall  time.\n" : "" );
    output << ( (          advanced ) ? "                           Value is in seconds, default is zero (no override).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Nelder-Mead optimiser options.                **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gNa e          - minimum f value (default -HUGE_VAL).               \n" : "" );
    output << ( (          advanced ) ? "         -gNb e          - relative f value tolerance (default 0).            \n" : "" );
    output << ( (          advanced ) ? "         -gNc e          - absolute f value tolerance (default 0).            \n" : "" );
    output << ( (          advanced ) ? "         -gNd e          - relative x value tolerance (default 0).            \n" : "" );
    output << ( (          advanced ) ? "         -gNg e          - relative x value tolerance (default 0).            \n" : "" );
    output << ( (          advanced ) ? "         -gNe m          - max number of function evaluations (default 1000). \n" : "" );
    output << ( (          advanced ) ? "         -gNf t          - algorithm:   0  is  subplex  (default), 1  original\n" : "" );
    output << ( (          advanced ) ? "                           Nelder-Mead algorithm.                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Stopping criteria are as follows:             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - f goes below minimum f value.               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - a step happens were  f changes by less than **         \n" : "" );
    output << ( (          advanced ) ? "                  **   the relative f tolerance times |f|.         **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - a step happens there the absolute change in **         \n" : "" );
    output << ( (          advanced ) ? "                  **   |f| is less than the absolute f tolderance. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - a step happens where x changes by less than **         \n" : "" );
    output << ( (          advanced ) ? "                  **   the relative x tolerance times ||x||.       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - a step happens there the absolute change in **         \n" : "" );
    output << ( (          advanced ) ? "                  **   |x[i]| for any i is  less than the absolute **         \n" : "" );
    output << ( (          advanced ) ? "                  **    x tolderance.                              **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - the max number  of function  evaluations is **         \n" : "" );
    output << ( (          advanced ) ? "                  **   exceeded.                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - the max training time is exceeded.          **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Model-based optimisation options.             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (this includes Bayesian optimisation)         **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gms            - Select single-objective optimisation model (dflt). \n" : "" );
    output << ( (          advanced ) ? "         -gmo            - Select multi-objective optimisation model.         \n" : "" );
    output << ( (          advanced ) ? "         -gma n          - set dim of default GPR for multi-objective optim.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmT [ xt ]     - set (sparse vector) template  for model data.  Data\n" : "" );
    output << ( (          advanced ) ? "                           added to  the model is  x overlaid onto  xt, so for\n" : "" );
    output << ( (          advanced ) ? "                           example  -gmT [ ~ 1 ] means  that if a  vector x is\n" : "" );
    output << ( (          advanced ) ? "                           added into the model as [ x ~ 1 ]. This can be used\n" : "" );
    output << ( (          advanced ) ? "                           in multi-task learning, with -kS to signify a split\n" : "" );
    output << ( (          advanced ) ? "                           kernel, -ks 2 to indicate two  parts to the kernel,\n" : "" );
    output << ( (          advanced ) ? "                           -ki 0 ... to set usual covariance (over x), and -ki\n" : "" );
    output << ( (          advanced ) ? "                           1 -kg 48 -kr v to  set the variance  between tasks.\n" : "" );
    output << ( (          advanced ) ? "                           If -gmw has been used to spacify an ML with data of\n" : "" );
    output << ( (          advanced ) ? "                           the form [ xi ~ 0 ] then:                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           K([xi ~ ti],[xj ~ tj]) = K0(xi,xj)   if ti == tj   \n" : "" );
    output << ( (          advanced ) ? "                                                  = v.K0(xi,xj) if ti != tj   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           will do transfer learning via multi-task kernel.   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmd s          - set noise variance for default GPR model.          \n" : "" );
    output << ( (          advanced ) ? "         -gmg g          - set length scale for default GPR kernel.           \n" : "" );
    output << ( (          advanced ) ? "         -gmgg g         - set ARD length scale vector for default GPR kernel.\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmn n          - sigma estimation model:                            \n" : "" );
    output << ( (          advanced ) ? "                           0 - mu and  sigma approximated by a single  ML that\n" : "" );
    output << ( (          advanced ) ? "                               gets updated after each \"batch\".               \n" : "" );
    output << ( (          advanced ) ? "                           1 - mu and sigma approximated by separate MLs.  The\n" : "" );
    output << ( (          advanced ) ? "                               mu model  is updated  after  each  \"batch\", the\n" : "" );
    output << ( (          advanced ) ? "                               sigma model after each  experiment (during each\n" : "" );
    output << ( (          advanced ) ? "                               batch).                                        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmma n         - Automatic tuning for optimisation model.           \n" : "" );
    output << ( (          advanced ) ? "         -gmmb n         - Automatic tuning for noise variance model.         \n" : "" );
    output << ( (          advanced ) ? "         -gmmc n         - Automatic tuning for source model (see -gmx).      \n" : "" );
    output << ( (          advanced ) ? "         -gmmd n         - Automatic tuning for difference model (see -gmx).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           The above tuning is of  (some of the) kernel params\n" : "" );
    output << ( (          advanced ) ? "                           on a heuristic basis using rudimentary grid search.\n" : "" );
    output << ( (          advanced ) ? "                           Note that the source model is only tuned once.  The\n" : "" );
    output << ( (          advanced ) ? "                           n value controls what is to be minimised:          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           0: no automatic tuning                             \n" : "" );
    output << ( (          advanced ) ? "                           1: negative-log-likelihood minimisation.           \n" : "" );
    output << ( (          advanced ) ? "                           2: leave-one-out error minimisation (dflt for all).\n" : "" );
    output << ( (          advanced ) ? "                           3: recall minimisation.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmw n          - by default the objective is modelled by a GPR.  You\n" : "" );
    output << ( (          advanced ) ? "                           can replace this with ML n using this command. Note\n" : "" );
    output << ( (          advanced ) ? "                           that for multi-objective  optimisation this must be\n" : "" );
    output << ( (          advanced ) ? "                           vector-valued.                                     \n" : "" );
    output << ( (          advanced ) ? "                        ** You can also use this for  transfer learning.  Data\n" : "" );
    output << ( (          advanced ) ? "                           already in this model is treated as observations of\n" : "" );
    output << ( (          advanced ) ? "                           y = -f(x) (NOTE THE NEGATIVE SIGN THERE).  See also\n" : "" );
    output << ( (          advanced ) ? "                           -gmx for more on transfer learning.                \n" : "" );
    output << ( (          advanced ) ? "         -gmW n          - like -gmw, but for the sigma model (if separate).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmt n          - model basis:                                       \n" : "" );
    output << ( (          advanced ) ? "                           0 - model f(p(x)) using p(x) (default).            \n" : "" );
    output << ( (          advanced ) ? "                           1 - model f(p(x)) using p(x), clear after subspace.\n" : "" );
    output << ( (          advanced ) ? "                           2 - model f(p(x)) using x, clear after subspace.   \n" : "" );
    output << ( (          advanced ) ? "                           3 - model f(p(x)) using x.                         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmq n          - direction oracle mode (for -gp, -gP, -gpr):        \n" : "" );
    output << ( (          advanced ) ? "                           0 - direction is sample from gradient GP at current\n" : "" );
    output << ( (          advanced ) ? "                               best solution, as per GP model (default).      \n" : "" );
    output << ( (          advanced ) ? "                           1 - direction  is gradient  of model GP  at current\n" : "" );
    output << ( (          advanced ) ? "                               best solution, as per GP model.                \n" : "" );
    output << ( (          advanced ) ? "                           2 - direction is random sample.                    \n" : "" );
    output << ( (          advanced ) ? "                           3 - mode 0 for primary axis, mode 2 for the rest.  \n" : "" );
    output << ( (          advanced ) ? "                           4 - mode 1 for primary axis, mode 2 for the rest.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmx n          - controls how data already in model is treated (that\n" : "" );
    output << ( (          advanced ) ? "                           is, if you use  -gmw n where ML n  has data already\n" : "" );
    output << ( (          advanced ) ? "                           added).  Options are:                              \n" : "" );
    output << ( (          advanced ) ? "                           0 - assume data from target model (default).       \n" : "" );
    output << ( (          advanced ) ? "                           1 - use env-GP as per Joy1/Shi21.                  \n" : "" );
    output << ( (          advanced ) ? "                           2 - use diff-GP as per Shi21.                      \n" : "" );
    output << ( (          advanced ) ? "         -gmxa a         - alpha0 value for env-GP.                           \n" : "" );
    output << ( (          advanced ) ? "         -gmxb b         - beta0 value for env-GP.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmy n          - Kernel transfer learning from ML n.                \n" : "" );
    output << ( (          advanced ) ? "         -gmya n         - Method for kernel  transfer, if -gmy used.  Options\n" : "" );
    output << ( (          advanced ) ? "                           as per -kt 8xx, so:                                \n" : "" );
    output << ( (          advanced ) ? "                           800 - trivial (K(x,y) = Kn(x,y)).                  \n" : "" );
    output << ( (          advanced ) ? "                           801 - m-norm (free kernel) transfer (default).     \n" : "" );
    output << ( (          advanced ) ? "                           802 - moment (Der and Lee) transfer.               \n" : "" );
    output << ( (          advanced ) ? "                           804 - K-learn transfer.                            \n" : "" );
    output << ( (          advanced ) ? "                           805 - K2-learn transfer.                           \n" : "" );
    output << ( (          advanced ) ? "                           806 - Multi-layer transfer.                        \n" : "" );
    output << ( (          advanced ) ? "         -gmyb n         - Kernel transfer normalisation:                     \n" : "" );
    output << ( (          advanced ) ? "                           0 - no normalisation.                              \n" : "" );
    output << ( (          advanced ) ? "                           1 - normalisation on (default).                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** env-GP:  - source model copied from the ML of **         \n" : "" );
    output << ( (          advanced ) ? "                  **            -gmw n, though  you can do further **         \n" : "" );
    output << ( (          advanced ) ? "                  **            tuning via var(90,5). Model asumed **         \n" : "" );
    output << ( (          advanced ) ? "                  **            to be already trained.             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** diff-GP: - source model as-per env-GP.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **          - difference model  also copied from **         \n" : "" );
    output << ( (          advanced ) ? "                  **            -gmw n, but  data added  and model **         \n" : "" );
    output << ( (          advanced ) ? "                  **            retrained throughout (var(90,6)).  **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Bayesian optimiser options.                   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbH n          - method used to select points in optimisation:      \n" : "" );
    output << ( (          advanced ) ? "                           0  - mean only minimisation.                       \n" : "" );
    output << ( (          advanced ) ? "                           1  - EI (expected improvement - default).          \n" : "" );
    output << ( (          advanced ) ? "                           2  - PI (probability of improvement).              \n" : "" );
    output << ( (          advanced ) ? "                           3  - GP-UCB as per Brochu (recommended GP-UCB).*   \n" : "" );
    output << ( (          advanced ) ? "                           4  - GP-UCB |D| finite as per Srinivas.            \n" : "" );
    output << ( (          advanced ) ? "                           5  - GP-UCB |D| infinite as per Srinivas.          \n" : "" );
    output << ( (          advanced ) ? "                           6  - GP-UCB p based on Brochu.                     \n" : "" );
    output << ( (          advanced ) ? "                           7  - GP-UCB p |D| finite based on Srinivas.        \n" : "" );
    output << ( (          advanced ) ? "                           8  - GP-UCB p |D| infinite based on Srinivas.      \n" : "" );
    output << ( (          advanced ) ? "                           9  - PE (variance-only maximisation).              \n" : "" );
    output << ( (          advanced ) ? "                           10 - mean-only minimisation.                       \n" : "" );
    output << ( (          advanced ) ? "                           11 - GP-UCB with user-defined beta_t (see -gbv).   \n" : "" );
    output << ( (          advanced ) ? "                           * beta_n = nu.2.log((n^{2+dim/2}).(pi^2)/(3.delta))\n" : "" );
    output << ( (          advanced ) ? "         -gbj n          - number  of random  start/seed  points  (default  -1\n" : "" );
    output << ( (          advanced ) ? "                           translates to d+1, where d is the dimension).      \n" : "" );
    output << ( (          advanced ) ? "         -gbt m          - max  iteration  count for  search algorithm  (0 for\n" : "" );
    output << ( (          advanced ) ? "                           no limit, default -1 which translates to 10d, -2 to\n" : "" );
    output << ( (          advanced ) ? "                           stop when min_x err(x) <= maxerr).                 \n" : "" );
    output << ( (          advanced ) ? "         -gbe e          - maxerr as required with -gbt -2 (see Kirschner et.,\n" : "" );
    output << ( (          advanced ) ? "                           default 0.1).                                      \n" : "" );
    output << ( (          advanced ) ? "         -gbz z          - zero  tolerance for  search algorithm  (def 1e-12).\n" : "" );
    output << ( (          advanced ) ? "                           Used when assessing if sigma^2 == 0.               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gba n          - if >=  0 then  this is  used to  seed the  RNG when\n" : "" );
    output << ( (          advanced ) ? "                           generating initial  points (see -gbj,  default 42).\n" : "" );
    output << ( (          advanced ) ? "                           -2 means seed with time, -1 means no seed.         \n" : "" );
    output << ( (          advanced ) ? "         -gbb n          - RNG seed right before main optimisation loop if >=0\n" : "" );
    output << ( (          advanced ) ? "                           Default 69.  -2 means seed with time, -1 no seed.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbG n          - do  grid-search,  where ML n  defines grid  data in\n" : "" );
    output << ( (          advanced ) ? "                           terms of x (dimensions  must agree  with definition\n" : "" );
    output << ( (          advanced ) ? "                           in -gb) and y (must be real).                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbD d          - delta factor used in GP-UCB method (default 0.1).  \n" : "" );
    output << ( (          advanced ) ? "         -gbk n          - nu factor used in GP-UCB (deft 0.2, see Srivinas). \n" : "" );
    output << ( (          advanced ) ? "         -gbx n          - |D| (size  of search  space grid) for  gpUCB finite\n" : "" );
    output << ( (          advanced ) ? "                           (default -1, which  means set to size  of grid data\n" : "" );
    output << ( (          advanced ) ? "                           (if available) or 10 (which is arbitrary)).        \n" : "" );
    output << ( (          advanced ) ? "         -gbo a          - a constant for Srinivas |D|-infinite gpUCB (def 1).\n" : "" );
    output << ( (          advanced ) ? "         -gbB b          - b constant for Srinivas |D|-infinite gpUCB (def 1).\n" : "" );
    output << ( (          advanced ) ? "         -gbr r          - r constant for Srinivas  |D|-infinite gpUCB (def 1,\n" : "" );
    output << ( (          advanced ) ? "                           which is correct in all cases due to scaling).     \n" : "" );
    output << ( (          advanced ) ? "         -gbu p          - p value for GP-UCB p variants (default 2).         \n" : "" );
    output << ( (          advanced ) ? "         -gbv $fn        - user function for beta in  GP-UCB user-defined.  In\n" : "" );
    output << ( (          advanced ) ? "                           this function:                                     \n" : "" );
    output << ( (          advanced ) ? "                           - var(0,1) (y) = iteration number.                 \n" : "" );
    output << ( (          advanced ) ? "                           - var(0,2) (z) = x dimension.                      \n" : "" );
    output << ( (          advanced ) ? "                           - var(0,3) (v) = delta.                            \n" : "" );
    output << ( (          advanced ) ? "                           - var(0,4) (w) = |D| specified by -gbx.            \n" : "" );
    output << ( (          advanced ) ? "                           - var(0,5) (g) = a as specified by -gbo.           \n" : "" );
    output << ( (          advanced ) ? "                           For   multi-recommendation    via   multi-objective\n" : "" );
    output << ( (          advanced ) ? "                           optimisation on (mu,sigma) use -gbv null. This will\n" : "" );
    output << ( (          advanced ) ? "                           apply  multi-objective  optimisation to  (mu,sigma)\n" : "" );
    output << ( (          advanced ) ? "                           and the  resultant Pareto  set will all be  used as\n" : "" );
    output << ( (          advanced ) ? "                           recommendations  (null  means  beta  undefined,  so\n" : "" );
    output << ( (          advanced ) ? "                           select for all values of beta).                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbp [ n0 n1 ... ]- Add penalty  terms to  the acquisition  function.\n" : "" );
    output << ( (          advanced ) ? "                           This is helpful if there are non-linear constraints\n" : "" );
    output << ( (          advanced ) ? "                           on the feasible region.  Each element of the vector\n" : "" );
    output << ( (          advanced ) ? "                           should be an  ML.  The total penalty  is the sum of\n" : "" );
    output << ( (          advanced ) ? "                           the  outputs of  all MLs.  Penalty  should  be near\n" : "" );
    output << ( (          advanced ) ? "                           zero  in the  feasible region,  very large  outside\n" : "" );
    output << ( (          advanced ) ? "                           (that  is, a  penalty for  a minimisation problem).\n" : "" );
    output << ( (          advanced ) ? "                           Default value is an empty vector.                  \n" : "" );
    output << ( (          advanced ) ? "         -gbl w          - distance   weight   (default   0).   Assuming   the\n" : "" );
    output << ( (          advanced ) ? "                           acquisition function is  strictly positive replaces\n" : "" );
    output << ( (          advanced ) ? "                           it with  (1-w.||p-q||_2)*acqusition(q),  where p is\n" : "" );
    output << ( (          advanced ) ? "                           the previous  parameter set  (specified by  -g) and\n" : "" );
    output << ( (          advanced ) ? "                           q  is the  proposed parameter  set.  Handy  to make\n" : "" );
    output << ( (          advanced ) ? "                           incremental retrains faster.                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbts $estring  - string  to be evaluated  before each (inner)  iter.\n" : "" );
    output << ( (          advanced ) ? "                           var(90,4) is the inner iteration count.            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbq n          - use IMP to pre-process  the output of the GP.  This\n" : "" );
    output << ( (          advanced ) ? "                           is required for multi-objective optimisation as the\n" : "" );
    output << ( (          advanced ) ? "                           improvement function requires a scalar. Essentially\n" : "" );
    output << ( (          advanced ) ? "                           mean :=  imp(mean,var).  Processing  done using IMP\n" : "" );
    output << ( (          advanced ) ? "                           with ML number  n (c/f -qw n) which  must be an IMP\n" : "" );
    output << ( (          advanced ) ? "                           type  object.  Note  that the  acquisition function\n" : "" );
    output << ( (          advanced ) ? "                           defined by -gbH will still be applied after this(to\n" : "" );
    output << ( (          advanced ) ? "                           do passthrough use -gbH 0).  Some IMPs  will update\n" : "" );
    output << ( (          advanced ) ? "                           the variance if  required.  For EHI use -gbH 0, for\n" : "" );
    output << ( (          advanced ) ? "                           SVM mono-surrogate use for example -gbH 3.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbmm n         - if using  Bayesian multi-step then  for all but the\n" : "" );
    output << ( (          advanced ) ? "                           first  recommendation in  each batch  pre-process x\n" : "" );
    output << ( (          advanced ) ? "                           using xi ->  f([ xi x0 ]), where f is  the function\n" : "" );
    output << ( (          advanced ) ? "                           of the  ML n  defined  here  and x0  is  the  first\n" : "" );
    output << ( (          advanced ) ? "                           recommendation in  this batch and the  dimension of\n" : "" );
    output << ( (          advanced ) ? "                           of xi is determined by -gbpd d (see below).        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbim {0,1,2,3} - iteration count method.  In GP-UCB beta calculation\n" : "" );
    output << ( (          advanced ) ? "                           an iteration  counter t  is used.   The value  of t\n" : "" );
    output << ( (          advanced ) ? "                           (var(0,1)  in -gbv equation/vector)  is  controlled\n" : "" );
    output << ( (          advanced ) ? "                           by this setting:                                   \n" : "" );
    output << ( (          advanced ) ? "                           0 - t  =  (N/B)+1  initially, t ->  t+1 after  each\n" : "" );
    output << ( (          advanced ) ? "                               batch, where  B is the batch size (size of -gbv\n" : "" );
    output << ( (          advanced ) ? "                               vector, or  1 by default) and N  is the initial\n" : "" );
    output << ( (          advanced ) ? "                               number of  training vectors in  the GP model (0\n" : "" );
    output << ( (          advanced ) ? "                               by default  unless you have  pre-training pts).\n" : "" );
    output << ( (          advanced ) ? "                               Default, consistent with the GP-UCB-PE method. \n" : "" );
    output << ( (          advanced ) ? "                           1 - t = N+1  initially,  t -> t+B after  each batch\n" : "" );
    output << ( (          advanced ) ? "                               (t is incremented by 1  for each recommendation\n" : "" );
    output << ( (          advanced ) ? "                               within a  batch).  This is  consistent with the\n" : "" );
    output << ( (          advanced ) ? "                               GP-BUCB method.                                \n" : "" );
    output << ( (          advanced ) ? "                           2 - like 1, but t  actually used to  calculate beta\n" : "" );
    output << ( (          advanced ) ? "                               is the t value at the start of this batch (that\n" : "" );
    output << ( (          advanced ) ? "                               is,  B*floor(t/B)).  Use  for GP-UCB  finite or\n" : "" );
    output << ( (          advanced ) ? "                               GP-UCB martingale.                             \n" : "" );
    output << ( (          advanced ) ? "                           3 - like 1, but t starts as 1.                     \n" : "" );
    output << ( (          advanced ) ? "         -gbs ibs        - intrinsic number  of recommendations  (see method 2\n" : "" );
    output << ( (          advanced ) ? "                           for multi-recommendation below).                   \n" : "" );
    output << ( (          advanced ) ? "         -gbm ims        - method for intrinsic batch:                        \n" : "" );
    output << ( (          advanced ) ? "                           0: use max mean, det(covar)^(1/(2*ibs)) (default). \n" : "" );
    output << ( (          advanced ) ? "                           1: use ave mean, det(covar)^(1/(2*ibs)).           \n" : "" );
    output << ( (          advanced ) ? "                           2: use min mean, det(covar)^(1/(2*ibs)).           \n" : "" );
    output << ( (          advanced ) ? "                           3: use max mean, sqrt(ibs/Tr(inv(covar))).         \n" : "" );
    output << ( (          advanced ) ? "                           4: use ave mean, sqrt(ibs/Tr(inv(covar))).         \n" : "" );
    output << ( (          advanced ) ? "                           5: use min mean, sqrt(ibs/Tr(inv(covar))).         \n" : "" );
    output << ( (          advanced ) ? "         -gbpp q         - pre-process DIRect output using ML q (see below).  \n" : "" );
    output << ( (          advanced ) ? "         -gbpd d         - dimension of pre-process input (see below).        \n" : "" );
    output << ( (          advanced ) ? "         -gbpl lv        - min vector for pre-process input (see below).      \n" : "" );
    output << ( (          advanced ) ? "         -gbpu uv        - max vector for pre-process input (see below).      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbsp p         - set >= 1 to enforce  mu_{1:p}-stability, where this\n" : "" );
    output << ( (          advanced ) ? "                           is the maximum allowed value of p (default 0).     \n" : "" );
    output << ( (          advanced ) ? "         -gbsP p         - minimum value of p (default 1).                    \n" : "" );
    output << ( (          advanced ) ? "         -gbsA A         - upper bound on output variation.                   \n" : "" );
    output << ( (          advanced ) ? "         -gbsB B         - maximum input perturbation.                        \n" : "" );
    output << ( (          advanced ) ? "         -gbsF F         - total output range (max-min).                      \n" : "" );
    output << ( (          advanced ) ? "         -gbsr b         - policy balance (0 = conservat (deflt), 1 = risky). \n" : "" );
    output << ( (          advanced ) ? "         -gbsz z         - zero reference point for f (default 0).            \n" : "" );
    output << ( (          advanced ) ? "         -gbss c         - use sigmoid compresion on stability scores (def 0).\n" : "" );
    output << ( (          advanced ) ? "         -gbst t         - threshold for sigmoid compression (def 0.8).       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Notes on Stable Methods:                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - sigmoid compression  can make EI  work, but **         \n" : "" );
    output << ( (          advanced ) ? "                  **   not very well.  Without compression however **         \n" : "" );
    output << ( (          advanced ) ? "                  **   the  algorithm tends  to stick  at unstable **         \n" : "" );
    output << ( (          advanced ) ? "                  **   peaks where high gain swamps low stability. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - strongly suggest using  GP-UCB, which works **         \n" : "" );
    output << ( (          advanced ) ? "                  **   very  well.  The  expected  return (mu)  is **         \n" : "" );
    output << ( (          advanced ) ? "                  **   scaled, but the variance isn't.             **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbuu {0,1}     - Unscented  optimisation on (1) or off  (0,default).\n" : "" );
    output << ( (          advanced ) ? "                           See Nogueira et al, Unscented Bayesian Optimisation\n" : "" );
    output << ( (          advanced ) ? "                           for Safe Robot Grasping.                           \n" : "" );
    output << ( (          advanced ) ? "         -gbuk k         - Set k value (0 or -3, 0 default).                  \n" : "" );
    output << ( (          advanced ) ? "         -gbuS S         - sqrt(Sigma), square-root of matrix variance of x.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Notes on Multi-Recommendation Methods:        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Method 1 (multi-objective optimisation):      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** To use multi-objective optimisation to define **         \n" : "" );
    output << ( (          advanced ) ? "                  ** multiple    recommendations,     where    the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** recommendations  correspond  to  the  Pareto- **         \n" : "" );
    output << ( (          advanced ) ? "                  ** optimal solutions of:                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **  max(-mu(x),sigma(x))                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (the negative  arising because  we are trying **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to minimise our target here) use the command: **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11 -gbv null                             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where beta =  null is shorthand  for beta not **         \n" : "" );
    output << ( (          advanced ) ? "                  ** defined, so  solve for  all beta.  -gbH 11 is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-UCB with  user-defined  beta_t (so -gbv is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** used instead).                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Method 2 (intrinsic batch):                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** In the standard methods  the two variables to **         \n" : "" );
    output << ( (          advanced ) ? "                  ** be optimised in the  inner loop are mu(x) and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** sigma(x).  Method 2 replaces these with:      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **               [   mu(x_0)   ]                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** mu(x) -> max( [   mu(x_1)   ] )               **         \n" : "" );
    output << ( (          advanced ) ? "                  **               [     ...     ]                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **               [ mu(x_{d-1}) ]                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                 [ covar_00 covar_01 ... ]     **         \n" : "" );
    output << ( (          advanced ) ? "                  ** sigma(x) -> det([ covar_10 covar_11 ... ])^r  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                 [    ...      ...   ... ]     **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (r = 1/2d)                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** To select this  use -gbs d.  Can  be combined **         \n" : "" );
    output << ( (          advanced ) ? "                  ** with other methods (assuming no constraints). **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Use -gbm 1 to slct mu(x) -> min(...) instead. **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** If you want to apply constraints to this use: **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - use -gbpp to  specify the  ML defining  the **         \n" : "" );
    output << ( (          advanced ) ? "                  **   them, so DIRect  optimises a(f(x)), where a **         \n" : "" );
    output << ( (          advanced ) ? "                  **   is the usual  activation function  and f is **         \n" : "" );
    output << ( (          advanced ) ? "                  **   specified by ml number q.                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - use -gbpd to specify  the dimension  of the **         \n" : "" );
    output << ( (          advanced ) ? "                  **   input to f.                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - use -gbpl/-gbpu to specify lower  and upper **         \n" : "" );
    output << ( (          advanced ) ? "                  **   bnd vector (respectively)  on the x vector. **         \n" : "" );
    output << ( (          advanced ) ? "                  **   In this case the variables specified in the **         \n" : "" );
    output << ( (          advanced ) ? "                  **   original -gb call are the outputs of f(x).  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Method 3 (hybrid multi-strategy):             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** To define  multiple GP-UCB  strategies select **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11 (GP-UCB with user defined beta_t) and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** then let -gbv be a vector, each corresponding **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to an equation for calculating beta.  eg:     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbv [ 2*log((x^(2+(y/2)))*zeta(2)/z) ;       **         \n" : "" );
    output << ( (          advanced ) ? "                  **        2*log((x^(3+(y/2)))*zeta(3)/z) ;       **         \n" : "" );
    output << ( (          advanced ) ? "                  **        2*log((x^(4+(y/2)))*zeta(4)/z) ]       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** gives  three equations  for beta  (p=2,3,4 in **         \n" : "" );
    output << ( (          advanced ) ? "                  ** this  case)  that   will  each  result  in  a **         \n" : "" );
    output << ( (          advanced ) ? "                  ** separate  recommendation.   Alternatively you **         \n" : "" );
    output << ( (          advanced ) ? "                  ** can define  beta_t indirectly  by making each **         \n" : "" );
    output << ( (          advanced ) ? "                  ** element of  the -gbv vector a  vector of  the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** form:                                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** [ methd {p betfn |D| nu delt a b r ibs ims} ] **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where method selects an option asper -gbH and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the  remaining   (optional)   arguments   the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** various parameters therein.  For example:     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbv [ [ 3 ] ;                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 3 ] ;                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 4 ] ]                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** is equivalent to the  first example.  You can **         \n" : "" );
    output << ( (          advanced ) ? "                  ** leave \"gaps\" using []  (don't overwrite).  So **         \n" : "" );
    output << ( (          advanced ) ? "                  ** for example to set nu = 0.9 for the first two **         \n" : "" );
    output << ( (          advanced ) ? "                  ** in the above you would use:                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbv [ [ 3 [ ] [ ] [ ] 0.9 ] ;                **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 3 [ ] [ ] 0.9 ] ;                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 4 ] ]                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** You can intersperse these techniques.  eg:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbv [ [ 3 [ ] [ ] [ ] 0.9 ] ;                **         \n" : "" );
    output << ( (          advanced ) ? "                  **        2*log((x^(5+(y/2)))*zeta(5)/z) ;       **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 3 [ ] [ ] 0.9 ] ;                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 4 ] ]                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** defines four recommendation methods; and also **         \n" : "" );
    output << ( (          advanced ) ? "                  ** that you can include method 1 as part of this **         \n" : "" );
    output << ( (          advanced ) ? "                  ** so for example:                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbv [ [ 3 [ ] [ ] [ ] 0.9 ] ;                **         \n" : "" );
    output << ( (          advanced ) ? "                  **        2*log((x^(5+(y/2)))*zeta(5)/z) ;       **         \n" : "" );
    output << ( (          advanced ) ? "                  **        null ;                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 3 [ ] [ ] 0.9 ] ;                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 4 ] ]                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** defines  five   recommendation  methods,  the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** third of which is itself multi-recommendation **         \n" : "" );
    output << ( (          advanced ) ? "                  ** method 1.                                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Finally,  you can  control how  the model  is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** updated  between each round (element  in -gbv **         \n" : "" );
    output << ( (          advanced ) ? "                  ** vector) using  the -gmn option.  If -gmn 0 is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** used (default) both mu and sigma use the same **         \n" : "" );
    output << ( (          advanced ) ? "                  ** model (GP) updated batchwise,  whereas -gmn 1 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** uses separate  models, the  mu model  updated **         \n" : "" );
    output << ( (          advanced ) ? "                  ** batchwise and  the sigma model  updated after **         \n" : "" );
    output << ( (          advanced ) ? "                  ** each recommendation (halucinated samples).    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Standard methods: GP-                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **UCB-PE: -gbH 11 -gmn 1 -gbv [ [ 3 ] ; [ 9 ] ...]**        \n" : "" );
    output << ( (          advanced ) ? "                  **BUCB: -gbH 11 -gmn 1 -gbim 1 -gbv [ [ 3 ] ; [ 3 ]**       \n" : "" );
    output << ( (          advanced ) ? "                  **                                         ... ] **         \n" : "" );
    output << ( (          advanced ) ? "                  ** UCB-multi: -gbH 11 -gbv null                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where  the number  of recommendations  is the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** total number of elements in the -gbv vector.  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Multi-recommendation with constraints:        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** We  take  as our  example  problem  the 2-dim **         \n" : "" );
    output << ( (          advanced ) ? "                  ** optimisation  problem given  previously  (see **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gb). The flags below should come just before **         \n" : "" );
    output << ( (          advanced ) ? "                  ** this statement.   The following  examples are **         \n" : "" );
    output << ( (          advanced ) ? "                  ** for a  batch size of  2 (recommendations  per **         \n" : "" );
    output << ( (          advanced ) ? "                  ** batch)  with the  restriction that  the first **         \n" : "" );
    output << ( (          advanced ) ? "                  ** element  of all  recommendations must  be the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** same).                                        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-UCB-det (based on method 2 above) ver 1:   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up map function:                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 2 -Zx -z fnb                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -blx M:[1,0,0;0,1,0;1,0,0;0,0,1]*x         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where -z fnb is a  block function and -blx **         \n" : "" );
    output << ( (          advanced ) ? "                  **    sets the map. -qw 2 sets it as ML block 2. **         \n" : "" );
    output << ( (          advanced ) ? "                  **    This implements the map:                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ [ x00 ] ]   [ 1 0 0 ] [ z0 ]             **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ [ x01 ] ]   [ 0 1 0 ] [ z1 ]             **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [         ] = [       ] [ z2 ]             **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ [ x10 ] ]   [ 1 0 0 ]                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ [ x11 ] ]   [ 0 0 1 ]                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    The inner DIRect optimiser in the Bayesian **         \n" : "" );
    output << ( (          advanced ) ? "                  **    optimisation  optimises over  z, while the **         \n" : "" );
    output << ( (          advanced ) ? "                  **    GP model is built over x.                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Set up the Bayesian optimiser:             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 0 -Zx                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbH 3 -gbs 2 -gbm 0 -gbpp 2 -gbpd 3       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpl [ -4 -4 -4 ] -gbpu [ 4 4 4 ]         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Bayesian optimiser using method 3 (GP-UCB) **         \n" : "" );
    output << ( (          advanced ) ? "                  **   (setby -gbH 3), 2 intrinsic recommendations **         \n" : "" );
    output << ( (          advanced ) ? "                  **    per  batch (set  by -gbs 2),  balanced max **         \n" : "" );
    output << ( (          advanced ) ? "                  **    mean / determinant  (set by -gbm 0), using **         \n" : "" );
    output << ( (          advanced ) ? "                  **    ML 2  as  a  map  to  enforce  constraint, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    pre-process  dimension (input  to ML 2) of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    3 (-gbpd 3), with lower  and upper  bounds **         \n" : "" );
    output << ( (          advanced ) ? "                  **    for direct set by -gbpl and -gbpu.         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-UCB-det (based on method 2 above) ver 2:   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up penalty function:                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 2 -Zx -z fnb                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -blx 1000*(([1;0;1;0]*x)^2)                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where -z fnb is a  block function and -blx **         \n" : "" );
    output << ( (          advanced ) ? "                  **    sets the map. -qw 2 sets it as ML block 2. **         \n" : "" );
    output << ( (          advanced ) ? "                  **    This is a quadratic penalty for failure to **         \n" : "" );
    output << ( (          advanced ) ? "                  **    satisfy the constraint.                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Set up the Bayesian optimiser:             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 0 -Zx                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbH 3 -gbs 2 -gbm 0 -gbp [ 2 ] -gbpd 4    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpl [ -4 -4 -4 -4 ] -gbpu [ 4 4 4 4 ]    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Bayesian optimiser using method 3 (GP-UCB) **         \n" : "" );
    output << ( (          advanced ) ? "                  **   (setby -gbH 3), 2 intrinsic recommendations **         \n" : "" );
    output << ( (          advanced ) ? "                  **    per  batch (set  by -gbs 2),  balanced max **         \n" : "" );
    output << ( (          advanced ) ? "                  **    mean / determinant  (set by -gbm 0), using **         \n" : "" );
    output << ( (          advanced ) ? "                  **    ML 2 as  a penalty to  enforce constraint, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    pre-process  dimension (input  to ML 2) of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    4 (-gbpd 4) - you need this -  with lower/ **         \n" : "" );
    output << ( (          advanced ) ? "                  **    upper bounds for direct set by -gbpl/-gbpu.**         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-UCB-cPE:                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up map function:                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 2 -Zx -z fnb                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -blx M:[0,1,0;1,0,0]*x                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where -z fnb is a  block function and -blx **         \n" : "" );
    output << ( (          advanced ) ? "                  **    sets the map. -qw 2 sets it as ML block 2. **         \n" : "" );
    output << ( (          advanced ) ? "                  **    This implements the map:                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ x10 ] = [ 0 1 0 ] [ z0  ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ x11 ]   [ 1 0 0 ] [ x01 ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                        [ x11 ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    The second recommendation  is generated by **         \n" : "" );
    output << ( (          advanced ) ? "                  **    the  DIRect  optimiser  optimising  on  z, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    while the GP model is built over x.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Set up the Bayesian optimiser:             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 0 -Zx                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbH 11 -gmn 1 -gbv [ [ 3 ] ; [ 9 ] ]      **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpp 2 -gbpd 1                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpl [ -4 ] -gbpu [ 4 ]                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Bayesian  optimiser using  above GP-UCB-PE **         \n" : "" );
    output << ( (          advanced ) ? "                  **    using ML 2 as a map to enforce constraint, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    pre-process  dimension (input  to ML 2) of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    1 for  non-first recommendation (-gbpd 1), **         \n" : "" );
    output << ( (          advanced ) ? "                  **    with lower and upper bounds for direct set **         \n" : "" );
    output << ( (          advanced ) ? "                  **    by -gbpl and -gbpu.                        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-cBUCB:                                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up map function:                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 2 -Zx -z fnb                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -blx M:[0,1,0;1,0,0]*x                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where -z fnb is  a block function and -blx **         \n" : "" );
    output << ( (          advanced ) ? "                  **    sets the map. -qw 2 sets it as ML block 2. **         \n" : "" );
    output << ( (          advanced ) ? "                  **    This implements the map:                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ x10 ] = [ 0 1 0 ] [ z0  ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ x11 ]   [ 1 0 0 ] [ x01 ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                        [ x11 ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    The second  recommendation is generated by **         \n" : "" );
    output << ( (          advanced ) ? "                  **    the  DIRect  optimiser  optimising  on  z, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    while the GP model is built over x.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Set up the Bayesian optimiser:             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 0 -Zx                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbH 11 -gmn 1 -gbim 1 -gbv [ [ 3 ] ; [ 3 ] ]**       \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpp 2 -gbpd 1                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpl [ -4 ] -gbpu [ 4 ]                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Bayesian  optimiser  using  above  GP-BUCB **         \n" : "" );
    output << ( (          advanced ) ? "                  **    using ML 2 as a map to enforce constraint, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    pre-process  dimension (input  to ML 2) of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    1 for non-first  recommendation (-gbpd 1), **         \n" : "" );
    output << ( (          advanced ) ? "                  **    with lower and upper bounds for direct set **         \n" : "" );
    output << ( (          advanced ) ? "                  **    by -gbpl and -gbpu.                        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-cBO:                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** This method uses an outer loop for the common **         \n" : "" );
    output << ( (          advanced ) ? "                  ** variable (y  in this case) and  an inner loop **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to recommend  batch of other  variables (z in **         \n" : "" );
    output << ( (          advanced ) ? "                  ** this case:                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up GP model for inner loop:            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 1 -Zx -z gpr -d 0.01               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Set up GP model for outer loop:            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 2 -Zx -z gpr -d 0.01               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 3. Inner loop is called  as a function, which **         \n" : "" );
    output << ( (          advanced ) ? "                  **    we define here.  This optimises over y and **         \n" : "" );
    output << ( (          advanced ) ? "                  **    z, but  the range  of y  is restricted  so **         \n" : "" );
    output << ( (          advanced ) ? "                  **    that y  = var(0,100).   Note that  this is **         \n" : "" );
    output << ( (          advanced ) ? "                  **    basically GP-UCB-PE.  Note also the use of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -fret to return var(0,1),  var(0,2) (y and **         \n" : "" );
    output << ( (          advanced ) ? "                  **    z), var(51,0) (f(y,z)) and var(53,10) (the **         \n" : "" );
    output << ( (          advanced ) ? "                  **    upper bound for the outer loop).           **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -fM 42 { -gbt 1 -gbH 11 -gmw 1 -gmn 1  **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbv [ [ 3 ] ; [ 9 ] ] -gb 2 \"-tM          **         \n" : "" );
    output << ( (          advanced ) ? "                  **    y^2+(z-1)^2\" \"fret 0 1 -fret 0 2           **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -fret 51 0 -fret 53 10\" fb 1 var(0,100)    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    var(0,100) 1 fb 2 -2 4 1 }                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 4. Run the outer loop.   This will return the **         \n" : "" );
    output << ( (          advanced ) ? "                  **    optimal  result in y  and z.  Note  use of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -fret on y and  z in $evalstring  as these **         \n" : "" );
    output << ( (          advanced ) ? "                  **    would otherwise be lost and -tMv to modify **         \n" : "" );
    output << ( (          advanced ) ? "                  **    variance using the upper bound.  Note also **         \n" : "" );
    output << ( (          advanced ) ? "                  **    that  in  $setstring   y  and  z  must  be **         \n" : "" );
    output << ( (          advanced ) ? "                  **    retrieved from var(50,...) and returned    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    using -fret.                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 0                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **-Zx -gbt 20 -gbH 3 -gmw 2 -gtx [ var(0,100) ; z ] **      \n" : "" );
    output << ( (          advanced ) ? "                  **    -gb 1 \"-MM 42 -Zx -tM var(51,0) -tMv       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    var(53,10)-0.01\" \"-fW 1 var(50,0) -fW 2    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    var(50,1) -fret 0 1 -fret 0 2\" fb 100 -2 3 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 5. Report result:                             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -echo y -echo z                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gBbH n         - EHI  calculation  method  for  multi-recommendation\n" : "" );
    output << ( (          advanced ) ? "                           via  multi-objective  optimisation.   See  -ie  for\n" : "" );
    output << ( (          advanced ) ? "                           possible values.                                   \n" : "" );
    output << ( (          advanced ) ? "         -gB...          - Options for multi-recommendation via multi-objctive\n" : "" );
    output << ( (          advanced ) ? "                           optimisation.   These control the  Bayesian (inner,\n" : "" );
    output << ( (          advanced ) ? "                           direct and generic) options used in the inner loop.\n" : "" );
    output << ( (          advanced ) ? "                           -gB.. canbe -gBy, -gBdc, -gBdf, -gBde, -gBda, -gBbt\n" : "" );
    output << ( (          advanced ) ? "                           -gBdy, -gBbj, -gBfm or -gBfM (eg -gBy  controls the\n" : "" );
    output << ( (          advanced ) ? "                           inner-loop training time, c/f see -gy).            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Transfer learning via kernels:                                                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -x n [ i j .. ] - if the current ML  is a binary SVM  from which SVMs\n" : "" );
    output << ( (          advanced ) ? "                           i,j,... inherit their  kernel via kernel  801 (with\n" : "" );
    output << ( (          advanced ) ? "                           no  additional non-linearities) then this  function\n" : "" );
    output << ( (          advanced ) ? "                           will define  a kernel with n bases (seen  from SVMs\n" : "" );
    output << ( (          advanced ) ? "                           i,j,...):                                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             K(x,y) = sum_ij alpha_i alpha_j K(x,y,zi,zj)     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           that is tuned to optimise performance.  This choice\n" : "" );
    output << ( (          advanced ) ? "                           is regularised  by C  (upper bound on  alpha_i) and\n" : "" );
    output << ( (          advanced ) ? "                           epsilon (linear sum on alpha) for current ML.      \n" : "" );
    output << ( (          advanced ) ? "         -xi  n          - max training iterations for -x (default 20).       \n" : "" );
    output << ( (          advanced ) ? "         -xt  t          - max training time for -x (default 0 = nothing).    \n" : "" );
    output << ( (          advanced ) ? "         -xs  s          - solution tolerance (alpha step) for -x (deft 0.01).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Feature selection options (after parameter tuning):                           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fsx            - feature selection via hill climbing, min LOO error.\n" : "" );
    output << ( (          advanced ) ? "         -fsr            - feature selection via hill climbing, min recal err.\n" : "" );
    output << ( (          advanced ) ? "         -fsc n          - feature  selection via  hill  climbing, min  n-fold\n" : "" );
    output << ( (          advanced ) ? "                           cross validation error.                            \n" : "" );
    output << ( (          advanced ) ? "         -fsC m n        - feature  selection via  hill  climbing, min  n-fold\n" : "" );
    output << ( (          advanced ) ? "                           cross-validation error, randomised, m repetitions. \n" : "" );
    output << ( (          advanced ) ? "         -fsf $file      - feature selection via hill climbing, min test err. \n" : "" );
    output << ( (          advanced ) ? "         -fsF i j $file  - feature selection via hill  climbing, min test err,\n" : "" );
    output << ( (          advanced ) ? "                           ignoring i  vectors  at start,  testing  at most  j\n" : "" );
    output << ( (          advanced ) ? "                           vectors (-1 if all).                               \n" : "" );
    output << ( (          advanced ) ? "         -fss n          - set number  of sweeps (0  default) for  this set of\n" : "" );
    output << ( (          advanced ) ? "                           feature  selection.  If  >1 then,  after the  first\n" : "" );
    output << ( (          advanced ) ? "                           hill-climb(descent) the  algorithm will follow with\n" : "" );
    output << ( (          advanced ) ? "                           hill-descent(climb)   starting  with   the  optimal\n" : "" );
    output << ( (          advanced ) ? "                           features found previously.   The alternating climb,\n" : "" );
    output << ( (          advanced ) ? "                           descent sequence will run n times.                 \n" : "" );
    output << ( (          advanced ) ? "         -fsd            - start  with  existing  features  for  this  set  of\n" : "" );
    output << ( (          advanced ) ? "                           feature selection rather than from scratch.        \n" : "" );
    output << ( (          advanced ) ? "         -fsD            - undoes -fsd.                                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Use -fS...  to use hill  descent rather  than **         \n" : "" );
    output << ( (          advanced ) ? "                  ** hill climbing.  Suffixes as per -tx etc.      **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Fuzzy ML Support (after feature selection):                                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** So-called  fuzzy MLs  use functions  inspired **         \n" : "" );
    output << ( (          advanced ) ? "                  ** by fuzzy  logic to  set the individual  C and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** epsilon  weights  for  each  training  vector **         \n" : "" );
    output << ( (          advanced ) ? "                  ** based  on   some  estimate  of   how  much  a **         \n" : "" );
    output << ( (          advanced ) ? "                  ** particular  vector  \"belongs\"  to its  class. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** The degree of belonging  is calculated by the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** membership function,  typically  based on the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** relative distances to the class centre of the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** class  to which  the vector  belongs and  the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** distances  to  other  classes.   The  inbuilt **         \n" : "" );
    output << ( (          advanced ) ? "                  ** membership functions are:                     **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "    q1 = 0.5+((exp(f*(d_d-d_l)/d)-exp(-f))/(2*(exp(f)-exp(-f))))              \n" : "" );
    output << ( (          advanced ) ? "    q2 = ((2*(0.5+((exp(f*(d_d-d_l)/d)-exp(-f))/(2*(exp(f)-exp(-f))))))-1)^m  \n" : "" );
    output << ( (          advanced ) ? "    q3 = 0.5+((1-(d_l/(r_l+f))/2)                                             \n" : "" );
    output << ( (          advanced ) ? "    q4 = 0.5*(1+tanh(f*((2*g_x)+m)))                                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** (Keller and  Hunt, modified Keller  and Hunt, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Lin  and  Wang, and  cluster-based).  In  all **         \n" : "" );
    output << ( (          advanced ) ? "                  ** cases, for each training vector pair (x,y):   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "    q   = var(2,0) = either t (C weight) or s (epsilon weight), pre-fuzzing.  \n" : "" );
    output << ( (          advanced ) ? "    d_l = var(2,1) = distance from x to the mean of class y.                  \n" : "" );
    output << ( (          advanced ) ? "    d_d = var(2,2) = min distance from x to the mean of any other class !y.   \n" : "" );
    output << ( (          advanced ) ? "    d   = var(2,3) = distance between the mean of classes y and !y.           \n" : "" );
    output << ( (          advanced ) ? "    r_l = var(2,4) = radius of  smallest  sphere  centred at  mean of  class y\n" : "" );
    output << ( (          advanced ) ? "                     containing all elements of class y.                      \n" : "" );
    output << ( (          advanced ) ? "    r_d = var(2,5) = radius of  smallest  sphere  centred at mean  of class !y\n" : "" );
    output << ( (          advanced ) ? "                     containing all elements of class !y.                     \n" : "" );
    output << ( (          advanced ) ? "    g_x = var(2,6) = output of 1-class ML trained  with all vectors of class y\n" : "" );
    output << ( (          advanced ) ? "    q1  = var(2,7) = Keller and Hunt membership.                              \n" : "" );
    output << ( (          advanced ) ? "    q2  = var(2,8) = Modified Keller and Hunt membership.                     \n" : "" );
    output << ( (          advanced ) ? "    q3  = var(2,9) = Lin and Wang membership.                                 \n" : "" );
    output << ( (          advanced ) ? "    q4  = var(2,10)= cluster-based membership.                                \n" : "" );
    output << ( (          advanced ) ? "    f   = var(3,0) = user parameters set below.                               \n" : "" );
    output << ( (          advanced ) ? "    m   = var(3,1) = user parameters set below.                               \n" : "" );
    output << ( (          advanced ) ? "    nu  = var(3,2) = nu value used for clustering.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fzt $fn        - Apply fuzzy to C weights using function given.     \n" : "" );
    output << ( (          advanced ) ? "         -fzs $fn        - Apply fuzzy to epsiln weights using function given \n" : "" );
    output << ( (          advanced ) ? "         -fztk...        - Modify kernel function  to be used in fuzzification\n" : "" );
    output << ( (          advanced ) ? "                           of C weights.   ... is any  of the kernel functions\n" : "" );
    output << ( (          advanced ) ? "                           above (so for  example -fztkt 3 uses the RBF kernel\n" : "" );
    output << ( (          advanced ) ? "                           for  fuzzification).  Note  that the kernel used is\n" : "" );
    output << ( (          advanced ) ? "                           initially set  to the  ML kernel,  and this  simply\n" : "" );
    output << ( (          advanced ) ? "                           modifies it.                                       \n" : "" );
    output << ( (          advanced ) ? "         -fztf f         - Set user parameter f in  -fzt function (default 1).\n" : "" );
    output << ( (          advanced ) ? "                           (should be small +ve number for Lin and Wang).     \n" : "" );
    output << ( (          advanced ) ? "         -fztm m         - Set user parameter m in -fzt function (default 1). \n" : "" );
    output << ( (          advanced ) ? "         -fztNlA nu      - Set nu for 1-class SVM if needed for -fzt (df 0.5).\n" : "" );
    output << ( (          advanced ) ? "         -fzsk...        - Modify kernel function to  be used in fuzzification\n" : "" );
    output << ( (          advanced ) ? "                           of eps weights.  ... is any of the kernel functions\n" : "" );
    output << ( (          advanced ) ? "                           above (so for example  -fztkt 3 uses the RBF kernel\n" : "" );
    output << ( (          advanced ) ? "                           for fuzzification).   Note that the  kernel used is\n" : "" );
    output << ( (          advanced ) ? "                           initially set  to the  ML kernel,  and this  simply\n" : "" );
    output << ( (          advanced ) ? "                           modifies it.                                       \n" : "" );
    output << ( (          advanced ) ? "         -fzsf f         - Set user parameter f in  -fzs function (default 1).\n" : "" );
    output << ( (          advanced ) ? "                           (should be small +ve number for Lin and Wang).     \n" : "" );
    output << ( (          advanced ) ? "         -fzsm m         - Set user parameter m in -fzs function (default 1). \n" : "" );
    output << ( (          advanced ) ? "         -fzsNlA nu      - Set nu for 1-clas SVM if needed for -fzs (dft 0.5).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Bootstrap ML Support (after fuzzy ML):                                        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** To approximate variance  calculations you can **         \n" : "" );
    output << ( (          advanced ) ? "                  ** use (pseuso)-boostrapping.                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up your ML (say ML 0).                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Make B copies of your ML (say 1..B):       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qc 1 0 -Zx                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qc 2 0 -Zx                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **      ...                                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qc B 0 -Zx                                **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 3. \"Bootstrap\" ML copies:                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qw 1 -boot -Zx                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qw 2 -boot -Zx                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **      ...                                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qw B -boot -Zx                            **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 4. Set up multi-block averaging block (B+1):  **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qw B+1                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -mba 1 1                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -mba 2 2                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **      ...                                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -mba B B                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 5. ML B+1 now calculates  variance and allows **         \n" : "" );
    output << ( (          advanced ) ? "                  **    (800) kernel transfer inc kernel variance. **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -boot           - pseudo-bootstrap ML by taking all (non-constrained)\n" : "" );
    output << ( (          advanced ) ? "                           training vectors (there  are m), randomly selecting\n" : "" );
    output << ( (          advanced ) ? "                           m with replacement, then constraining the rest.    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Macros (after bootstrap ML):                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -M0             - execute macro var(130,0):                          \n" : "" );
    output << ( ( basic || advanced ) ? "                           1. normalise training data.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           2. perform grid-search of C,d on polynomial kernel.\n" : "" );
    output << ( ( basic || advanced ) ? "                           3. perform hill-climbing feature selection.        \n" : "" );
    output << ( ( basic || advanced ) ? "                           4. repeat steps 2,3.                               \n" : "" );
    output << ( ( basic || advanced ) ? "                           5. repeat steps 2,3,4 for RBF kernel (C,g) and arc-\n" : "" );
    output << ( ( basic || advanced ) ? "                              cosine kernel (C,n).                            \n" : "" );
    output << ( ( basic || advanced ) ? "                           6. select most optimal kernel and C found.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -M1             - execute macro var(130,1):                          \n" : "" );
    output << ( ( basic || advanced ) ? "                           1. normalise training data.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           2. perform grid-search of C,d on polynomial kernel.\n" : "" );
    output << ( ( basic || advanced ) ? "                           3. perform hill-climbing feature selection.        \n" : "" );
    output << ( ( basic || advanced ) ? "                           4. repeat steps  2,3 for RBF kernel  (C,g) and arc-\n" : "" );
    output << ( ( basic || advanced ) ? "                              cosine kernel (C,n).                            \n" : "" );
    output << ( ( basic || advanced ) ? "                           5. select most optimal kernel and C found.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -M2             - execute macro var(130,2):                          \n" : "" );
    output << ( ( basic || advanced ) ? "                           1. normalise training data.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           2. perform grid-search of C,d on polynomial kernel.\n" : "" );
    output << ( ( basic || advanced ) ? "                           3. repeat  step  2 for  RBF kernel  (C,g) and  arc-\n" : "" );
    output << ( ( basic || advanced ) ? "                              cosine kernel (C,n).                            \n" : "" );
    output << ( ( basic || advanced ) ? "                           4. select most optimal kernel and C found.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -M3             - execute macro var(130,3):                          \n" : "" );
    output << ( ( basic || advanced ) ? "                           1. perform grid-search of C,d on polynomial kernel.\n" : "" );
    output << ( ( basic || advanced ) ? "                           2. perform hill-climbing feature selection.        \n" : "" );
    output << ( ( basic || advanced ) ? "                           3. repeat steps 1,2.                               \n" : "" );
    output << ( ( basic || advanced ) ? "                           4. repeat steps 1,2,3 for RBF kernel (C,g) and arc-\n" : "" );
    output << ( ( basic || advanced ) ? "                              cosine kernel (C,n).                            \n" : "" );
    output << ( ( basic || advanced ) ? "                           5. select most optimal kernel and C found.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -M4             - execute macro var(130,4):                          \n" : "" );
    output << ( ( basic || advanced ) ? "                           1. perform grid-search of C,d on polynomial kernel.\n" : "" );
    output << ( ( basic || advanced ) ? "                           2. perform hill-climbing feature selection.        \n" : "" );
    output << ( ( basic || advanced ) ? "                           3. repeat steps  1,2 for RBF kernel  (C,g) and arc-\n" : "" );
    output << ( ( basic || advanced ) ? "                              cosine kernel (C,n).                            \n" : "" );
    output << ( ( basic || advanced ) ? "                           4. select most optimal kernel and C found.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -M5             - execute macro var(130,5):                          \n" : "" );
    output << ( ( basic || advanced ) ? "                           1. perform grid-search of C,d on polynomial kernel.\n" : "" );
    output << ( ( basic || advanced ) ? "                           2. repeat  step  1 for  RBF kernel  (C,g) and  arc-\n" : "" );
    output << ( ( basic || advanced ) ? "                              cosine kernel (C,n).                            \n" : "" );
    output << ( ( basic || advanced ) ? "                           3. select most optimal kernel and C found.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -M6...11        - like -M0...5, but only search the polynomial kernel\n" : "" );
    output << ( ( basic || advanced ) ? "         -M12            - grid-search to minimise negative log-likelihood:   \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (21 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M13            - grid-search to minimise recall error:              \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (21 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M14            - grid-search to minimise leave-one-out error:       \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (21 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M15            - grid-search to minimise random 10-fold error:      \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (21 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M16            - grid-search to minimise negative log-likelihood:   \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-1 <= g <= 1e1 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M17            - grid-search to minimise recall error:              \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-1 <= g <= 1e1 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M18            - grid-search to minimise leave-one-out error:       \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-1 <= g <= 1e1 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M19            - grid-search to minimise random 10-fold error:      \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-1 <= g <= 1e1 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M20            - grid-search to minimise negative log-likelihood:   \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           o d = 1,2,3,4,5                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -M21            - grid-search to minimise recall error:              \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           o d = 1,2,3,4,5                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -M22            - grid-search to minimise leave-one-out error:       \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           o d = 1,2,3,4,5                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -M23            - grid-search to minimise random 10-fold error:      \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-2 <= C <= 1e2 (11 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           o d = 1,2,3,4,5                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -M24            - grid-search to minimise negative log-likelihood:   \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-1 <= g <= 1e1 (21 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M25            - grid-search to minimise recall error:              \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-1 <= g <= 1e1 (21 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M26            - grid-search to minimise leave-one-out error:       \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-1 <= g <= 1e1 (21 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M27            - grid-search to minimise random 10-fold error:      \n" : "" );
    output << ( ( basic || advanced ) ? "                           o 1e-1 <= g <= 1e1 (21 increments)                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -M28            - grid-search to minimise negative log-likelihood:   \n" : "" );
    output << ( ( basic || advanced ) ? "                           o d = 1,2,3,4,5                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -M29            - grid-search to minimise recall error:              \n" : "" );
    output << ( ( basic || advanced ) ? "                           o d = 1,2,3,4,5                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -M30            - grid-search to minimise leave-one-out error:       \n" : "" );
    output << ( ( basic || advanced ) ? "                           o d = 1,2,3,4,5                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -M31            - grid-search to minimise random 10-fold error:      \n" : "" );
    output << ( ( basic || advanced ) ? "                           o d = 1,2,3,4,5                                    \n" : "" );
    output << ( (          advanced ) ? "         -MM  i          - execute  var(130,j) (must  be a string).   See pre-\n" : "" );
    output << ( (          advanced ) ? "                           defined macros below.                              \n" : "" );
    output << ( (          advanced ) ? "         -MF  i          - like -MM, but works  as a function (won't overwrite\n" : "" );
    output << ( (          advanced ) ? "                           variables).                                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Optimization options (after macros):                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -oo             - do not optimise the ML this time.                  \n" : "" );
    output << ( ( basic || advanced ) ? "         -oO             - optimise the ML this time (default).               \n" : "" );
    output << ( ( basic || advanced ) ? "                           NB: -oo and  -oO are persistent, so if  you use -oo\n" : "" );
    output << ( ( basic || advanced ) ? "                               to save time you need  to use -oO later so that\n" : "" );
    output << ( ( basic || advanced ) ? "                               optimisation occurs.                           \n" : "" );
    output << ( ( basic || advanced ) ? "         -oe  e          - accuracy e>=0 required of  g(x) in soln (dflt .01).\n" : "" );
    output << ( ( basic || advanced ) ? "                           (use -oe A to set accuracy = max(0.01*eps,100*zt)).\n" : "" );
    output << ( ( basic || advanced ) ? "         -oz  z          - set zero tolerance.                                \n" : "" );
    output << ( ( basic || advanced ) ? "         -ot  m          - terminate optimization after  m iterations, even if\n" : "" );
    output << ( ( basic || advanced ) ? "                           solution not found (default LONG_MAX).             \n" : "" );
    output << ( ( basic || advanced ) ? "         -oy  t          - set  training  timeout  time  (t  in  seconds).  If\n" : "" );
    output << ( ( basic || advanced ) ? "                           training is not  completed after this  time then it\n" : "" );
    output << ( ( basic || advanced ) ? "                           will be stopped  prematurely.  t is a float.  Times\n" : "" );
    output << ( ( basic || advanced ) ? "                           less than 1 second will be interpretted unlimited. \n" : "" );
    output << ( ( basic || advanced ) ? "         -oM  n          - size of  kernel  cache  in MB  (default  200).  For\n" : "" );
    output << ( ( basic || advanced ) ? "                           unlimited (aka 50000 rows), select -1.             \n" : "" );
    output << ( ( basic || advanced ) ? "         -ofy            - turn on Cholesky factorisation fudging (that is, if\n" : "" );
    output << ( ( basic || advanced ) ? "                           Cholesky becomes  near singular  then  add a  small\n" : "" );
    output << ( ( basic || advanced ) ? "                           diagonal offset.  This is  arguably a bad idea, but\n" : "" );
    output << ( ( basic || advanced ) ? "                           sometimes it is necessary to make it work.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -ofn            - turn off Cholesky factorisation fudging (default). \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -om  {a,s,d,g}  - optimisation method used.  Methods are:            \n" : "" );
    output << ( (          advanced ) ? "                           a - active set optimisation (default).             \n" : "" );
    output << ( (          advanced ) ? "                           s - SMO   optimisation   (only  valid   for  binary\n" : "" );
    output << ( (          advanced ) ? "                               classification, regression or single-class with\n" : "" );
    output << ( (          advanced ) ? "                               no tube shrinking).                            \n" : "" );
    output << ( (          advanced ) ? "                           d - D2C  optimisation   (only   valid   for  binary\n" : "" );
    output << ( (          advanced ) ? "                               classification, regression or single-class with\n" : "" );
    output << ( (          advanced ) ? "                               no tube shrinking).                            \n" : "" );
    output << ( (          advanced ) ? "                           g - gradient descent (using -ofa, -of... settings).\n" : "" );
    output << ( (          advanced ) ? "         -ofa {0,1,2,3}  - method for 4-norm optimisation:                    \n" : "" );
    output << ( (          advanced ) ? "                           0: simple gradient steps.                          \n" : "" );
    output << ( (          advanced ) ? "                           1: line-search gradient steps (default).           \n" : "" );
    output << ( (          advanced ) ? "                           2: simple Newton steps.                            \n" : "" );
    output << ( (          advanced ) ? "                           3: line-search Newton steps.                       \n" : "" );
    output << ( (          advanced ) ? "                           (2 and 3 are not recommended unless using -R q).   \n" : "" );
    output << ( (          advanced ) ? "         -ofe e          - tolerance e>=0 for 4-norm optim (default 0.005).   \n" : "" );
    output << ( (          advanced ) ? "         -ofm m          - Momentum factor for 4-norm optim (default 0.05).   \n" : "" );
    output << ( (          advanced ) ? "         -ofr t          - Learning rate for 4-norm optim (default 0.3).      \n" : "" );
    output << ( (          advanced ) ? "         -ofs s          - lr scaleback factor for 4-norm optim (default 0.8).\n" : "" );
    output << ( (          advanced ) ? "         -oft m          - max iterations for 4-norm optim (default 100).     \n" : "" );
    output << ( (          advanced ) ? "         -ofM n          - max 4-kernel cache for 4-norm optim (default 1000).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- ONN specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -olr r          - sets learning rate (def 0.01).                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SSV specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -oge e          - tolerance e>=0 for SSV optim (default 0.005).      \n" : "" );
    output << ( ( basic || advanced ) ? "         -ogm m          - Momentum factor for SSV optim (default 0.05).      \n" : "" );
    output << ( ( basic || advanced ) ? "         -ogr t          - Learning rate for SSV optim (default 0.3).         \n" : "" );
    output << ( ( basic || advanced ) ? "         -ogs s          - lr scaleback factor for SSV optim (default 0.8).   \n" : "" );
    output << ( ( basic || advanced ) ? "         -ogt m          - max iterations for SSV optim (default 100).        \n" : "" );
    output << ( ( basic || advanced ) ? "         -ogT n          - max training time for SSV optim (default 1000).    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- MLM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -omr r          - Learning rate for MLM optim (default 0.3).         \n" : "" );
    output << ( ( basic || advanced ) ? "         -ome d          - Stop when change in average error less than (0.02).\n" : "" );
    output << ( ( basic || advanced ) ? "         -oms s          - Initialisation sparsity (default 1).               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Performance estimation options (after training):                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** The result will be stored in var(1,1). **                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tQ             - Store the actual outputs in var(1,5), var(1,6).    \n" : "" );
    output << ( (          advanced ) ? "         -tnQ            - Don't store the actual output (default).           \n" : "" );
    output << ( (          advanced ) ? "         -tvar           - Save x variance to .var file when testing.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tn  n          - Sets performance measure:                          \n" : "" );
    output << ( (          advanced ) ? "                           0: error (default)                                 \n" : "" );
    output << ( (          advanced ) ? "                           1: 1 - precision.                                  \n" : "" );
    output << ( (          advanced ) ? "                           2: 1 - recall.                                     \n" : "" );
    output << ( (          advanced ) ? "                           3: 1 - F1 score.                                   \n" : "" );
    output << ( (          advanced ) ? "                           4: 1 - AUC score.                                  \n" : "" );
    output << ( (          advanced ) ? "                           5: 1 - sparsity.                                   \n" : "" );
    output << ( (          advanced ) ? "         -tm  $fn        - Sets performance measurement function, based on:   \n" : "" );
    output << ( (          advanced ) ? "                           var(1,2)  = error (default - see below).           \n" : "" );
    output << ( (          advanced ) ? "                           var(1,3)  = count vector.                          \n" : "" );
    output << ( (          advanced ) ? "                           var(1,4)  = confusion matrix.                      \n" : "" );
    output << ( (          advanced ) ? "                           var(1,37) = accuracy   (1-error   for   classifier,\n" : "" );
    output << ( (          advanced ) ? "                                       1/(error+eps) for regressor).          \n" : "" );
    output << ( (          advanced ) ? "                           var(1,38) = precision (if binary classifier).      \n" : "" );
    output << ( (          advanced ) ? "                           var(1,39) = recall (if binary classifier).         \n" : "" );
    output << ( (          advanced ) ? "                           var(1,40) = F1 score (if binary classifier).       \n" : "" );
    output << ( (          advanced ) ? "                           var(1,41) = AUC score (if binary classifier).      \n" : "" );
    output << ( (          advanced ) ? "                           var(1,42) = sparsity of ML (0 dense -> 1 sparse).  \n" : "" );
    output << ( (          advanced ) ? "                           all other variables are described below.           \n" : "" );
    output << ( (          advanced ) ? "         -tM  $fn        - Like  -tm, but  this then  directly  evaluates  the\n" : "" );
    output << ( (          advanced ) ? "                           performance  measurement   function  straight  away\n" : "" );
    output << ( (          advanced ) ? "                           without  first  evaluating  performance.  That  is,\n" : "" );
    output << ( (          advanced ) ? "                           this  function can  load anything  into the  result\n" : "" );
    output << ( (          advanced ) ? "                           variable,  allowing   non-standard  usage   of  for\n" : "" );
    output << ( (          advanced ) ? "                           example using -g to minimise an arbitrary function.\n" : "" );
    output << ( (          advanced ) ? "         -tMd d r $fn    - Like -tM,  but  evaluates  L2 distance  between  r,\n" : "" );
    output << ( (          advanced ) ? "                           which must be  an RKHS Vector, and $fn,  which must\n" : "" );
    output << ( (          advanced ) ? "                           be  a   function  of   var(0,0),   var(0,1),   ...,\n" : "" );
    output << ( (          advanced ) ? "                           var(0,d-1), where d is the dimension.              \n" : "" );
    output << ( (          advanced ) ? "         -tMD d $fn $fn  - Like  -tMd but  between two  functions, integrating\n" : "" );
    output << ( (          advanced ) ? "                           over [0,1]^d.                                      \n" : "" );
    output << ( (          advanced ) ? "         -tMv v          - Appends variance  to the result.  This  is used for\n" : "" );
    output << ( (          advanced ) ? "                           Bayesian optimisation  - it operates  by converting\n" : "" );
    output << ( (          advanced ) ? "                           the  result r  to a set  { r v }.  This  must  come\n" : "" );
    output << ( (          advanced ) ? "                           after the result is evaluated (eg after -tM).  Note\n" : "" );
    output << ( (          advanced ) ? "                           that -tC stores variance in var(1,46).             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tMpy   fname x - evaluate python script fname with scalar x.        \n" : "" );
    output << ( (          advanced ) ? "         -tMpyv  fname x - evaluate python script fname with vector x.        \n" : "" );
    output << ( (          advanced ) ? "         -tMpyf  fname x - evaluate python script fname with function x.      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tMxpy  fname x - like -tMpy, but write to lRateList.txt (15).*      \n" : "" );
    output << ( (          advanced ) ? "         -tMxpyv fname x - like -tMpyv, but write to lRateList.txt (15).*     \n" : "" );
    output << ( (          advanced ) ? "         -tMxpyf fname x - like -tMpyf, but write to lRateList.txt (15).*     \n" : "" );
    output << ( (          advanced ) ? "                           *result loaded from pyres.txt.                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tMypy  fname x - like -tMpy, but write to lRateList.txt (15).*      \n" : "" );
    output << ( (          advanced ) ? "         -tMypyv fname x - like -tMpyv, but write to lRateList.txt (15).*     \n" : "" );
    output << ( (          advanced ) ? "         -tMypyf fname x - like -tMpyf, but write to lRateList.txt (15).*     \n" : "" );
    output << ( (          advanced ) ? "                           *result loaded from pyres.txt.                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tMexe  fname x - evaluate executable fname with scalar x.           \n" : "" );
    output << ( (          advanced ) ? "         -tMexev fname x - evaluate executable fname with vector x.           \n" : "" );
    output << ( (          advanced ) ? "         -tMexef fname x - evaluate executable fname with function x.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Function  evaluation works as  follows.  SVM- **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Heavy works as follows:                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - SVMHeavy opens a unix socket as server.     **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - SVMHeavy calls  external program  pname and **         \n" : "" );
    output << ( (          advanced ) ? "                  **   tells it the socket name, fname sockname.   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - SVMHeavy waits for pname to send gentype i: **         \n" : "" );
    output << ( (          advanced ) ? "                  **   * if !NULL, scalar x SVMHeavy returns x.    **         \n" : "" );
    output << ( (          advanced ) ? "                  **   * if !NULL, vector x SVMHeavy returns x(i). **         \n" : "" );
    output << ( (          advanced ) ? "                  **   * if !NULL, function x SVMHeavy returns x(i)**         \n" : "" );
    output << ( (          advanced ) ? "                  ** - when  NULL   received  SVMHeavy   waits  to **         \n" : "" );
    output << ( (          advanced ) ? "                  **   receive the result, then kills the socket.  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** The python script is  assumed to wait 5 secs, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** open the unix socket as  client, then perform **         \n" : "" );
    output << ( (          advanced ) ? "                  ** as per above.                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Example: see unixevaltest.py                  **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -tl             - Calculate  negative  (of) log-likelihood.  This  is\n" : "" );
    output << ( ( basic || advanced ) ? "                           well defined for  the GPR and formally  defined (by\n" : "" );
    output << ( ( basic || advanced ) ? "                           analogy) for SVM and LSV.                          \n" : "" );
    output << ( ( basic || advanced ) ? "         -tx             - Calculate  leave-one-out error.  In  the regression\n" : "" );
    output << ( ( basic || advanced ) ? "                           case the result is the RMS leave-one-error.        \n" : "" );
    output << ( ( basic || advanced ) ? "         -tr             - Calculate recall error.  In the regression case the\n" : "" );
    output << ( ( basic || advanced ) ? "                           result is the RMS recall error.                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -tc  n          - Calculate the n-fold cross-validation error (in the\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression case RMS cross-fold error).             \n" : "" );
    output << ( ( basic || advanced ) ? "         -tC  m n        - Calculate the n-fold cross-validation error (in the\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression   case  RMS   cross-fold   error)   with\n" : "" );
    output << ( ( basic || advanced ) ? "                           randomisation (ie. shuffle points before starting).\n" : "" );
    output << ( ( basic || advanced ) ? "                           This variant will repeat m times and average result\n" : "" );
    output << ( ( basic || advanced ) ? "                           over all runs.  Note  that the result of  this will\n" : "" );
    output << ( ( basic || advanced ) ? "                           vary from run to run due to randomisation.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -tf  $file      - validate ML using file, result sent to file.res.   \n" : "" );
    output << ( ( basic || advanced ) ? "         -tF  i j $file  - validate ML  using file,  result sent  to file.res,\n" : "" );
    output << ( ( basic || advanced ) ? "                           ignoring  i vectors  at start,  testing  at  most j\n" : "" );
    output << ( ( basic || advanced ) ? "                           vectors (-1 if all).                               \n" : "" );
    output << ( ( basic || advanced ) ? "         -tb i j m v     - test performance of ML  with added \"noise\" features\n" : "" );
    output << ( ( basic || advanced ) ? "                           in x vectors.  Tests range  from i->j such features\n" : "" );
    output << ( ( basic || advanced ) ? "                           where each feature is gaussian random N(m,v).      \n" : "" );
    output << ( ( basic || advanced ) ? "         -tV  [d] [[x]]  - test training vectors (see -AV).                   \n" : "" );
    output << ( (          advanced ) ? "         -tg  N d f v    - generate and test training data. N pairs generated,\n" : "" );
    output << ( (          advanced ) ? "                           vectors have dim d, function is f with noise var v,\n" : "" );
    output << ( (          advanced ) ? "                           features N(0,1).                                   \n" : "" );
    output << ( (          advanced ) ? "         -tG  N d f v    - generate and test training data. N pairs generated,\n" : "" );
    output << ( (          advanced ) ? "                           vectors have dim d, function is f with noise var v,\n" : "" );
    output << ( (          advanced ) ? "                           features U(0,1).                                   \n" : "" );
    output << ( (          advanced ) ? "         -tgc N d f v c  - like -tg, but only uses vectors x for which c(x)=1.\n" : "" );
    output << ( (          advanced ) ? "         -tGc N d f v c  - like -tG, but only uses vectors x for which c(x)=1.\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** If a test stalls,  it may be  useful to reset **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the ML prior to performing each optimisation. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Use the z suffix  to do this.  To  obtain raw **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ML outputs for recdiv tests, use the B suffix.**         \n" : "" );
    output << ( (          advanced ) ? "                  ** To  test only  on  first cross  batch use  Z. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Other suffixes for -tf and -tF as per -AA.    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** The complete list of suffixed options are:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tx, -txz, -txB, -txzB                        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tr, -trz, -trB, -trzB                        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tc, -tcz, -tcB, -tczB                        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tC, -tCz, -tCB, -tCzB                        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tfe,-tfi,-tfl,-tfu,-tfel,-tfeu,-tfil,-tfiu,  **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tFe,-tFi,-tFr,-tFl,-tFu,-tFel,-tFeu,-tFil,   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tFiu,-tFrl,-tFru                             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tfeB,-tfiB,-tflB,-tfuB,-tfelB,-tfeuB,-tfilB,-tfiuB, **  \n" : "" );
    output << ( (          advanced ) ? "                  ** -tFeB,-tFiB,-tFrB,-tFlB,-tFuB,-tFelB,-tFeuB,-tFilB,  **  \n" : "" );
    output << ( (          advanced ) ? "                  ** -tFiuB,-tFrlB,-tFruB                          **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tV  (plus I/R variants)                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Single class:                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tfl, -tFl etc: these  will assume base truth **         \n" : "" );
    output << ( (          advanced ) ? "                  ** labels are present in  training file and test **         \n" : "" );
    output << ( (          advanced ) ? "                  ** against these.                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP Pareto:                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP Pareto  acts as  a classifier  (interior / **         \n" : "" );
    output << ( (          advanced ) ? "                  ** exterior).  Hence  if test file does not have **         \n" : "" );
    output << ( (          advanced ) ? "                  ** class information use -tfl filename 1.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Logfiles written:                             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.cfm: confusion matrix.               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.cnt: vector of numbers in each class.**         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.clr: error for each class.           **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.res: actual output g(x) for vectors. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.cla: classification h(g(x)) " ".     **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.gra: classification gradients.       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.sum: results summary, containing:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **   Accuracy: this is 1-error for classifier or **         \n" : "" );
    output << ( (          advanced ) ? "                  **             1/(RMSerror+1e-6) for regressor.  **         \n" : "" );
    output << ( (          advanced ) ? "                  **   Precision: binary only, otherwise -1.       **         \n" : "" );
    output << ( (          advanced ) ? "                  **   Recall: binary only, otherwise -1.          **         \n" : "" );
    output << ( (          advanced ) ? "                  **   F1 score: binary only, otherwise -1.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **   AUC: binary only, otherwise -1.             **         \n" : "" );
    output << ( (          advanced ) ? "                  **   Sparsity: meaning depends on ML.            **         \n" : "" );
    output << ( (          advanced ) ? "                  **   Error: classification error for classifier, **         \n" : "" );
    output << ( (          advanced ) ? "                  **             RMSerror for regressor, see above.**         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Note:- * is the base logname set by -L or -LL **         \n" : "" );
    output << ( (          advanced ) ? "                  **      - method is LOO for -tx, recall for -tr, **         \n" : "" );
    output << ( (          advanced ) ? "                  **        nfoldcross for -tc,  filename for -tf, **         \n" : "" );
    output << ( (          advanced ) ? "                  **        and test for -tV (and -tW below).      **         \n" : "" );
    output << ( (          advanced ) ? "                  **      - for -tc and  -tC a suffix  is added to **         \n" : "" );
    output << ( (          advanced ) ? "                  **        .res, .cla and .gra  to indicate which **         \n" : "" );
    output << ( (          advanced ) ? "                  **        repetition output  refers to (eg .res0 **         \n" : "" );
    output << ( (          advanced ) ? "                  **        is the  output for the first  round of **         \n" : "" );
    output << ( (          advanced ) ? "                  **        tests run, .res1 for the second etc).  **         \n" : "" );
    output << ( (          advanced ) ? "                  **      - if  MEX  is  present  these  are  also **         \n" : "" );
    output << ( (          advanced ) ? "                  **        written to  Matlab variables  with the **         \n" : "" );
    output << ( (          advanced ) ? "                  **        same  name,  except that  all .'s  are **         \n" : "" );
    output << ( (          advanced ) ? "                  **        replaced by _'s.                       **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- MEX (Matlab) only options                     --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tW  $yvar $xvar -test training vectors (see -AW)                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Reporting options (after performance estimation):                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -s   $file      - write ML to $file.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -a   $file      - write alpha vector to $file.                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -b   $file      - write bias value to $file.                         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** -a and -b also write to mex if present        **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -echo $fn       - echoes $fn to cerr and summary file.               \n" : "" );
    output << ( (          advanced ) ? "         -ECHO $fn       - echoes  $fn (evaluated and  finalised) to cerr  and\n" : "" );
    output << ( (          advanced ) ? "                           summary file.                                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -echosock $sock $fn - echoes $fn to unix socket server $sock.        \n" : "" );
    output << ( (          advanced ) ? "         -ECHOsock $sock $fn - echoes $fn (eval/final) to \" \" \" $sock.        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -K0             - evaluate K0().                                     \n" : "" );
    output << ( (          advanced ) ? "         -K1  [x]        - evaluate K1(x).                                    \n" : "" );
    output << ( (          advanced ) ? "         -K2  [x] [y]    - evaluate K2(x,y).                                  \n" : "" );
    output << ( (          advanced ) ? "         -K3  [x] [y] [u]- evaluate K3(x,y,u).                                \n" : "" );
    output << ( (          advanced ) ? "      -K4 [x] [y] [u] [v]- evaluate K4(x,y,u,v).                              \n" : "" );
    output << ( (          advanced ) ? "         -Km  m [x0] ... - evaluate Km(x0,...).                               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hU  [x]        - test vector (see -AU) and echo result.             \n" : "" );
    output << ( (          advanced ) ? "         -hY  x          - test vector (see -AY) and echo result.             \n" : "" );
    output << ( (          advanced ) ? "         -hZ  x m        - like -hY but for ML m.                             \n" : "" );
    output << ( (          advanced ) ? "         -hV  [[x]]      - test vectors (see -AV) and echo results.           \n" : "" );
    output << ( (          advanced ) ? "         -hW  i          - test training vector i and echo result.            \n" : "" );
    output << ( (          advanced ) ? "         -hX             - test all training vectors and echo result.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hUv [x]        - variance test vector (see -AU) and echo result.    \n" : "" );
    output << ( (          advanced ) ? "         -hYv x          - variance test vector (see -AY) and echo result.    \n" : "" );
    output << ( (          advanced ) ? "         -hZv x m        - like -hYv but for ML m.                            \n" : "" );
    output << ( (          advanced ) ? "         -hVv [[x]]      - variance test vectors (see -AV) and echo results.  \n" : "" );
    output << ( (          advanced ) ? "         -hWv i          - variance test training vector i and echo result.   \n" : "" );
    output << ( (          advanced ) ? "         -hXv            - variance test all training vectors and echo result.\n" : "" );
    output << ( (          advanced ) ? "         -hVV [[x]]      - covariance matrix version of -hVv                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hUc [x] [y]    - covariance test vectors and echo result.           \n" : "" );
    output << ( (          advanced ) ? "         -hYc x y        - covariance test vectors and echo result.           \n" : "" );
    output << ( (          advanced ) ? "         -hZc x y m      - covariance test vectors and echo result.           \n" : "" );
    output << ( (          advanced ) ? "         -hWc i j        - covariance test vectors and echo result.           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hP [x] p mu B n- test Pr(1:p) stability, ||.||_n norm.              \n" : "" );
    output << ( (          advanced ) ? "         -hp [x] p mu B  - test Pr(1:p) stability, rotated ||.||_inf norm.    \n" : "" );
    output << ( (          advanced ) ? "         -hPi  i p mu B n- test Pr(1:p) stability, ||.||_n norm.              \n" : "" );
    output << ( (          advanced ) ? "         -hpi  i p mu B  - test Pr(1:p) stability, rotated ||.||_inf norm.    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** For -hU, -hV, -hW, -hX results are stored in: **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** var(1,8)  = result of evaluation.             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** var(1,9)  = classification (sgn).             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** var(1,10) = output of ML (not just sgn).      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Gradients of  g, var and cov can  be accessed **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (when available) using augmented format.  For **         \n" : "" );
    output << ( (          advanced ) ? "                  ** example -hU  [ x :::  6:1  ] the  gradient of **         \n" : "" );
    output << ( (          advanced ) ? "                  ** g(x), and -hUv [ x :::  6:2 ] the variance of **         \n" : "" );
    output << ( (          advanced ) ? "                  ** d^2/dx^2 g(x).                                **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- MEX (Matlab) only options                     --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hM  $dst v     - Set MATLAB variable $dst = v (evaluated).          \n" : "" );
    output << ( (          advanced ) ? "         -hN  n $dst     - Set  var(1,n) =  MATLAB variable  $dst.  If type is\n" : "" );
    output << ( (          advanced ) ? "                           ambiguous favour existing type of var(1,n).        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Multipass and input switching options (asynchronous):                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Zx             - Multiple training  passes are  possible by  putting\n" : "" );
    output << ( ( basic || advanced ) ? "                           commands into blocks,  separated by -Zx.  The first\n" : "" );
    output << ( ( basic || advanced ) ? "                           block starts  things going,  and subsequent  blocks\n" : "" );
    output << ( ( basic || advanced ) ? "                           can  adjust  parameters,  add/remove  points,   run\n" : "" );
    output << ( ( basic || advanced ) ? "                           various  tests etc.   Blocks are  separated  by the\n" : "" );
    output << ( ( basic || advanced ) ? "                           -Zx  flag.  Note  that a  separate logfile  will be\n" : "" );
    output << ( ( basic || advanced ) ? "                           written for each block and may overwrite.          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Zmute          - mute standard error.                               \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZMute          - mute standard out.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZMUTE          - mute standard error and standard out.              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Zunmute        - un-mute standard error.                            \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZunMute        - un-mute standard out.                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZunMUTE        - un-mute standard error and standard out.           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zs  s          - seed random number generator  with seed s.  To seed\n" : "" );
    output << ( (          advanced ) ? "                           with time use -Zs time.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zinteract      - start interactive (god) mode operation.            \n" : "" );
    output << ( (          advanced ) ? "         -Zgod           - enable entry to god-mode during optimisation if key\n" : "" );
    output << ( (          advanced ) ? "                           pressed (default).                                 \n" : "" );
    output << ( (          advanced ) ? "         -Zdawkins       - disable god-mode during optimisation.              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zif b args     - if condition b is true (non-zero) then process args\n" : "" );
    output << ( (          advanced ) ? "                           args should be enclosed in {}.                     \n" : "" );
    output << ( (          advanced ) ? "         -Zifelse b t f  - if condition b is true (non-zero) then process args\n" : "" );
    output << ( (          advanced ) ? "                           t, otherwise process args f.                       \n" : "" );
    output << ( (          advanced ) ? "         -Zwhile b args  - while condition b is true (non-zero), process args \n" : "" );
    output << ( (          advanced ) ? "         -Zrep i args    - process args i times (count var(0,1000)=0,1,...).  \n" : "" );
    output << ( (          advanced ) ? "         -Zwait b        - pause until conditions b become true (non-zero).   \n" : "" );
    output << ( (          advanced ) ? "         -Zusleep n      - Sleep for n usec (accuracy is system dependent).   \n" : "" );
    output << ( (          advanced ) ? "         -Zmsleep n      - Sleep for n msec (accuracy is system dependent).   \n" : "" );
    output << ( (          advanced ) ? "         -Zsleep  n      - Sleep for n sec  (accuracy is system dependent).   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zw  $file      - process args from $file (push).                    \n" : "" );
    output << ( (          advanced ) ? "         -Zk             - process args from cin (push).                      \n" : "" );
    output << ( (          advanced ) ? "         -Zc  n          - process args from shared stream n.                 \n" : "" );
    output << ( (          advanced ) ? "         -Zwf $file $of  - process args from $file, feedback to $of (push).   \n" : "" );
    output << ( (          advanced ) ? "         -Zkf            - process args from cin, feedback cout (\").          \n" : "" );
    output << ( (          advanced ) ? "         -Zaw $file      - pop first, then -Zw $file.                         \n" : "" );
    output << ( (          advanced ) ? "         -Zak            - pop first, then -Zk.                               \n" : "" );
    output << ( (          advanced ) ? "         -Zac n          - pop first, then -Zc.                               \n" : "" );
    output << ( (          advanced ) ? "         -Zawf $file $of - pop first, then -Zwf $file $of.                    \n" : "" );
    output << ( (          advanced ) ? "         -Zakf           - pop first, then -Zkf.                              \n" : "" );
#ifdef ALLOW_SOCKETS
    output << ( (          advanced ) ? "         -Zu  $p         - process args from UDP socket client port $p (push).\n" : "" );
    output << ( (          advanced ) ? "         -Zt  $p         - process args from TCP socket client port $p (push).\n" : "" );
    output << ( (          advanced ) ? "         -Zn  $f         - process args from unix sock. client file $f (push).\n" : "" );
    output << ( (          advanced ) ? "         -ZU  $p $addr   - process args from UDP socket server port $p (push).\n" : "" );
    output << ( (          advanced ) ? "         -ZT  $p $addr   - process args from TCP socket server port $p (push).\n" : "" );
    output << ( (          advanced ) ? "         -ZN  $f         - process args from unix sock. server file $f (push).\n" : "" );
    output << ( (          advanced ) ? "         -Zuf $p         - like -Zu but with feedback.                        \n" : "" );
    output << ( (          advanced ) ? "         -Ztf $p         - like -Zt but with feedback.                        \n" : "" );
    output << ( (          advanced ) ? "         -Znf $f         - like -Zn but with feedback.                        \n" : "" );
    output << ( (          advanced ) ? "         -ZUf $p $addr   - like -ZU but with feedback.                        \n" : "" );
    output << ( (          advanced ) ? "         -ZTf $p $addr   - like -ZT but with feedback.                        \n" : "" );
    output << ( (          advanced ) ? "         -ZNf $f         - like -ZN but with feedback.                        \n" : "" );
    output << ( (          advanced ) ? "         -Zau $p         - pop first, then -Zu $p.                            \n" : "" );
    output << ( (          advanced ) ? "         -Zat $p         - pop first, then -Zt $p.                            \n" : "" );
    output << ( (          advanced ) ? "         -Zan $f         - pop first, then -Zn $p.                            \n" : "" );
    output << ( (          advanced ) ? "         -ZaU $p $addr   - pop first, then -ZU $p $addr.                      \n" : "" );
    output << ( (          advanced ) ? "         -ZaT $p $addr   - pop first, then -ZT $p $addr.                      \n" : "" );
    output << ( (          advanced ) ? "         -ZaN $f         - pop first, then -ZN $p $addr.                      \n" : "" );
    output << ( (          advanced ) ? "         -Zauf $p        - like -Zau but with feedback.                       \n" : "" );
    output << ( (          advanced ) ? "         -Zatf $p        - like -Zat but with feedback.                       \n" : "" );
    output << ( (          advanced ) ? "         -Zanf $f        - like -Zan but with feedback.                       \n" : "" );
    output << ( (          advanced ) ? "         -ZaUf $p $addr  - like -ZaU but with feedback.                       \n" : "" );
    output << ( (          advanced ) ? "         -ZaTf $p $addr  - like -ZaT but with feedback.                       \n" : "" );
    output << ( (          advanced ) ? "         -ZaNf $f        - like -ZaN but with feedback.                       \n" : "" );
#endif
    output << ( (          advanced ) ? "         -Za             - abandom input stream and return to previous (pop). \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zcw n $pstring - push string onto shared stream n.                  \n" : "" );
    output << ( (          advanced ) ? "         -ZcW n arg      - push evaluated arg onto shared stream n.           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** When svmheavy is  run, the command  line args **         \n" : "" );
    output << ( (          advanced ) ? "                  ** are placed in an istream  and the buffer gets **         \n" : "" );
    output << ( (          advanced ) ? "                  ** pushed onto the \"input  stack\".  The commands **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -Zw,-Zk,-Zu create new istreams and push them **         \n" : "" );
    output << ( (          advanced ) ? "                  ** onto the same stack.  Input is taken from the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** istream on top of the stack until that stream **         \n" : "" );
    output << ( (          advanced ) ? "                  ** is exhausted,  at which point  it pops it off **         \n" : "" );
    output << ( (          advanced ) ? "                  ** and moves  to the  next one  down.  Once  the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** last  istream is  popped  off  the stack  the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** program will exit.                            **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Feedback is a method  of passing results back **         \n" : "" );
    output << ( (          advanced ) ? "                  ** when  using UDP/TCP  streaming.  If selected, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** some results/output  (eg -echo)  will be sent **         \n" : "" );
    output << ( (          advanced ) ? "                  ** back up  the stream.  These results  can also **         \n" : "" );
    output << ( (          advanced ) ? "                  ** be sent to files or cout.                     **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Shared  streams  (-Zc and  -Zac above)  allow **         \n" : "" );
    output << ( (          advanced ) ? "                  ** commands  to be  passed between  threads.  To **         \n" : "" );
    output << ( (          advanced ) ? "                  ** write a string  to a shared stream  that will **         \n" : "" );
    output << ( (          advanced ) ? "                  ** appear  as  input  to   whichever  thread  is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** accessing this stream (ie has -Zc or -Zac for **         \n" : "" );
    output << ( (          advanced ) ? "                  ** relevant  stream  number)  use -Zcw  (put raw **         \n" : "" );
    output << ( (          advanced ) ? "                  ** string into stream) or  -ZcW (evaluated).  To **         \n" : "" );
    output << ( (          advanced ) ? "                  ** pass variables use either -ZcW (note possible **         \n" : "" );
    output << ( (          advanced ) ? "                  ** loss  of  precision)  or   global  variables. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Notes:                                        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - if stream  is empty  then thread  will wait **         \n" : "" );
    output << ( (          advanced ) ? "                  **   indefinitely  for something to  be put into **         \n" : "" );
    output << ( (          advanced ) ? "                  **   it.  Thus  to terminate  a stream  you must **         \n" : "" );
    output << ( (          advanced ) ? "                  **   end the stream with eg.:                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **     -Zcw n \"-Zx -Za\"                          **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - The operations -Zcw and -ZcW are atomic, so **         \n" : "" );
    output << ( (          advanced ) ? "                  **   you  can  have  multiple   threads  passing **         \n" : "" );
    output << ( (          advanced ) ? "                  **   commands (preferably  ending with -Zx) to a **         \n" : "" );
    output << ( (          advanced ) ? "                  **   single stream.                              **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zf  $file      - create an empty file of given name.                \n" : "" );
    output << ( (          advanced ) ? "         -Zp  $file      - pause until $file exists.                          \n" : "" );
    output << ( (          advanced ) ? "         -Zff $file v    - create file and write v to it.                     \n" : "" );
    output << ( (          advanced ) ? "         -Zpp $file n    - pause until $file exists and read var(0,n) to it.  \n" : "" );
#ifdef ENABLE_THREADS
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Background  training  will  optimise  the  ML **         \n" : "" );
    output << ( (          advanced ) ? "                  ** while waiting for input  from the user.  Note **         \n" : "" );
    output << ( (          advanced ) ? "                  ** that  there is  no guarantee  that the  ML is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** optimal  at  any given  time when  background **         \n" : "" );
    output << ( (          advanced ) ? "                  ** training is enabled - use fnA(h,14) to **         \n" : "" );
    output << ( (          advanced ) ? "                  ** see this information at any time.             **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zob            - turn background training off (default).            \n" : "" );
    output << ( (          advanced ) ? "         -ZoB            - turn background training on (stop on interupt).    \n" : "" );
    output << ( (          advanced ) ? "         -ZoBB           - turn background training on (train to completion). \n" : "" );
#endif
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZZ             - End command sequence now, even if stream nonempty. \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZZZZ           - Exit now.                                          \n" : "" );
    output << ( (          advanced ) ? "         -ZZif t         - Run -ZZ if t evaluates true (non-zero integer).    \n" : "" );
    output << ( (          advanced ) ? "         -ZZZZif t       - Run -ZZZZ if t evaluates true (non-zero integer).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Shortcuts:                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. -Zx can be replaced by ;                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. -ZZ can be replaced by end.                **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 3. -ZZZZ can be replaced by exit.             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 4. -ZZif can be replaced by endif.            **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 5. -ZZZZif can be replaced by exitif.         **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Training examples                                                             \n" : "" );
    output << ( ( basic || advanced ) ? "=================                                                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Example 1: svmheavyv6 -c 1 -kt 3 -kg 20 -tc 5 -AA tr1s.txt                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "SV  classifier with  C/N = 1  (-c 1),  using  a gaussian  RBF kernel  function\n" : "" );
    output << ( ( basic || advanced ) ? "(-kt 2) with gamma = 20  (-kg 20).  Validation carried  out using 5-fold cross\n" : "" );
    output << ( ( basic || advanced ) ? "validation  (-tc 5).  Training data  is contained in  the file  tr1s.txt.  The\n" : "" );
    output << ( ( basic || advanced ) ? "model file will be saved to tr1s.txt.svm and the logfile to tr1s.txt.log.     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Eg 2: svmheavyv6 -c 1 -kt 3 -kg 20 -fo 1 tr1s.txt -ANr 0 \"idiv(2*var(0,1),3)\" \n" : "" );
    output << ( (          advanced ) ? "      -1 1 -tfi 1                                                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Similar to  example 1, except  that in this  case the training  file is opened\n" : "" );
    output << ( (          advanced ) ? "using the -fo  flag first.  2/3 are  then randomly selected  from it using the\n" : "" );
    output << ( (          advanced ) ? "-ANr flag and the equation idiv(2*var(0,1),3) (the quotes may not be needed on\n" : "" );
    output << ( (          advanced ) ? "some OSes).  The remaining 1/3 is used for testing via the -tfi flag.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Eg 3: svmheavyv6 -c 1 -kt 3 -kg 20 -fo 1 tr1s.txt -ANr 0 \"idiv(2*var(0,1),3)\" \n" : "" );
    output << ( (          advanced ) ? "      -1 1 -g 2 \"-c var(0,1) -kg var(0,2) -tc 5\" \"-c var(0,1) -kg var(0,2)\"   \n" : "" );
    output << ( (          advanced ) ? "       fl 1 1e-1 1e1 5 fl 2 1 1e2 5 -tfi 1                                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Building on  example 2, this incorporates  a grid-search to select  C and g to\n" : "" );
    output << ( (          advanced ) ? "minimise 5-fold cross-validation  error, with C ranging 0.1 to 10 over 5 steps\n" : "" );
    output << ( (          advanced ) ? "and g ranging from 1 to 100 over 5 steps (log in both cases).                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Eg 4: svmheavyv6 -c 1 -kt 3 -kg 20 -tc 5 -AA tr1s.txt -Zx -L trx -c 2 -tc 5   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Similar to  example 1, but  with an  additional  block.  The  additional block\n" : "" );
    output << ( (          advanced ) ? "(ie. after the -Zx flag)  sets C/N = 2 (-c 2).  The model  file for the second\n" : "" );
    output << ( (          advanced ) ? "block will be  written to trx.svm  (-L trx), and  the logfile to  trx.log.  As\n" : "" );
    output << ( (          advanced ) ? "for the  first block,  validation of  the second  block is  done using  5-fold\n" : "" );
    output << ( (          advanced ) ? "cross-validation.                                                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );

    return;
}

void printhelpkernel(std::ostream &output, int basic, int advanced)
{
    output << ( ( basic || advanced ) ? "List of available kernel functions:                                           \n" : "" );
    output << ( ( basic || advanced ) ? "===================================                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Numbering: 0-99    are intended for ML use (default 2).                       \n" : "" );
    output << ( ( basic || advanced ) ? "           100-299 are intended for NN use (default 201).                     \n" : "" );
    output << ( ( basic || advanced ) ? "           300-399 are intended for kNN use (default 300).                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "0   = Constant kernel: K(x,y) = r1                                            \n" : "" );
    output << ( ( basic || advanced ) ? "1   = Linear kernel: K(x,y) = <x,y>/(r0.r0)                                   \n" : "" );
    output << ( ( basic || advanced ) ? "2   = Polynomial kernel: K(x,y) = ( r1 + <x,y>/(r0.r0) )^i0                   \n" : "" );
    output << ( ( basic || advanced ) ? "3   = Gaussian kernel: K(x,y) = exp(-||x-y||^2/(2*r0*r0))                     \n" : "" );
    output << ( ( basic || advanced ) ? "4   = Laplacian kernel: K(x,y) = exp(-||x-y||/r0)                             \n" : "" );
    output << ( ( basic || advanced ) ? "5   = Polynoise kernel: K(x,y) = exp(-||x-y||^r1/(r1*r0^r1))                  \n" : "" );
    output << ( ( basic || advanced ) ? "6   = ANOVA kernel: K(x,y) = sum_k (-r4*((xk/r0)^r1-(yk/r0)^r1)^r2)^r3        \n" : "" );
    output << ( ( basic || advanced ) ? "7   = Sigmoid kernel (CPD): K(x,y) = tanh( <x,y>/(r0.r0) + r1 )               \n" : "" );
    output << ( ( basic || advanced ) ? "8   = Rational quadratic kernel: K(x,y) = 1-(||x-y||/r0)^2/((||x-y||/r0)^2+r1)\n" : "" );
    output << ( ( basic || advanced ) ? "9   = Multiquadric kernel (NM): K(x,y) = sqrt(||x-y||^2/(r0.r0)+r1^2)         \n" : "" );
    output << ( ( basic || advanced ) ? "10  = Inverse multiquadric kernel: K(x,y) = 1/sqrt(||x-y||^2/(r0.r0)+r1^2)    \n" : "" );
    output << ( ( basic || advanced ) ? "11  = Circular kernel (MR2): K(x,y) = 2/pi * ( arccos(-||x-y||/r0)            \n" : "" );
    output << ( ( basic || advanced ) ? "                                         - ||x-y||*sqrt(1-||x-y||^2/r0^2)/r0 )\n" : "" );
    output << ( ( basic || advanced ) ? "12  = Spherical kernel (MR3): K(x,y) = 1 - 1.5*||x-y||/r0 + 0.5*||x-y||^3/r0^3\n" : "" );
    output << ( ( basic || advanced ) ? "13  = Wave kernel: K(x,y) = (r0/||x-y||).sin(||x-y||/r0)                      \n" : "" );
    output << ( ( basic || advanced ) ? "14  = Power kernel: K(x,y) = -(||x-y||/r0)^r1                                 \n" : "" );
    output << ( ( basic || advanced ) ? "15  = Log kernel (CPD): K(x,y) = -log((||x-y||/r0)^r1 + 1)                    \n" : "" );
    output << ( ( basic || advanced ) ? "16  = Spline kernel: prod_k ( 1 + x_k.y_k + x_k.y_k.min(x_k,y_k) ...          \n" : "" );
    output << ( ( basic || advanced ) ? "                        - ((x_k+y_k).min(x_k,y_k)^2)/2 + (min(x_k,y_k)^3)/3 ) \n" : "" );
    output << ( ( basic || advanced ) ? "17  = B-Spline kernel: sum_k B_(2i0+1)(x_k-y_k)                               \n" : "" );
    output << ( ( basic || advanced ) ? "19  = Cauchy kernel: K(x,y) = 1/(1+((||x-y||^2/(r0.r0))))                     \n" : "" );
    output << ( ( basic || advanced ) ? "20  = Chi-Square kernel: K(x,y) = 1 - sum_k (2.xk.yk/(r0.r0))/(xk/r0+yk/r0)   \n" : "" );
    output << ( ( basic || advanced ) ? "21  = Histogram kernel: K(x,y) = sum_k min(xk,yk)                             \n" : "" );
    output << ( ( basic || advanced ) ? "22  = Generalised histogram kernel: K(x,y) = sum_k min(|xk|^r0,|yk|^r1)       \n" : "" );
    output << ( ( basic || advanced ) ? "23  = Generalised T-Student kernel: K(x,y) = 1/(1+||x-y||^r0)                 \n" : "" );
    output << ( ( basic || advanced ) ? "24  = Vovk's real polynomial: K(x,y)= (1-((<x,y>/(r0^2))^i0))/(1-<x,y>/(r0^2))\n" : "" );
    output << ( ( basic || advanced ) ? "25  = Weak fourier kernel: K(x,y) = pi.cosh(pi-(||x-y||/r0))                  \n" : "" );
    output << ( ( basic || advanced ) ? "26  = Thin spline (1): K(x,y) = ((||x-y||/r0)^(r1+0.5))                       \n" : "" );
    output << ( ( basic || advanced ) ? "27  = Thin spline (2): K(x,y) = ((||x-y||/r0)^r1)*ln(sqrt(||x-y||/r0))        \n" : "" );
    output << ( ( basic || advanced ) ? "28  = Generic kernel (CPD): K(x,y) = r10(varxy)                               \n" : "" );
    output << ( ( basic || advanced ) ? "29  = Arc-cosine kernel: K(x,y) = 1/pi ||x||^i0 ||y||^i0 J_i0(theta)          \n" : "" );
    output << ( ( basic || advanced ) ? "      where: theta = <x,y>/(||x||.||y||)                                      \n" : "" );
    output << ( ( basic || advanced ) ? "             Jn(t) = sin^(2n+1) (t) (-1/sin(t) d/dt)^n ((pi-t)/sin(t))        \n" : "" );
    output << ( ( basic || advanced ) ? "30  = Chaotic logistic kernel: K(x,y) = <phi_r0^i0(x),phi_r0^i0(y)>           \n" : "" );
    output << ( ( basic || advanced ) ? "      where: - phi_r0^i0(x) = phi_r0^{i0-1}(phi_r0^1(x))                      \n" : "" );
    output << ( ( basic || advanced ) ? "             - phi_r0^1 (x) = [ r0.x_0.(2-x_0) r0.x_1.(1-x_1) ... ]           \n" : "" );
    output << ( ( basic || advanced ) ? "             - phi_r0^0 (x) = x                                               \n" : "" );
    output << ( ( basic || advanced ) ? "             - x is assumed to be ranged from 0 to 1 for each element         \n" : "" );
    output << ( ( basic || advanced ) ? "             - x is actually pre-scaled to xi+r1/(1+2.r1) for each element    \n" : "" );
    output << ( ( basic || advanced ) ? "               where r1 is assumed small positive real.                       \n" : "" );
    output << ( ( basic || advanced ) ? "             - r0 should range from 0 to 1, where the chaotic threshold       \n" : "" );
    output << ( ( basic || advanced ) ? "               1.784975 and above (approx).  Be default, r0 = 1.8             \n" : "" );
    output << ( ( basic || advanced ) ? "31  = Summed Chaotic logistic kernel: K(x,y) = sum_i=0,i0 K30(i0=i;x,y)       \n" : "" );
    output << ( ( basic || advanced ) ? "32  = Diagonal offset kernel: r1 if diagonal Hessian, 0 otherwise             \n" : "" );
    output << ( ( basic || advanced ) ? "33  = Uniform kernel: K(x,y) = 1/2r0 if |||x-y||| < r0, 0 otherwise           \n" : "" );
    output << ( ( basic || advanced ) ? "34  = Triang kernel: K(x,y) = 1/r0 (1-|||x-y|||/r0) if |||x-y||| < r0, 0 other\n" : "" );
    output << ( ( basic || advanced ) ? "36  = Weiner kernel: prod_i min(x_i/r0,y_i/r0)                                \n" : "" );
    output << ( ( basic || advanced ) ? "37  = Half-Integer Matern kernel: of degree nu = i0+1/2, lengthscale r0       \n" : "" );
    output << ( ( basic || advanced ) ? "38  = 1/2-Matern kernel: K(x,y) = exp(-||x-y||/r0)                            \n" : "" );
    output << ( ( basic || advanced ) ? "39  = 3/2-Matern kernel: K(x,y) = (1+ sq(3)*||x-y||/r0).exp(-sq(3)*||x-y||/r0)\n" : "" );
    output << ( ( basic || advanced ) ? "40  = 5/2-Matern kernel: K = (1 + sqrt(5)*||x-y||/r0 + 5*||x-y||^2/r0^2).     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                      exp(-sqrt(5)*||x-y||/r0)\n" : "" );
    output << ( ( basic || advanced ) ? "41  = RBF rescale kernel: K(x,y) = z^(1/(2*r0*r0)) = exp(log(z)/(2*r0*r0))    \n" : "" );
    output << ( ( basic || advanced ) ? "42  = Inverse gudermannian kernel: K(x,y) = igd(<x,y>/(r0.r0))                \n" : "" );
    output << ( ( basic || advanced ) ? "43  = Log ratio kernel: K(x,y) = log((1+<x,y>/(r0.r0))/(1-<x,y>/(r0.r0)))     \n" : "" );
    output << ( ( basic || advanced ) ? "44  = Exponential kernel: K(x,y) = exp(<x,y>/(r0.r0))                         \n" : "" );
    output << ( ( basic || advanced ) ? "45  = Hyperbolic sine kernel: K(x,y) = sinh(<x,y>/(r0.r0))                    \n" : "" );
    output << ( ( basic || advanced ) ? "46  = Hyperbolic cosine kernel: K(x,y) = cosh(<x,y>/(r0.r0))                  \n" : "" );
    output << ( ( basic || advanced ) ? "47  = Sinc kernel: K(x,y) = sinc(||x-y||/r0).cos(2*pi*||x-y||/(r0.r1))        \n" : "" );
    output << ( ( basic || advanced ) ? "48  = LUT kernel: K(x,y) = r1((int) x, (int) y) if r1 is a matrix.            \n" : "" );
    output << ( ( basic || advanced ) ? "                         = r1 if (int) x != (int) y and r1 not a matrix.      \n" : "" );
    output << ( ( basic || advanced ) ? "                         = 1  if (int) x == (int) y and r1 not a matrix.      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "100 = Linear 0/1 neuron:    K(z) = z                                          \n" : "" );
    output << ( ( basic || advanced ) ? "101 = Logistic 0/1 neuron:  K(z) = 1/(1+exp(-r0.z))                           \n" : "" );
    output << ( ( basic || advanced ) ? "102 = Gen. logistic 0/1:    K(z) = 1/(1+r1.exp(-r0.r2.(z-r3)))^(1/r2)         \n" : "" );
    output << ( ( basic || advanced ) ? "103 = Heavyside 0/1 neuron: K(z) = 1 if real(z) > 0, 0 otherwise              \n" : "" );
    output << ( ( basic || advanced ) ? "104 = Rectifier 0/1 neuron: K(z) = z if real(z) > 0, 0 otherwise              \n" : "" );
    output << ( ( basic || advanced ) ? "105 = Softplus 0/1 neuron:  K(z) = ln(r1+exp(r0.z))                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "200 = Linear -1/+1 neuron:    K(z) = z-1                                      \n" : "" );
    output << ( ( basic || advanced ) ? "201 = Logistic -1/+1 neuron:  K(z) = 2/(1+exp(-r0.z)) -1                      \n" : "" );
    output << ( ( basic || advanced ) ? "202 = Gen. logistic -1/+1:    K(z) = 2/(1+r1.exp(-r0.r2.(z-r3)))^(1/r2) - 1   \n" : "" );
    output << ( ( basic || advanced ) ? "203 = Heavyside -1/+1 neuron: K(z) = 1   if real(z) > 0, -1 otherwise         \n" : "" );
    output << ( ( basic || advanced ) ? "204 = Rectifier -1/+1 neuron: K(z) = z-1 if real(z) > 0, -1 otherwise         \n" : "" );
    output << ( ( basic || advanced ) ? "205 = Softplus -1/+1 neuron:  K(z) = 2.ln(r1+exp(r0.z)) -1                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "300 = Euclidean distance: K(x,y) = -1/2 ||x-y||_2^2                           \n" : "" );
    output << ( ( basic || advanced ) ? "301 = 1-norm distance:    K(x,y) = -1/2 ||x-y||_1^2                           \n" : "" );
    output << ( ( basic || advanced ) ? "302 = inf-norm distance:  K(x,y) = -1/2 ||x-y||_inf^2                         \n" : "" );
    output << ( ( basic || advanced ) ? "303 = 0-norm distance:    K(x,y) = -1/2 ||x-y||_0^2                           \n" : "" );
    output << ( ( basic || advanced ) ? "304 = p-norm distance:    K(x,y) = -1/2 ||x-y||_r0^2                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "400 = Logistic (NM):       prod_k 1/(1+exp(-r0*(x_k-y_k)))                    \n" : "" );
    output << ( ( basic || advanced ) ? "401 = Error function (NM): prod_k (1+erf(r0*(x_k-y_k))/2                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "500 = Logist intg: prod_k r0.exp(-r0*(x_k-y_k))/((1+exp(-r0*(x_k-y_k)))^2)    \n" : "" );
    output << ( ( basic || advanced ) ? "501 = Error integ: (1/r0/sqrt(pi))^k exp(-r0*||x-y||^2)                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "600 = Logistic (NM):       prod_k 2/(1+exp(-r0*(x_k-y_k))) -1                 \n" : "" );
    output << ( ( basic || advanced ) ? "601 = Error function (NM): prod_k erf(r0*(x_k-y_k))                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "700 = Logist intg: pd_k 2.r0.exp(-r0*(x_k-y_k))/((1+exp(-r0*(x_k-y_k)))^2)    \n" : "" );
    output << ( ( basic || advanced ) ? "701 = Error integ: (2/r0/sqrt(pi))^k exp(-r0*||x-y||^2)                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "800 = Kernel transfer                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Key: NM  = non-mercer kernel                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "     MRx = positive definite only in R^x                                      \n" : "" );
    output << ( ( basic || advanced ) ? "     CPD = conditionally positive definite                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Notes: - kernels 300-399 defined so K(x,x) + K(y,y) - 2K(x,y) = ||x-y||_...^2 \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernel 4xx is the dense integral of kernel 5xx                       \n" : "" );
    output << ( ( basic || advanced ) ? "         (ie K4xx = int_x0 int_x1 ... K5xx)                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernel 5xx is the dense derivative of kernel 4xx                     \n" : "" );
    output << ( ( basic || advanced ) ? "         (ie K5xx = d/dx0 d/dx1 ... K4xx)                                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernel 6xx is the dense integral of kernel 7xx                       \n" : "" );
    output << ( ( basic || advanced ) ? "         (ie K6xx = int_x0 int_x1 ... K7xx)                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernel 7xx is the dense derivative of kernel 6xx                     \n" : "" );
    output << ( ( basic || advanced ) ? "         (ie K7xx = d/dx0 d/dx1 ... K6xx)                                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - For kernel 28:                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,0) = m                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,1) = x'y                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,2) = conj(x)'conj(y)                                        \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,3) = ||x-y||^2                                              \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,4) = ||x||^2                                                \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,5) = ||y||^2                                                \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(1,0) = ri (as set, not evaluated, including r10)              \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(2,i) = Ki (evaluated only if var(2,i) explicitly included)    \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(3,.) = x  (evaluated only if var(3,.) explicitly included)    \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(4,.) = y  (evaluated only if var(4,.) explicitly included)    \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernel 28 is slow to evaluate: consider adding kernel to code instead\n" : "" );
    output << ( ( basic || advanced ) ? "       - not all kernels are (conditionally) positive definite (C)PD.         \n" : "" );
    output << ( ( basic || advanced ) ? "       - ' indicates conjugate transpose                                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - ||x||^2   = conj(x)'.x      (not the norm if (hyper-)complex)        \n" : "" );
    output << ( ( basic || advanced ) ? "       - ||x-y||^2 = (x-y').(x-y') = ||x||^2 + ||y||^2 - 2<x,y>               \n" : "" );
    output << ( ( basic || advanced ) ? "       - <x,y> = ( x'y + conj(x)'conj(y) )/2                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - for the m-norm (m>=4):                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                  <x,y>     -> <<xa,xb,xc,...>>_m                             \n" : "" );
    output << ( ( basic || advanced ) ? "                  ||x-y||^2 -> <<xa,xa,xa,...>>_m + <<xb,xb,xb,...>>_m +      \n" : "" );
    output << ( ( basic || advanced ) ? "                             + <<xc,xc,xc,...>>_m + ... - m.<<xa,xb,xc,...>>_m\n" : "" );
    output << ( ( basic || advanced ) ? "                  ||x||^2   -> <<xa,xa,xa,xa>>_m                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  ||y||^2   -> <<xb,xb,xb,xb>>_m if m == 4                    \n" : "" );
    output << ( ( basic || advanced ) ? "                               <<xa,xa,xa,xa>>_m if m == 6,8,...              \n" : "" );
    output << ( ( basic || advanced ) ? "       - m-norms and m-inner products do not mix well with (hyper-) complex   \n" : "" );
    output << ( ( basic || advanced ) ? "         values.  No conjugation is done for m-norms and m-inner products, so \n" : "" );
    output << ( ( basic || advanced ) ? "         results are unlikely to satisfy any relevant extensions in the       \n" : "" );
    output << ( ( basic || advanced ) ? "         m-norm (hyper-)complex case.                                         \n" : "" );
    output << ( ( basic || advanced ) ? "       - m-norms and m-inner products on symbolic features do procucts        \n" : "" );
    output << ( ( basic || advanced ) ? "         pairwise, so eg \"a\".\"a\".\"b\".\"b\" = (\"a\".\"a\").(\"b\".\"b\") = 1.           \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernels 6,16,17,20,21,22 actually work directly on x,y rather than   \n" : "" );
    output << ( ( basic || advanced ) ? "         via the norms and inner products and so no extension to m-norms and  \n" : "" );
    output << ( ( basic || advanced ) ? "         m-inner products is defined in this case.  Also note that the inv,   \n" : "" );
    output << ( ( basic || advanced ) ? "         max, min etc of symbolic features is ill-defined.                    \n" : "" );
    output << ( ( basic || advanced ) ? "       - the arc-cosine kernel for i0 >= 3 is calculated on the fly - v. slow.\n" : "" );
    output << ( ( basic || advanced ) ? "       - for neural kernels, z=<x,y>                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );

    return;
}

void printhelpvars(std::ostream &output, int basic, int advanced)
{
    output << ( ( basic || advanced ) ? "Available constants                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "===================                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The following vars may be used in variable evaluation (note that other vars   \n" : "" );
    output << ( ( basic || advanced ) ? "may also be defined, but these are typically transient and unreliable):       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "var(0,n)   = if set by -fo or -foe, this is the number of vectors remaining in\n" : "" );
    output << ( ( basic || advanced ) ? "             that file.  May also be set directly by -fV.                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,0)   = used during grid-search to store the distance from the optimal   \n" : "" );
    output << ( ( basic || advanced ) ? "             gridpoint to the grid centre.                                    \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,1)   = the result found by the most recent performance estimation -t....\n" : "" );
    output << ( ( basic || advanced ) ? "var(1,2)   = the unprocessed result of the most recent -t...                  \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,3)   = the unprocessed count vector of the most recent -t...            \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,4)   = the unprocessed confusion matrix of the most recent -t...        \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,5)   = result of most recent -t... in terms of h(g(x)).                 \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,6)   = result of most recent -t... in terms of g(x).                    \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,8)   = result of most recent -hU in terms of h(g(x)).                   \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,9)   = result of most recent -hU in terms of g(x).                      \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,11)  = filename of most recently added data.                            \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,12)  = log filename.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,13)  = verbosity level.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,14)  = name of ML file if loaded.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,15)  = name of alpha file if loaded.                                    \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,16)  = name of bias file if loaded.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,17)  = name of ML file if saved.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,18)  = name of alpha file if saved.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,19)  = name of bias file if saved.                                      \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,37)  = Accuracy of most recent performance estimation -t....            \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,38)  = Precision recorded by most recent performance estimation -t....  \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,39)  = Recall recorded by most recent performance estimation -t....     \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,40)  = F1 score recorded by most recent performance estimation -t....   \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,41)  = AUC recorded by most recent performance estimation -t....        \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,42)  = Sparsity recorded by most recent performance estimation -t....   \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,44)  = the unprocessed classwise error vector of the most recent -t...  \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,46)  = variance of var(1,37) if most recent was repeated cross-fold.    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "var(0,1000)= counter for -Zrep (counts 0,1,...).                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "var(42,42) = current ML index.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  ** var(1,0)-var(1,34) only defined if requested  **         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "var(130,0) = long and complicated macro used by -M0                           \n" : "" );
    output << ( ( basic || advanced ) ? "var(130,1) = long and complicated macro used by -M1                           \n" : "" );
    output << ( ( basic || advanced ) ? "var(130,2) = long and complicated macro used by -M2                           \n" : "" );
    output << ( ( basic || advanced ) ? "var(130,3) = long and complicated macro used by -M3                           \n" : "" );
    output << ( ( basic || advanced ) ? "var(130,4) = long and complicated macro used by -M4                           \n" : "" );
    output << ( ( basic || advanced ) ? "var(130,5) = long and complicated macro used by -M5                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Global functions                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "================                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "In the  following we  use h (h = var(0,2) >= 0)  to access the  current ML. To\n" : "" );
    output << ( ( basic || advanced ) ? "access a  different ML replace  h with the  relevant  index.  Derivatives  are\n" : "" );
    output << ( ( basic || advanced ) ? "available as:                                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "dfnB(h,k,a,i)     = d^i/da^i fnB(h,k,a)                                       \n" : "" );
    output << ( ( basic || advanced ) ? "dfnC(h,k,a,i,b,j) = d^i/da^i d^j/db^j fnB(h,k,a,b)                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "See code for more information about obscure details.                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,0)       = C                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1)       = epsilon                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2)       = sigma (1/C)                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,3)       = beta rank (-mvb)                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,4)       = output (target) space dimension.                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5)       = output (target) order.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6)       = solution sparsity.                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,7)       = input space dimension                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8)       = is target sparse?                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9)       = is input sparse?                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,10)      = N                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,11)      = ML type                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,12)      = ML subtype                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,13)      = number of classes                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,14)      = is ML trained?                                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,15)      = is ML mutable?                                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,16)      = is ML pool?                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,17)      = is ML target treated as scalar?                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,18)      = is ML target treated as vector?                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,19)      = is ML target treated as anion?                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,20)      = is ML classifier?                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,21)      = is ML regressor?                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,22)      = number of (internal) classes.                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,23)      = number of basis vectors.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,24)      = basis type.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,25)      = defProj().                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,26)      = g(x) output type.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,27)      = h(x) output type.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,28)      = target type.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,29)      = class labels (vector).                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,30)      = zero tolerance.                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,31)      = optimality tolerance.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,32)      = max training time.                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,33)      = mv-rank learning rate.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,34)      = mv-rank zero tolerance.                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,35)      = kernel cache max size.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,36)      = max iteration count.                                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,37)      = mv-rank max iteration count.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,38)      = y vector.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,39)      = d vector.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,40)      = C weight vector.                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,41)      = epsilon weight vector.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,42)      = alpha state vector.                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,43)      = C weight fuzzy vector.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,44)      = sigma weight vector.                                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,45)      = target basis.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,46)      = index key.                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,47)      = index key count.                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,48)      = data type key.                                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,49)      = data type key breakdown.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,50)      = x (training vector set) sum.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,51)      = x (training vector set) mean.                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,52)      = x (training vector set) mean squared.                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,53)      = x (training vector set) squared sum.                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,54)      = x (training vector set) squared mean.                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,55)      = x (training vector set) median.                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,56)      = x (training vector set) variance.                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,57)      = x (training vector set) standard deviation.                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,58)      = x (training vector set) inverse standard deviation.          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,59)      = x (training vector set) max.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,60)      = x (training vector set) min.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,100,i)   = C weight for class i.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,101,i)   = epsilon weight for class i.                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,102,i)   = d(i) (non-zero if this training vector enabled).             \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,103,i)   = number of unconstrained vectors in class i.                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,104,i)   = d(i) (non-zero if this training vector enabled).             \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,105,i)   = label associated with (internal) class i.                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,106,i)   = training vector i (in vector/set representation).            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,200,a,b) = distance between a and b, assuming a = g(xa), b = g(xb).     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,300,i)   = g(xi).                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,301,i)   = h(xi).                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,302,i)   = var(xi).                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,303,i)   = dg(xi)/dxi.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,304,i)   = dg(xi)/dxi weights wrt training set vectors.                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,305,i)   = dg(xi)/dxi weights wrt xi.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,306,i)   = s(xi) (stability probability).                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,400,i,j) = cov(xi,xj).                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,401,i,j) = K2(xi,xj).                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,402,i,j) = <xi,xj>.                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,403,i,j) = ||xi-xj||_K2.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,499)     = K in equational form.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,500,x)   = g(x).#                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,501,x)   = h(x).#                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,502,x)   = var(x).#                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,503,x)   = dg(x)/dx.#                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,504,x)   = dg(x)/dx weights wrt training set vectors.#                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,505,x)   = dg(x)/dx weights wrt x.#                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,506,x)   = s(x) (stability probability).#                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,600,x,z) = cov(x,z).#                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,601,x,z) = K2(x,z).#                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,602,x,z) = <x,z>.#                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,603,x,z) = ||x-z||_K2.#                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,701,m,x) = Km(x).##                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8xx), fnB(h,8xx), fnC(h,8xx) all refer to the kernel K, function xx.    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9xx), fnB(h,9xx), fnC(h,9xx) like 7xx, but for the output kernel.       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "xx = 0,...,30: see mercer.cc MercerKernel::getparam function.                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x50,8,i) = weight of kernel function i.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x51,8,i) = type of kernel function i.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x52,8,i) = is kernel function i normalised?                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x53,8,i) = is kernel function i un-normalised?                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x54,8,i) = is kernel function i chained?                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x55,8,i) = is kernel function i un-chained?                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x56,8,i) = vector of real constants for kernel function i.              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x57,8,i) = vector of integer constants for kernel function i.           \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x60,8,i) = first real constant for kernel function i.                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x61,8,i) = first integer constant for kernel function i.                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Notes: # x (and z) must be in non-sparse vector/set format. If x is a set then\n" : "" );
    output << ( ( basic || advanced ) ? "         it is converted as { s0 s1 s2 ... } -> [ s0 : s1 :: s2 ::: ... ].  If\n" : "" );
    output << ( ( basic || advanced ) ? "         NULLs are included then these  components are skipped, so for example\n" : "" );
    output << ( ( basic || advanced ) ? "         {  [ 1 2 3 ] null  null  [ null  null null  null null  null 3 ]  } ->\n" : "" );
    output << ( ( basic || advanced ) ? "         [ 1 2 3 ::: 6:3 ] (that is, vector [ 1 2 3 ], want 3rd derivative).  \n" : "" );
    output << ( ( basic || advanced ) ? "      ## For Km evaluation, x is a vector of m vectors (see # for format).    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9000)    = NZ                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9001)    = NF                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9002)    = NS                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9003)    = NC                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9004)    = NLB                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9005)    = NLF                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9006)    = NUF                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9007)    = NUB                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9008)    = isLinearCost                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9009)    = isQuadraticCost                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9010)    = is1NormCost                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9011)    = isVarBias                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9012)    = isPosBias                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9013)    = isNegBias                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9014)    = isFixedBias                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9015)    = isOptActive                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9016)    = isOptSMO                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9017)    = isOptD2C                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9018)    = isOptGrad                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9019)    = isFixedTube                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9020)    = isShrinkTube                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9021)    = isRestrictEpsPos                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9022)    = isRestrictEpsNeg                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9023)    = isClassifyViaSVR                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9024)    = isClassifyViaSVM                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9025)    = is1vsA                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9026)    = is1vs1                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9027)    = isDAGSVM                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9028)    = isMOC                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9029)    = ismaxwins                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9030)    = isrecdiv                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9031)    = isatonce                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9032)    = isredbin                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9033)    = isKreal                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9034)    = isKunreal                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9035)    = isanomalyOn                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9036)    = isanomalyOff                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9037)    = isautosetOff                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9038)    = isautosetCscaled                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9039)    = isautosetCKmean                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9040)    = isautosetCKmedian                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9041)    = isautosetCNKmean                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9042)    = isautosetCNKmedian                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9043)    = isautosetLinBiasForce                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9044)    = outerlr                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9045)    = outermom                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9046)    = outermethod                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9047)    = outertol                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9048)    = outerovsc                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9049)    = outermaxitcnt                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9050)    = outermaxcache                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9051)    = maxiterfuzzt                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9052)    = usefuzzt                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9053)    = lrfuzzt                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9054)    = ztfuzzt                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9055)    = costfnfuzzt                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9056)    = m                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9057)    = LinBiasForce                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9058)    = QuadBiasForce                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9059)    = nu                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9060)    = nuQuad                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9061)    = theta                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9062)    = simnorm                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9063)    = anomalyNu                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9064)    = anomalyClass                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9065)    = autosetCval                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9066)    = autosetnuval                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9067)    = anomclass                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9068)    = singmethod                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9069)    = rejectThreshold                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9070)    = Gp                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9071)    = XX                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9072)    = kerndiag                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9073)    = bias                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9074)    = alpha                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9075)    = quasiloglikelihood                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9076)    = weight vector for linear SVM.#                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9077)    = bias for linear SVM.                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9100,i)  = NF(i)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9101,i)  = NZ(i)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9102,i)  = NS(i)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9103,i)  = NC(i)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9104,i)  = NLB(i)                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9105,i)  = NLF(i)                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9106,i)  = NUF(i)                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9107,i)  = NUB(i)                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9108,i)  = ClassRep()(i)                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9109,i)  = findID(i)                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9110,i)  = getu()(i)                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9111,i)  = isVarBias(i)                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9112,i)  = isPosBias(i)                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9113,i)  = isNegBias(i)                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9114,i)  = isFixedBias(i)                                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9115,i)  = LinBiasForce(i)                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9116,i)  = QuadBiasForce(i)                                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,9200,i,j)= Gp(i,j)                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,9201,i,j)= XX(i,j)                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- LSV specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5000)    = isVardelta                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5001)    = isZerodelta                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5002)    = gamma                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5003)    = delta                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5004)    = LSV (quasi) log-likelihood.                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SSV specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8000)    = Nzx                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8001)    = beta                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8002)    = b                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8003)    = zmin                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8004)    = zmax                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8005)    = x state                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8006)    = xact?                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8007)    = M                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8008)    = n                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8009)    = quadratic regularised?                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8010)    = linear regularised?                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8011)    = bias force.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8012)    = anomaly class.                                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8013)    = ssv learning rate.                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8014)    = ssv momentum.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8015)    = ssv tolerance.                                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8016)    = ssv ovsc.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8017)    = ssv max iteration count.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8018)    = ssv max training time.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,8100,i)  = z(i)                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- GPR specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2000)    = mu weight.                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2001)    = mu bias.                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2002)    = isZeromubias.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2003)    = isVarmubias.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2004)    = isSampleMode.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- MLM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6000)    = tsize.                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6001)    = knum.                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6004)    = mlmlr.                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6005)    = diffstop.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6006)    = lsparse.                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,6100,i)  = regtype(i).                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,6101,i)  = regC(i).                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,6102,i)  = GGp(i).                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- KNN specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,4000)    = k.                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,4001)    = ktp.                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- ONN specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,7000)    = lr.                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,7001)    = W.                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,7004)    = B.                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- IMP specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,3000)    = zref.                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,3001)    = EHI method.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,3002)    = needdg.                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,3003)    = hypervolume dominated.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,3100,x)  = imp(x).#                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- BLK specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1000)    = outfn.                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1001)    = outfngrad.                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1002)    = mex call.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1003)    = mex call id.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1004)    = mercer cache size.                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1005)    = mercer cache norm.                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1006)    = system call.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1007)    = Bernstein order.                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1008)    = Bernstein index.                                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Global test functions                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "=====================                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "To access  test functions (as  per -fu  and -fuu)  we use fnB/C  with negative\n" : "" );
    output << ( ( basic || advanced ) ? "indexes:                                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(-1,i,x)    = evaluate fi(x) where fi is test function i (see -fu).        \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(-2,i,x,A)  = like fnB(-1,i,x) but with A matrix A (see -fuu).             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(-3,i,x,M)  = evaluate fi(x) for multi-objective test fn i (see -ft).      \n" : "" );

    return;
}


void printhelpgentype(std::ostream &output, int basic, int advanced)
{
    output << ( ( basic || advanced ) ? "Variable types and functions:                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "=============================                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "As well as  simple values  for arguments  you  can  enter equations,  refer to\n" : "" );
    output << ( ( basic || advanced ) ? "global variables and functions etc - for example sqrt(20) will evaluate to the\n" : "" );
    output << ( ( basic || advanced ) ? "square root  of 20, x^2 will  be evaluated  if x  is defined  (and left  as an\n" : "" );
    output << ( ( basic || advanced ) ? "equation otherwise) etc.                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The following types are supported:                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Integers: 0, -1, 42 etc                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Reals: 1.0, 5e-3, 32.3E12 etc                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - Complex: i, 1+3.2i, etc                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Quaternion: I, J, K, -21J etc                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - Octonion: l, m, n, o, p, q, r, 12.3p etc                             \n" : "" );
    output << ( ( basic || advanced ) ? "       - Anion: im_anion(n,i), where 0<=i<2^n.                                \n" : "" );
    output << ( ( basic || advanced ) ? "       - Strings: a,d,'t',\"hello world\"                                       \n" : "" );
    output << ( ( basic || advanced ) ? "       - Vectors: eg. [ 1 -3.2 4I ... ]                                       \n" : "" );
    output << ( ( basic || advanced ) ? "       - Matrices: eg. M:[ 1 2 3 ; 4 5 6 ]                                    \n" : "" );
    output << ( ( basic || advanced ) ? "       - Sets: eg. { -2 14.3 \"chickens\" \"cats\" ... }                          \n" : "" );
    output << ( ( basic || advanced ) ? "       - Directed graphs: G:{ [ n1 n2 .. ] ;  M:[ w11 w12 .. ; w21 w22 .. ] },\n" : "" );
    output << ( ( basic || advanced ) ? "         where n1 n2 ... are nodes, w11 w12 ... are edge weights.             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The following operators are expanded, in order, with given direction (e- means\n" : "" );
    output << ( ( basic || advanced ) ? "elementwise operation), and replaced with the functional form:                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: negation:           -a    -> neg(a)                   \n" : "" );
    output << ( ( basic || advanced ) ? "                        posation (null):    +a    -> pos(a)                   \n" : "" );
    output << ( ( basic || advanced ) ? "                        logical not:        ~a    -> lnot(a)                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - Right to left: power:              a^b   -> pow(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-power:            a.^b  -> epow(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: multiplication:     a*b   -> mul(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        division:           a/b   -> div(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        right division:     a\b   -> rdiv(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "                        modulus:            a%b   -> mod(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-multiplication:   a.*b  -> emul(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-division:         a./b  -> ediv(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-right division:   a.\b  -> erdiv(a,b)               \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-modulus:          a.%b  -> emod(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: addition:           a+b   -> add(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        subtraction:        a-b   -> sub(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: C-D construct:      a|b   -> cayleyDickson(a,b)       \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: equality:           a==b  -> eq(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        inequality:         a~=b  -> ne(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        greater than:       a>b   -> gt(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        not less than:      a>=b  -> ge(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        not greater than:   a<=b  -> le(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        less than:          a<b   -> lt(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-equality:         a.==b -> eeq(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-inequality:       a.~=b -> ene(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-greater than:     a.>b  -> egt(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-not less than:    a.>=b -> ege(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-not greater than: a.<=b -> ele(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-less than:        a.<b  -> elt(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: logical or:         a||b  -> lor(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        logical and:        a&&b  -> land(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "where we note that:                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two vectors is the inner-product of the vectors.      \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two strings is 1 if they are identical, 0 otherwise.  \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two sets is the number of elements in common.         \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two directed graphs A = { An ; Aw }, B = { Bn ; Bw} is\n" : "" );
    output << ( ( basic || advanced ) ? "         A*B = { Cn ; Cw }. where:                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         Cn = [ [An0,Bn0], [An0,Bn1], ..., [An1,Bn0], [An1,Bn1], ... ]        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         Cw = [ Aw00*Bw00  Aw00*Bw01  ...  Aw01*Bw00  Aw01*Bw01  ... ]        \n" : "" );
    output << ( ( basic || advanced ) ? "              [ Aw00*Bw10  Aw00*Bw11  ...  Aw01*Bw10  Aw01*Bw11  ... ]        \n" : "" );
    output << ( ( basic || advanced ) ? "              [  ...        ...             ...        ...           ]        \n" : "" );
    output << ( ( basic || advanced ) ? "              [ Aw10*Bw00  Aw10*Bw01  ...  Aw11*Bw00  Aw11*Bw01  ... ]        \n" : "" );
    output << ( ( basic || advanced ) ? "              [ Aw10*Bw10  Aw10*Bw11  ...  Aw11*Bw10  Aw11*Bw11  ... ]        \n" : "" );
    output << ( ( basic || advanced ) ? "              [  ...        ...             ...        ...           ]        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - The sum of two strings is the concatenation of those strings.        \n" : "" );
    output << ( ( basic || advanced ) ? "       - The sum of two sets is the union, the difference the intersection.   \n" : "" );
    output << ( ( basic || advanced ) ? "       - The sum of two directed graphs is undefined (returns type error).    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - The negation of a string is that string in reverse order.            \n" : "" );
    output << ( ( basic || advanced ) ? "       - The conjugate of a string is that string with reversed case.         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "and the following string expansions are completed:                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - pi -> pi()                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "       - euler -> euler()                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - x -> var(0,0)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - y -> var(0,1)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - z -> var(0,2)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - v -> var(0,3)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - w -> var(0,4)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - g -> var(0,5)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - h -> var(42,42) (this is used to hold the index of the current ML).  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Scalar functions are a special type of  function that behave more like vectors\n" : "" );
    output << ( ( basic || advanced ) ? "and are written @(i,j,n):fn where fn is a function, (i,j) defines the variable\n" : "" );
    output << ( ( basic || advanced ) ? "in the function being treated as a (continuous) index ranging [0,1] and n sets\n" : "" );
    output << ( ( basic || advanced ) ? "the number of  steps used in various approximations.  If i,j are  vectors then\n" : "" );
    output << ( ( basic || advanced ) ? "multiple variables are treated as continuous indexes.                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The result of the product of two scalar functions is the inner product:       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "@(if,jf,n):f*@(ig,jg,n):g = int_0^1 conj(f(var(if,jf)=x)).g(var(ig,jg)=x) dx  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "which is  approximated using  n steps.  The product  of a vector  and a scalar\n" : "" );
    output << ( ( basic || advanced ) ? "function is similar,  where the vector is  treated as a step  function ranging\n" : "" );
    output << ( ( basic || advanced ) ? "over [0,1].  The shorthand @():f assuming i,j = 0 and n = 100.                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The following functions may be used in equations:                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Variables: var(i,j).                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Unary operators: pos(a), neg(a), inv(a).                             \n" : "" );
    output << ( ( basic || advanced ) ? "       - Binary operators: add(a,b),  sub(a,b), mul(a,b),  div(a,b), idiv(a,b)\n" : "" );
    output << ( ( basic || advanced ) ? "         (integer division),  rdiv(a,b) (right division),  mod(a,b), pow(a,b),\n" : "" );
    output << ( ( basic || advanced ) ? "         powl(a,b) (left power), powr(a,b) (right power).  For non-commutative\n" : "" );
    output << ( ( basic || advanced ) ? "         variables  left power  is powl(a,b) =  exp(b*log(a)), right  power is\n" : "" );
    output << ( ( basic || advanced ) ? "         powr(a,b) = exp(log(a)*b), and pow(a,b) = (powl(a,b)+powr(a,b))/2.   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Comparative: eq(a,b), ne(a,b), gt(a,b), ge(a,b), le(a,b), lt(a,b).   \n" : "" );
    output << ( ( basic || advanced ) ? "       - Logical: lnot(a), lor(a,b), land(a,b).                               \n" : "" );
    output << ( ( basic || advanced ) ? "       - Ternary: ifthenelse(a,b,c).                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Type   testing:    isnull(a),   isint(a),   isreal(a),    isanion(a),\n" : "" );
    output << ( ( basic || advanced ) ? "         isvector(a),   ismatrix(a),   isset(a),   isdgraph(a),   isstring(a),\n" : "" );
    output << ( ( basic || advanced ) ? "         iserror(a).                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "       - Type information: size(a), numRows(a), numCols(a).                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Constants:  null(), pi(), euler(),  pinf(), ninf(), vnan(),  eye(i,j)\n" : "" );
    output << ( ( basic || advanced ) ? "         (i*j identity matrix).                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Operators: conj(a),  realDeriv(i,j,a) (derivative of  function a with\n" : "" );
    output << ( ( basic || advanced ) ? "         respect to var(i,j)).                                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Vector constructs:  vect_const(n,x)  (vector of size n  with elements\n" : "" );
    output << ( ( basic || advanced ) ? "         x),  vect_unit(n,i) (vector  of size  n, all  zero except  element i,\n" : "" );
    output << ( ( basic || advanced ) ? "         which is 1), ivect(a,b,c) (equivalent of a:b:c in matlab).           \n" : "" );
    output << ( ( basic || advanced ) ? "       - Anion constructs: im_complex(a)  (1 if a = 0, i if a = 1), im_quat(a)\n" : "" );
    output << ( ( basic || advanced ) ? "         (1 if a = 0, I if a = 1, J if  a = 2, K if a = 3), im_octo(a) (1 if a\n" : "" );
    output << ( ( basic || advanced ) ? "         = 0, l if a = 1, m if a = 2, ..., r if l = 7).                       \n" : "" );
    output << ( ( basic || advanced ) ? "       - Cayley-dickson construction: cayleyDickson(a,b) (a|b).               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Commutators    and    associators:     commutate(a,b)    ((ab-ba)/2),\n" : "" );
    output << ( ( basic || advanced ) ? "         associate(a,b,c)  ((a(bc)-(ab)c)/2), anticommutate(a,b)  ((ab+ba)/2),\n" : "" );
    output << ( ( basic || advanced ) ? "         antiassociate(a,b,c) ((a(bc)+(ab)c)/2).                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Algebraic constants:  eps_comm(n,q,r,s) (commutator  structure const,\n" : "" );
    output << ( ( basic || advanced ) ? "         order n, element q,r,s (0 real, 1,2,...  imag)), eps_assoc(n,q,r,s,t)\n" : "" );
    output << ( ( basic || advanced ) ? "         (associator  structure constant,  order n,  element q,r,s,t  (0 real,\n" : "" );
    output << ( ( basic || advanced ) ? "         1,2,... imag)).                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Combinatorics: kronDelta(a,b), diracDelta(a,b), perm(a,b), comb(a,b),\n" : "" );
    output << ( ( basic || advanced ) ? "         fact(a).                                                             \n" : "" );
    output << ( ( basic || advanced ) ? "       - Elementwise combinatorics: ekronDelta(a,b), ediracDelta(a,b).        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Magnitude and angle: abs1(a)  (||a||_1), abs2(a) (||a||_2), absp(a,p)\n" : "" );
    output << ( ( basic || advanced ) ? "         (||a||_p),  absinf(a)   (||a||_inf),  norm1(a)   (||a||_1),  norm2(a)\n" : "" );
    output << ( ( basic || advanced ) ? "         (||a||_2^2), normp(a) (||a||_p^p), angle(a) (a/||a||_2).             \n" : "" );
    output << ( ( basic || advanced ) ? "       - Real, imaginary,  argument: real(a), imagx(a)  (imaginary part of a),\n" : "" );
    output << ( ( basic || advanced ) ? "         imagd(a)  (unit  imaginary  part   of a,  imagx(a)/||a||_2),  imag(a)\n" : "" );
    output << ( ( basic || advanced ) ? "         (sign-corrected imaginary  magnitude of a),  argx(a) (imagx(log(a))),\n" : "" );
    output << ( ( basic || advanced ) ? "         argd(a) (imagd(log(a))), arg(a) (imag(log(a))).                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - Polar anions:  polarx(a,b)  (a*exp(b)),  polard(a,b,q)  (a*exp(b*q)),\n" : "" );
    output << ( ( basic || advanced ) ? "         polar(a,b,q) (a*exp(b*q)).                                           \n" : "" );
    output << ( ( basic || advanced ) ? "       - Sign: sgn(a).                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Exponentials and similar: sqrt(a),  exp(a), tenup(a)  (10^a), log(a),\n" : "" );
    output << ( ( basic || advanced ) ? "         log10(a),  logb(a,b),  logbl(a,b),    logbr(a,b)  (here  logbl(a,b) =\n" : "" );
    output << ( ( basic || advanced ) ? "         log(a)*inv(log(b)),  logbr(a,b)   =  inv(log(b))*log(a),  logb(a,b) =\n" : "" );
    output << ( ( basic || advanced ) ? "         (logbl(a,b)+logbr(a,b))/2).                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Trigonometric: sin(a), cos(a), tan(a), cosec(a), sec(a), cot(a).     \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse trigonometric: asin(a), acos(a), atan(a), acosec(a), asec(a),\n" : "" );
    output << ( ( basic || advanced ) ? "         acot(a).                                                             \n" : "" );
    output << ( ( basic || advanced ) ? "       - Related: sinc(a), cosc(a), tanc(a).                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - Versed: vers(a), covers(a), hav(a).                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse versed: avers(a), acovers(a), ahav(a).                       \n" : "" );
    output << ( ( basic || advanced ) ? "       - External: excosec(a), exsec(a).                                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse external: aexcosec(a), aexsec(a).                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Hyperbolic: sinh(a), cosh(a), tanh(a), cosech(a), sech(a), coth(a).  \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse   hyperbolic:  asinh(a),   acosh(a),  atanh(a),   acosech(a),\n" : "" );
    output << ( ( basic || advanced ) ? "         asech(a), acoth(a).                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - Related: sinhc(a), coshc(a), tanhc(a).                               \n" : "" );
    output << ( ( basic || advanced ) ? "       - Versed: versh(a), coversh(a), havh(a).                               \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse versed: aversh(a), acovrsh(a), ahavh(a).                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - External: excosech(a), exsech(a).                                    \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse external: aexcosech(a), aexsech(a).                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Miscellaneous:  sigm(a)  (sigmoid),  gd(a)  (gudermannian),  asigm(a)\n" : "" );
    output << ( ( basic || advanced ) ? "         (inverse sigmoid), agd(a) (inverse gudermannian).                    \n" : "" );
    output << ( ( basic || advanced ) ? "       - Distribution functions: normDistr(a), polyDistr(x,n).                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Special  functions:   gamma(a),  lngamma(a)   (log   gamma),   psi(a)\n" : "" );
    output << ( ( basic || advanced ) ? "         (digamma), psi_n(i,a) (polygamma of  degree i), gami(a,x) (incomplete\n" : "" );
    output << ( ( basic || advanced ) ? "         gamma), gamic(a,b) (inverse incomplete gamma function), erf(a) (error\n" : "" );
    output << ( ( basic || advanced ) ? "         function), erfc(a) (complementary  error function), dawson(a) (Dawson\n" : "" );
    output << ( ( basic || advanced ) ? "         function), zeta(a)  (Reimann zeta  function), lambertW(a)  (Lambert's\n" : "" );
    output << ( ( basic || advanced ) ? "         W  function,  main  branch  W0  (W>-1)),  lambertWx(a)  (Lambert's  W\n" : "" );
    output << ( ( basic || advanced ) ? "         function, lower branch W1 (W<-1).                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        - Type conversion: rint(a), ceil(a), floor(a).                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        - Vector  and  matrix:  outerProd(a,b),  fourProd(a,b,c,d),  trans(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          det(a),  trace(a),  miner(a,i,j),  cofactor(a,i,j), adj(a),  max(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          min(a), maxdiag(a), mindiag(a), argmax(a), argmin(a), argmaxdiag(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          argmindiag(a),    allargmax(a),   allargmin(a),    allargmaxdiag(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          allargmindiag(a),      maxabs(a),     minabs(a),      maxabsdiag(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          minabsdiag(a),    argmaxabs(a),   argminabs(a),    argmaxabsdiag(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          argminabsdiag(a),         allargmaxabs(a),          allargminabs(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          allargmaxabsdiag(a), allargminabsdiag(a).                           \n" : "" );
    output << ( ( basic || advanced ) ? "        - Sums, products, means  etc:  sum(a),  prod(a),  mean(a),  median(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          argmedian(a).                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "        - Element reference: deref(a,x),  derefv(a,i) (element i of vector a),\n" : "" );
    output << ( ( basic || advanced ) ? "          derefm(a,i,j) (element i,j of matrix a), derefa(a,i) (anions).      \n" : "" );
    output << ( ( basic || advanced ) ? "        - Compound matrix/vector collapse: collapse(a).                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        - External functions: fnA(i), fnB(i,a), fnC(i,a,b).                   \n" : "" );
    output << ( ( basic || advanced ) ? "        - External function derivatives: dfnB(i,j,a), dfnC(i,j,a,b).          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        - Draws from distributions: irand(i) (uniform random integer [0,i-1]),\n" : "" );
    output << ( ( basic || advanced ) ? "          urand(a,b)  (uniform  random  real on  [a,b]), grand(a,b)  (Gaussian\n" : "" );
    output << ( ( basic || advanced ) ? "          random from N(a,b)).  These include vector-valued draws.            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        - Test functions:  testfn(i,a) (evaluate  test function  i with vector\n" : "" );
    output << ( ( basic || advanced ) ? "          a),  testfnA(i,a,A) (like  before, with matrix  A), partestfn(i,M,a)\n" : "" );
    output << ( ( basic || advanced ) ? "          (multi-objective test  function i, target dimension  M, evaluated on\n" : "" );
    output << ( ( basic || advanced ) ? "          a), partestfnA(i,M,a,alpha) (like before, with constant alpha).     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( ( basic || advanced ) ? "|  i: Function name             | d | Range                  | Minimum       |\n" : "" );
    output << ( ( basic || advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( ( basic || advanced ) ? "|  1: Rastrigin function        | d | -5.12   <= x_i <= 5.12 | f(0)     = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  2: Ackley's function         | d | -5      <= x_i <= 5    | f(0)     = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  3: Sphere function           | d | -inf    <= x_i <= inf  | f(0)     = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  4: Rosenbrock function       | d | -inf    <= x_i <= inf  | f(1)     = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  5: Beale's function          | 2 | -4.5    <= x,y <= 4.5  | f(3,0.5) = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  6: Goldstein-Price function  | 2 | -2      <= x,y <= 2    | f(0,-1)  = 3  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  7: Booth's function          | 2 | -10     <= x,y <= 10   | f(1,3)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  8: Bukin function N.6        | 2 | -15,-3  <= x,y <= -5,3 | f(-10,1) = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  9: Matyas function           | 2 | -10     <= x,y <= 10   | f(0,0)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 10: Levi function N.13        | 2 | -10     <= x,y <= 10   | f(1,1)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 11: Himmelblau's function     | 2 | -5      <= x,y <= 5    | f(s,t)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 12: Three-hump camel function | 2 | -5      <= x,y <= 5    | f(0,0)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 13: Easom function            | 2 | -100    <= x,y <= 100  | f(pi,pi) = -1 |\n" : "" );
    output << ( ( basic || advanced ) ? "| 14: Cross-in-tray function    | 2 | -10     <= x,y <= 10   | f(a,a)   = b  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 15: Eggholder function        | 2 | -512    <= x,y <= 512  | f(c,d)   = e  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 16: Holder table function     | 2 | -10     <= x,y <= 10   | f(f,f)   = g  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 17: McCormick function        | 2 | -1.5,-3 <= x,y <= 4,4  | f(h,j)   = k  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 18: Schaffer function N. 2    | 2 | -100    <= x,y <= 100  | f(0,0)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 19: Schaffer function N. 4    | 2 | -100    <= x,y <= 100  | f(0,l)   = m  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 20: Styblinski-Tang function  | d | -5      <= x_i <= 5    | q <= f(p) <= r|\n" : "" );
    output << ( ( basic || advanced ) ? "| 21: Stability test function 1 | 1 | 0       <= x   <= 1    | f(0.2)   = 1.3|\n" : "" );
    output << ( ( basic || advanced ) ? "|     (also has unstable max at |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     f(0.5) = 1.65 (2nd order) |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     and f(1) = 1.5 (1st))     |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "| 22: Stability test function 2 | 1 | 0       <= x   <= 1    | f(1)     = 4  |\n" : "" );
    output << ( ( basic || advanced ) ? "|     Sum  of   two  gaussians, |   |                        | f(0.5)   = 1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|     gamma  =   1/(5.sqrt(2)), |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     centres  at  1  (alpha 4) |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     and 0.5  (alpha  1).  Use |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     A = 0.2, B = 0.05, pmax=2 |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "| 23: Sum   of  RBFs   function | d | 0       <= x   <= 1    | depends       |\n" : "" );
    output << ( ( basic || advanced ) ? "|     sum_i a_{i,0} exp(-(||x-a_{i,2:...}||_2^2)/(2*a_{i,1}*a_{i,1}))        |\n" : "" );
    output << ( ( basic || advanced ) ? "| 10xx: function xx, normalised | d | -1      <= x   <= 1    | depends       |\n" : "" );
    output << ( ( basic || advanced ) ? "|     (nominally) so  -1<=x<=1, |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     0<=f(x)<=1                |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           Stability test function:                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           f(x) = exp(-20*(x-0.2)*(x-0.2))                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                + exp(-20*sqrt(0.00001+((x-0.5)*(x-0.5))))    \n" : "" );
    output << ( ( basic || advanced ) ? "                                + exp(2*(x-0.8))                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           Constants: a = +-1.34941                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                      b = -2.06261                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      c = 512                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                      d = 404.2319                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      e = -959.6407                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                      f = +-8.05502                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                      g = -19.2085                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      h = -0.54719                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      j = -1.54719                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      k = -1.9133                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                      l = 1.25313                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                      m = 0.292579                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      p = -2.903534                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                      q = -39.16617n                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                      r = -39.16616n                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                      (s,t) = (3,2), (-2.805,3.131),          \n" : "" );
    output << ( ( basic || advanced ) ? "                                          (-3.779,-3.283), (3.584,-1.848)     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "+-------------+---+-----+-----------------------------------------+----------+\n" : "" );
    output << ( ( basic || advanced ) ? "|  i: Fn name | n | M   | Function                                | Range    |\n" : "" );
    output << ( ( basic || advanced ) ? "+-------------+---+-----+-----------------------------------------+----------+\n" : "" );
    output << ( ( basic || advanced ) ? "|  1: DTLZ1   | n | <=n | [ x0...xM-2.(1+g(z))/2          ; ]     | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ x0...xM-3.(1-xM-2).(1+g(z))/2 ; ]     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                         ; ]     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ x0...(1-x1).(1+g(z))/2        ; ]     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1-x0).(1+g(z))/2               ]     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = 100.( #(z) + sum_i ( (zi-0.5)^2  |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |               - cos(20*pi*(zi-0.5)) ) ) |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  2: DTLZ2   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | s(x) = sin( x.pi/2 )                    |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | c(x) = cos( x.pi/2 )                    |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  3: DTLZ3   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = 100.( #(z) + sum_i ( (zi-0.5)^2  |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |               - cos(20*pi*(zi-0.5)) ) ) |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | s(x) = sin( x.pi/2 )                    |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | c(x) = cos( x.pi/2 )                    |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  4: DTLZ4   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x1<=1 |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] | -5<=xi<=5|\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                               ] | 2<=i<=n  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | s(x) = sin( (x^alpha).pi/2 )            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | c(x) = cos( (x^alpha).pi/2 )            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  5: DTLZ5   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | s(x) = sin( theta.pi/2 )                |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | c(x) = cos( theta.pi/2 )                |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | theta = (pi/(4.(1+g(z)))).(1+2.g(z).xi) |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  6: DTLZ6   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = sum_i zi^0.1                     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | s(x) = sin( theta.pi/2 )                |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | c(x) = cos( theta.pi/2 )                |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | theta = (pi/(4.(1+g(z)))).(1+2.g(z).xi) |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  7: DTLZ7   | n | <=n | [ x0                              ; ]   | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ x1                              ; ]   |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                             ]   |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ xM-2                            ; ]   |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).h(f1,f2,...,fM-2,g(z))   ]   |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = 1 + (9/#(z)) sum_i zi            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | h = M-sum_i((fi/(1+g)).(1+sin(3pi.fi))) |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  8: DTLZ8   | n | <n  | [ sum_ib^is xi ], i = 0,1,...,M-1       | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |        ib = floor(i*n/M)-1              |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |        is = floor((i+1)*n/M)-1          |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | **constraints are not implemented yet.  |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  9: DTLZ9   | n | <n  | [ sum_ib^is xi^0.1 ], i = 0,1,...,M-1   | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |        ib = floor(i*n/M)-1              |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |        is = floor((i+1)*n/M)-1          |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | **constraints are not implemented yet.  |          |\n" : "" );
    output << ( ( basic || advanced ) ? "| 10: FON1    | n | 2   | [ 1-exp(-|| x - 1/sqrt(n) ||^2) ; ]     | -4<=x<=4 |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ 1-exp(-|| x + 1/sqrt(n) ||^2)   ]     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "| 11: SCH1    | 1 | 2   | [ x^2     ; ]                           | free     |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (x-2)^2   ]                           |          |\n" : "" );
    output << ( ( basic || advanced ) ? "| 12: SCH2    | 1 | 2   | [ { -x     if     x <= 1 } ]            | -5<=x<=10|\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ { x-2    if 1 < x <= 3 } ]            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ { 4-x    if 3 < x <= 4 } ]            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ { x-4    if 4 < x      } ]            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [                          ]            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (x-5)^2                  ]            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "+-------------+---+-----------------------------------------------+----------+\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           SCH1: Schaffer, J. D.:  Some Experiments in Machine\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Learning   using  Vector   Evaluated  Genetic\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Algorithms - PhD Thesis, 1984.               \n" : "" );
    output << ( ( basic || advanced ) ? "                           DTLZ: Deb,   Kalyanmoy  and   Thiele,  Lothar   and\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Laumanns,   Marco    and   Zitzler,   Eckart:\n" : "" );
    output << ( ( basic || advanced ) ? "                                 \"Scalable  Test  Problems   for  Evolutionary\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Multiobjective Optimization\".                \n" : "" );
    output << ( ( basic || advanced ) ? "                           FON1: Fonseca,  C.  M.  and   Fleming,  P.  J.:  An\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Overview of Evolutionary Algorithms in Multi-\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Objective     Optimisation.      Evolutionary\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Multiobjective   Optimisation,    Theoretical\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Advances and Applications, pg. 105-145, 2005.\n" : "" );
    output << ( ( basic || advanced ) ? "                                 (as re-interpretted in DTLZ for n-dim).      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Elementwise versions of  functions are indicated by a leading e (or  E for the\n" : "" );
    output << ( ( basic || advanced ) ? "conjugated version).  The following elementwise functions are defined:        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "einv, emul, ediv, eidiv, erdiv, emod,  epow, epowl, epowr, eeq, ene, egt, ege,\n" : "" );
    output << ( ( basic || advanced ) ? "ele, elt, eabs1, eabs2, eabsp, eabsinf,  enorm1, enorm2, enormp, eangle, efnB,\n" : "" );
    output << ( ( basic || advanced ) ? " efnC,edfnB,edfnC.                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Conjugated versions of functions are  indicated by capitalisation of the first\n" : "" );
    output << ( ( basic || advanced ) ? "letter (eg Acosh(a) = conj(acosh(a))).  The following conjugated functions are\n" : "" );
    output << ( ( basic || advanced ) ? "defined:                                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Var,  Pow,  Powl, Powr,  Epow,  Epowl, Epowr,  Im_complex,  Im_quat,  Im_octo,\n" : "" );
    output << ( ( basic || advanced ) ? "Im_anion, CayleyDickson,  Imagd, Argd,  Argx, Sqrt,  Log, Log10,  Logb, Logbl,\n" : "" );
    output << ( ( basic || advanced ) ? "Logbr,  Asin,  Acos, Acosec,  Asec, Avers,  Acovers, Ahav,  Aexcosec,  Aexsec,\n" : "" );
    output << ( ( basic || advanced ) ? "Acosh, Atanh, Asech, Acoth, Aversh, Ahavh, Aexsech, Asigm, Agd, PolyDistr.    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "With regard to anionic extensions of  complex functions, these are extended to\n" : "" );
    output << ( ( basic || advanced ) ? "the general case by noting that the  set of numbers of the form R + q.I, where\n" : "" );
    output << ( ( basic || advanced ) ? "R and I are real and q  is an imaginary unit anion, are  isomorphic to complex\n" : "" );
    output << ( ( basic || advanced ) ? "numbers by simple  interchange of q and i (noting that q  commutes with reals,\n" : "" );
    output << ( ( basic || advanced ) ? "qq = -1, and the conjugate of q is -q).                                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "So, given a quaternion  a, write a = R + M, where  R is real and M is entirely\n" : "" );
    output << ( ( basic || advanced ) ? "imaginary (ie. R = real(a) and M = imagx(a)), and let I = abs2(M), q= angle(M)\n" : "" );
    output << ( ( basic || advanced ) ? "we may write a = R + q.I.   Then, if f is an  analytic complex valued function\n" : "" );
    output << ( ( basic || advanced ) ? "of a single complex variable, ie.:                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "f(z) = f_R(real(z),imagx(z)) + i.f_I(real(z),imagx(z))                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "then, given that q is indistinguishable  from the complex imaginery element i,\n" : "" );
    output << ( ( basic || advanced ) ? "the natural extension of f to the anions is:                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "f(a) = f_R(R,I) + q.f_I(R,I)                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "which  is how  we  extend  most functions.  For functions  of  more  than  one\n" : "" );
    output << ( ( basic || advanced ) ? "variable, however (e.g.  logb), there are difficulties  due to the presence of\n" : "" );
    output << ( ( basic || advanced ) ? "multiple unit imaginary anions  which do not in general commute or multiply to\n" : "" );
    output << ( ( basic || advanced ) ? "give -1 (see above for implementation of these).                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "In  some cases  where functions  are well  defined for  the complex  case, for\n" : "" );
    output << ( ( basic || advanced ) ? "example acosh(-2),  we could define  an infinite number of  results, as we are\n" : "" );
    output << ( ( basic || advanced ) ? "going from real  to complex without  a well defined  unit anion q to  base our\n" : "" );
    output << ( ( basic || advanced ) ? "result.  If we assume  q is complex (and we do)  then q is defined  up to sign\n" : "" );
    output << ( ( basic || advanced ) ? "and the sign can be  taken care of.  If, however, we  don't so restrict q then\n" : "" );
    output << ( ( basic || advanced ) ? "we have an infinite family to chose from, and they all give valid answers.  To\n" : "" );
    output << ( ( basic || advanced ) ? "get around this we arbitrarily set q = i whenever ambiguity occurs (and q = -i\n" : "" );
    output << ( ( basic || advanced ) ? "if for conjugated versions - eg Acosh(-2) = conj(acosh(-2)).                  \n" : "" );

    return;
}
